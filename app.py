import os,sys,json,math

# è®¾ç½®ç¯å¢ƒå˜é‡æ¥è§£å†³numbaç¼“å­˜é—®é¢˜
os.environ['NUMBA_CACHE_DIR'] = '/tmp/numba_cache'
os.environ['NUMBA_DISABLE_JIT'] = '0'

host = '0.0.0.0'
port = 5092
threads = 4
# é»˜è®¤æŒ‰ç…§Nåˆ†é’Ÿå°†éŸ³è§†é¢‘è£åˆ‡ä¸ºå¤šæ®µï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚ç°åœ¨å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ CHUNK_MINITE æ¥è°ƒæ•´ã€‚
# 8Gæ˜¾å­˜å»ºè®®è®¾ç½®ä¸º 10-15 åˆ†é’Ÿä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
CHUNK_MINITE = int(os.environ.get('CHUNK_MINITE', '10'))
# æœåŠ¡é—²ç½®Nåˆ†é’Ÿåè‡ªåŠ¨å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜ï¼Œè®¾ç½®ä¸º0åˆ™ç¦ç”¨
IDLE_TIMEOUT_MINUTES = int(os.environ.get('IDLE_TIMEOUT_MINUTES', '30'))
# æ‡’åŠ è½½å¼€å…³ï¼Œé»˜è®¤ä¸º trueã€‚è®¾ç½®ä¸º 'false' å¯åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹ã€‚
ENABLE_LAZY_LOAD = os.environ.get('ENABLE_LAZY_LOAD', 'true').lower() not in ['false', '0', 'f']
# Whisper å…¼å®¹çš„ API Keyã€‚å¦‚æœç•™ç©ºï¼Œåˆ™ä¸è¿›è¡Œèº«ä»½éªŒè¯ã€‚
API_KEY = os.environ.get('API_KEY', None)
import shutil
import uuid
import subprocess
import datetime
import threading
import time
from werkzeug.utils import secure_filename

from flask import Flask, request, jsonify, Response
from waitress import serve
from pathlib import Path
# ROOT_DIR is not needed in Docker environment
os.environ['HF_ENDPOINT']='https://hf-mirror.com'
# HF_HOME is set in the Dockerfile
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = 'true'
# PATH for ffmpeg is handled by the Docker image's system PATH

import nemo.collections.asr as nemo_asr
import torch
import gc

# --- å…¨å±€è®¾ç½®ä¸æ¨¡å‹çŠ¶æ€ ---
asr_model = None
last_request_time = None
model_lock = threading.Lock()


# ç¡®ä¿ä¸´æ—¶ä¸Šä¼ ç›®å½•å­˜åœ¨
if not os.path.exists('/app/temp_uploads'):
    os.makedirs('/app/temp_uploads')

def load_model_if_needed():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œåˆ™è¿›è¡ŒåŠ è½½ã€‚"""
    global asr_model
    # ä½¿ç”¨é”ç¡®ä¿å¤šçº¿ç¨‹ç¯å¢ƒä¸‹æ¨¡å‹åªè¢«åŠ è½½ä¸€æ¬¡
    with model_lock:
        if asr_model is None:
            print("="*50)
            print("æ¨¡å‹å½“å‰æœªåŠ è½½ï¼Œæ­£åœ¨ä»ç£ç›˜åŠ è½½...")
            print("æ¨¡å‹åç§°: nvidia/parakeet-tdt-0.6b-v2")
            try:
                # ç¡®ä¿numbaç¼“å­˜ç›®å½•å­˜åœ¨
                numba_cache_dir = os.environ.get('NUMBA_CACHE_DIR', '/tmp/numba_cache')
                if not os.path.exists(numba_cache_dir):
                    os.makedirs(numba_cache_dir, exist_ok=True)
                    os.chmod(numba_cache_dir, 0o777)
                
                model_path = "/app/models/parakeet-tdt-0.6b-v2.nemo"
                if not os.path.exists(model_path):
                    raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}ï¼Œè¯·ç¡®è®¤ models æ–‡ä»¶å¤¹å·²æ­£ç¡®æŒ‚è½½ã€‚")

                # æ£€æŸ¥æ–‡ä»¶æƒé™
                if not os.access(model_path, os.R_OK):
                    raise PermissionError(f"æ— æ³•è¯»å–æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™ã€‚")

                if torch.cuda.is_available():
                    print(f"æ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿå¹¶å¼€å¯åŠç²¾åº¦(FP16)ä¼˜åŒ–ã€‚")
                    # å…ˆåœ¨CPUä¸ŠåŠ è½½æ¨¡å‹ï¼Œç„¶åè½¬ç§»åˆ°GPUå¹¶å¯ç”¨FP16
                    loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=model_path, map_location=torch.device('cpu'))
                    loaded_model = loaded_model.cuda()
                    loaded_model = loaded_model.half()
                else:
                    print("æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU è¿è¡Œã€‚")
                    loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=model_path)
                
                asr_model = loaded_model
                print("âœ… NeMo ASR æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                print("="*50)
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("="*50)
                import traceback
                traceback.print_exc()
                # å‘ä¸ŠæŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿æ¥å£å¯ä»¥æ•è·å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
                raise e
    return asr_model

def unload_model():
    """ä»å†…å­˜/æ˜¾å­˜ä¸­å¸è½½æ¨¡å‹ã€‚"""
    global asr_model, last_request_time
    with model_lock:
        if asr_model is not None:
            print(f"æ¨¡å‹é—²ç½®è¶…è¿‡ {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿï¼Œæ­£åœ¨ä»æ˜¾å­˜ä¸­å¸è½½...")
            asr_model = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            last_request_time = None # é‡ç½®è®¡æ—¶å™¨ï¼Œé˜²æ­¢é‡å¤å¸è½½
            print("âœ… æ¨¡å‹å·²æˆåŠŸå¸è½½ã€‚")

def model_cleanup_checker():
    """åå°çº¿ç¨‹ï¼Œå‘¨æœŸæ€§æ£€æŸ¥æ¨¡å‹æ˜¯å¦é—²ç½®è¿‡ä¹…å¹¶æ‰§è¡Œå¸è½½ã€‚"""
    while True:
        # æ¯ 60 ç§’æ£€æŸ¥ä¸€æ¬¡
        time.sleep(60)
        if asr_model is not None and last_request_time is not None:
            idle_duration = (datetime.datetime.now() - last_request_time).total_seconds()
            if idle_duration > IDLE_TIMEOUT_MINUTES * 60:
                unload_model()


# --- Flask åº”ç”¨åˆå§‹åŒ– ---
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = '/app/temp_uploads'
app.config['MAX_CONTENT_LENGTH'] = 2000 * 1024 * 1024  

# --- è¾…åŠ©å‡½æ•° ---
def get_audio_duration(file_path: str) -> float:
    """ä½¿ç”¨ ffprobe è·å–éŸ³é¢‘æ–‡ä»¶çš„æ—¶é•¿ï¼ˆç§’ï¼‰"""
    command = [
        'ffprobe',
        '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        file_path
    ]
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        return float(result.stdout)
    except (subprocess.CalledProcessError, ValueError) as e:
        print(f"æ— æ³•è·å–æ–‡ä»¶ '{file_path}' çš„æ—¶é•¿: {e}")
        return 0.0

def format_srt_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º SRT æ—¶é—´æˆ³æ ¼å¼ HH:MM:SS,ms"""
    delta = datetime.timedelta(seconds=seconds)
    # æ ¼å¼åŒ–ä¸º 0:00:05.123000
    s = str(delta)
    # åˆ†å‰²ç§’å’Œå¾®ç§’
    if '.' in s:
        parts = s.split('.')
        integer_part = parts[0]
        fractional_part = parts[1][:3] # å–å‰ä¸‰ä½æ¯«ç§’
    else:
        integer_part = s
        fractional_part = "000"

    # å¡«å……å°æ—¶ä½
    if len(integer_part.split(':')) == 2:
        integer_part = "0:" + integer_part
    
    return f"{integer_part},{fractional_part}"


def segments_to_srt(segments: list) -> str:
    """å°† NeMo çš„åˆ†æ®µæ—¶é—´æˆ³è½¬æ¢ä¸º SRT æ ¼å¼å­—ç¬¦ä¸²"""
    srt_content = []
    for i, segment in enumerate(segments):
        start_time = format_srt_time(segment['start'])
        end_time = format_srt_time(segment['end'])
        text = segment['segment'].strip()
        
        if text: # ä»…æ·»åŠ æœ‰å†…å®¹çš„å­—å¹•
            srt_content.append(str(i + 1))
            srt_content.append(f"{start_time} --> {end_time}")
            srt_content.append(text)
            srt_content.append("") # ç©ºè¡Œåˆ†éš”
            
    return "\n".join(srt_content)

# --- Flask è·¯ç”± ---

@app.route('/v1/audio/transcriptions', methods=['POST'])
def transcribe_audio():
    """
    å…¼å®¹ OpenAI çš„è¯­éŸ³è¯†åˆ«æ¥å£ï¼Œæ”¯æŒé•¿éŸ³é¢‘åˆ†ç‰‡å¤„ç†ã€‚
    """
    # --- -1. API Key è®¤è¯ ---
    if API_KEY:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({"error": "Authorization header is missing or invalid. It must be in 'Bearer <key>' format."}), 401
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != API_KEY:
            return jsonify({"error": "Invalid API key."}), 401

    # --- 0. ç¡®ä¿æ¨¡å‹åŠ è½½å¹¶æ›´æ–°æ—¶é—´æˆ³ ---
    try:
        # å¦‚æœæ‡’åŠ è½½å¯ç”¨ï¼Œåˆ™æŒ‰éœ€åŠ è½½ï¼›å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„å…¨å±€æ¨¡å‹
        local_asr_model = load_model_if_needed() if ENABLE_LAZY_LOAD else asr_model
        if not local_asr_model:
             # æ­¤æƒ…å†µæ¶µç›–äº†æ‡’åŠ è½½å¤±è´¥å’Œé¢„åŠ è½½å¤±è´¥ä¸¤ç§åœºæ™¯
             return jsonify({"error": "æ¨¡å‹åŠ è½½å¤±è´¥æˆ–å°šæœªåŠ è½½ï¼Œæ— æ³•å¤„ç†è¯·æ±‚"}), 500
    except Exception as e:
        return jsonify({"error": f"æ¨¡å‹åŠ è½½æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"}), 500
    
    # å¦‚æœå¯ç”¨äº†æ‡’åŠ è½½ï¼Œåˆ™æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
    if ENABLE_LAZY_LOAD:
        global last_request_time
        last_request_time = datetime.datetime.now()


    # --- 1. åŸºæœ¬æ ¡éªŒ ---
    if 'file' not in request.files:
        return jsonify({"error": "è¯·æ±‚ä¸­æœªæ‰¾åˆ°æ–‡ä»¶éƒ¨åˆ†"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    if not shutil.which('ffmpeg'):
        return jsonify({"error": "FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿ PATH ä¸­"}), 500
    if not shutil.which('ffprobe'):
        return jsonify({"error": "ffprobe æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿ PATH ä¸­"}), 500

    # è·å–è¯·æ±‚å‚æ•°
    model_name = request.form.get('model', 'whisper-1')
    response_format = request.form.get('response_format', 'json')  # æ”¯æŒ json, text, srt, verbose_json, vtt
    language = request.form.get('language', None)
    prompt = request.form.get('prompt', None)
    temperature = float(request.form.get('temperature', 0))
    
    print(f"æ¥æ”¶åˆ°è¯·æ±‚ï¼Œæ¨¡å‹: '{model_name}', å“åº”æ ¼å¼: '{response_format}'")

    original_filename = secure_filename(file.filename)
    unique_id = str(uuid.uuid4())
    temp_original_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_{original_filename}")
    target_wav_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}.wav")
    
    # ç”¨äºæ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶çš„åˆ—è¡¨
    temp_files_to_clean = []

    try:
        # --- 2. ä¿å­˜å¹¶ç»Ÿä¸€è½¬æ¢ä¸º 16k å•å£°é“ WAV ---
        file.save(temp_original_path)
        temp_files_to_clean.append(temp_original_path)
        
        print(f"[{unique_id}] æ­£åœ¨å°† '{original_filename}' è½¬æ¢ä¸ºæ ‡å‡† WAV æ ¼å¼...")
        ffmpeg_command = [
            'ffmpeg', '-y', '-i', temp_original_path,
            '-ac', '1', '-ar', '16000', target_wav_path
        ]
        result = subprocess.run(ffmpeg_command, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"FFmpeg é”™è¯¯: {result.stderr}")
            return jsonify({"error": "æ–‡ä»¶è½¬æ¢å¤±è´¥", "details": result.stderr}), 500
        temp_files_to_clean.append(target_wav_path)

        # --- 3. éŸ³é¢‘åˆ‡ç‰‡ (Chunking) ---
        CHUNK_DURATION_SECONDS = CHUNK_MINITE * 60  
        total_duration = get_audio_duration(target_wav_path)
        if total_duration == 0:
            return jsonify({"error": "æ— æ³•å¤„ç†æ—¶é•¿ä¸º0çš„éŸ³é¢‘"}), 400

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡ç‰‡ï¼Œå¦‚æœéŸ³é¢‘æ—¶é•¿å°äºåˆ‡ç‰‡é˜ˆå€¼ï¼Œåˆ™ç›´æ¥å¤„ç†
        if total_duration <= CHUNK_DURATION_SECONDS:
            print(f"[{unique_id}] æ–‡ä»¶æ€»æ—¶é•¿: {total_duration:.2f}s. å°äºåˆ‡ç‰‡é˜ˆå€¼({CHUNK_DURATION_SECONDS}s)ï¼Œæ— éœ€åˆ‡ç‰‡ã€‚")
            chunk_paths = [target_wav_path]
            num_chunks = 1
        else:
            num_chunks = math.ceil(total_duration / CHUNK_DURATION_SECONDS)
            chunk_paths = []
            print(f"[{unique_id}] æ–‡ä»¶æ€»æ—¶é•¿: {total_duration:.2f}s. å°†åˆ‡åˆ†ä¸º {num_chunks} ä¸ªç‰‡æ®µã€‚")
            
            for i in range(num_chunks):
                start_time = i * CHUNK_DURATION_SECONDS
                chunk_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_chunk_{i}.wav")
                chunk_paths.append(chunk_path)
                temp_files_to_clean.append(chunk_path)
                
                print(f"[{unique_id}] æ­£åœ¨åˆ›å»ºåˆ‡ç‰‡ {i+1}/{num_chunks}...")
                chunk_command = [
                    'ffmpeg', '-y', '-i', target_wav_path,
                    '-ss', str(start_time),
                    '-t', str(CHUNK_DURATION_SECONDS),
                    '-c', 'copy',
                    chunk_path
                ]
                subprocess.run(chunk_command, capture_output=True, text=True)
            
        # --- 4. å¾ªç¯è½¬å½•å¹¶åˆå¹¶ç»“æœ ---
        all_segments = []
        all_words = []
        cumulative_time_offset = 0.0

        for i, chunk_path in enumerate(chunk_paths):
            print(f"[{unique_id}] æ­£åœ¨è½¬å½•åˆ‡ç‰‡ {i+1}/{num_chunks}...")
            
            # å¯¹å½“å‰åˆ‡ç‰‡è¿›è¡Œè½¬å½•
            # ä½¿ç”¨ with torch.cuda.amp.autocast() åœ¨åŠç²¾åº¦ä¸‹è¿è¡Œæ¨ç†
            if torch.cuda.is_available():
                with torch.cuda.amp.autocast():
                    output = local_asr_model.transcribe([chunk_path], timestamps=True)
            else:
                 output = local_asr_model.transcribe([chunk_path], timestamps=True)

            # ç«‹å³æ¸…ç†æ˜¾å­˜ï¼Œé¿å…ç´¯ç§¯
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            if output and output[0].timestamp:
                # ä¿®æ­£å¹¶æ”¶é›† segment æ—¶é—´æˆ³
                if 'segment' in output[0].timestamp:
                    for seg in output[0].timestamp['segment']:
                        seg['start'] += cumulative_time_offset
                        seg['end'] += cumulative_time_offset
                        all_segments.append(seg)
                
                # ä¿®æ­£å¹¶æ”¶é›† word æ—¶é—´æˆ³
                if 'word' in output[0].timestamp:
                     for word in output[0].timestamp['word']:
                        word['start'] += cumulative_time_offset
                        word['end'] += cumulative_time_offset
                        all_words.append(word)

            # æ›´æ–°ä¸‹ä¸€ä¸ªåˆ‡ç‰‡çš„æ—¶é—´åç§»é‡
            # ä½¿ç”¨å®é™…åˆ‡ç‰‡æ—¶é•¿æ¥æ›´æ–°ï¼Œæ›´ç²¾ç¡®
            chunk_actual_duration = get_audio_duration(chunk_path)
            cumulative_time_offset += chunk_actual_duration

        print(f"[{unique_id}] æ‰€æœ‰åˆ‡ç‰‡è½¬å½•å®Œæˆï¼Œæ­£åœ¨åˆå¹¶ç»“æœã€‚")

        # --- 5. æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º ---
        if not all_segments:
            return jsonify({"error": "è½¬å½•å¤±è´¥ï¼Œæ¨¡å‹æœªè¿”å›ä»»ä½•æœ‰æ•ˆå†…å®¹"}), 500

        # æ„å»ºå®Œæ•´çš„è½¬å½•æ–‡æœ¬
        full_text = " ".join([seg['segment'].strip() for seg in all_segments if seg['segment'].strip()])
        
        # æ ¹æ® response_format è¿”å›ä¸åŒæ ¼å¼
        if response_format == 'text':
            return Response(full_text, mimetype='text/plain')
        elif response_format == 'srt':
            srt_result = segments_to_srt(all_segments)
            return Response(srt_result, mimetype='text/plain')
        elif response_format == 'vtt':
            vtt_result = segments_to_vtt(all_segments)
            return Response(vtt_result, mimetype='text/plain')
        elif response_format == 'verbose_json':
            # è¯¦ç»†çš„ JSON æ ¼å¼ï¼ŒåŒ…å«æ›´å¤šä¿¡æ¯
            response_data = {
                "task": "transcribe",
                "language": language or "en",
                "duration": total_duration,
                "text": full_text,
                "segments": [
                    {
                        "id": i,
                        "seek": int(seg['start'] * 100),  # è½¬æ¢ä¸º centiseconds
                        "start": seg['start'],
                        "end": seg['end'],
                        "text": seg['segment'].strip(),
                        "tokens": [],  # NeMo ä¸æä¾› tokensï¼Œç•™ç©º
                        "temperature": temperature,
                        "avg_logprob": -0.5,  # æ¨¡æ‹Ÿå€¼
                        "compression_ratio": 1.0,  # æ¨¡æ‹Ÿå€¼
                        "no_speech_prob": 0.0,  # æ¨¡æ‹Ÿå€¼
                        "words": [
                            {
                                "word": word['word'],
                                "start": word['start'],
                                "end": word['end'],
                                "probability": 0.9  # æ¨¡æ‹Ÿå€¼
                            }
                            for word in all_words 
                            if word['start'] >= seg['start'] and word['end'] <= seg['end']
                        ] if all_words else []
                    }
                    for i, seg in enumerate(all_segments) if seg['segment'].strip()
                ]
            }
            return jsonify(response_data)
        else:
            # é»˜è®¤ JSON æ ¼å¼ (response_format == 'json')
            response_data = {
                "text": full_text
            }
            return jsonify(response_data)

    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", "details": str(e)}), 500
    finally:
        # --- 6. æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ ---
        print(f"[{unique_id}] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for f_path in temp_files_to_clean:
            if os.path.exists(f_path):
                os.remove(f_path)
        print(f"[{unique_id}] ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
        
        # --- 7. å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼Œé¿å…ç´¯ç§¯ ---
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print(f"[{unique_id}] æ˜¾å­˜å·²æ¸…ç†ã€‚")


def segments_to_vtt(segments: list) -> str:
    """å°† NeMo çš„åˆ†æ®µæ—¶é—´æˆ³è½¬æ¢ä¸º VTT æ ¼å¼å­—ç¬¦ä¸²"""
    vtt_content = ["WEBVTT", ""]
    
    for i, segment in enumerate(segments):
        start_time = format_vtt_time(segment['start'])
        end_time = format_vtt_time(segment['end'])
        text = segment['segment'].strip()
        
        if text:  # ä»…æ·»åŠ æœ‰å†…å®¹çš„å­—å¹•
            vtt_content.append(f"{start_time} --> {end_time}")
            vtt_content.append(text)
            vtt_content.append("")  # ç©ºè¡Œåˆ†éš”
            
    return "\n".join(vtt_content)


def format_vtt_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º VTT æ—¶é—´æˆ³æ ¼å¼ HH:MM:SS.mmm"""
    delta = datetime.timedelta(seconds=seconds)
    # æ ¼å¼åŒ–ä¸º 0:00:05.123000
    s = str(delta)
    # åˆ†å‰²ç§’å’Œå¾®ç§’
    if '.' in s:
        parts = s.split('.')
        integer_part = parts[0]
        fractional_part = parts[1][:3]  # å–å‰ä¸‰ä½æ¯«ç§’
    else:
        integer_part = s
        fractional_part = "000"

    # å¡«å……å°æ—¶ä½
    if len(integer_part.split(':')) == 2:
        integer_part = "0:" + integer_part
    
    return f"{integer_part}.{fractional_part}"

# --- Waitress æœåŠ¡å™¨å¯åŠ¨ ---
if __name__ == '__main__':
    
    # æ ¹æ®æ˜¯å¦å¯ç”¨æ‡’åŠ è½½æ¥å†³å®šæ˜¯é¢„åŠ è½½æ¨¡å‹è¿˜æ˜¯å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
    if ENABLE_LAZY_LOAD:
        print("æ‡’åŠ è½½æ¨¡å¼å·²å¯ç”¨ã€‚æ¨¡å‹å°†åœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶åŠ è½½ã€‚")
        # å¯åŠ¨åå°çº¿ç¨‹æ¥ç›‘æ§å’Œå¸è½½é—²ç½®çš„æ¨¡å‹
        if IDLE_TIMEOUT_MINUTES > 0:
            print(f"å°†å¯ç”¨æ¨¡å‹è‡ªåŠ¨å¸è½½åŠŸèƒ½ï¼Œé—²ç½®è¶…æ—¶: {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿã€‚")
            cleanup_thread = threading.Thread(target=model_cleanup_checker, daemon=True)
            cleanup_thread.start()
        else:
            print("æ¨¡å‹è‡ªåŠ¨å¸è½½åŠŸèƒ½å·²ç¦ç”¨ (IDLE_TIMEOUT_MINUTES=0)ã€‚")
    else:
        # æ‡’åŠ è½½è¢«ç¦ç”¨ï¼Œåœ¨å¯åŠ¨æ—¶ç›´æ¥åŠ è½½æ¨¡å‹
        print("æ‡’åŠ è½½æ¨¡å¼å·²ç¦ç”¨ï¼Œæ­£åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹...")
        try:
            load_model_if_needed()
        except Exception as e:
            print(f"âŒ å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            # é¢„åŠ è½½å¤±è´¥æ—¶é€€å‡ºï¼Œä»¥é¿å…è¿è¡Œä¸€ä¸ªæŸåçš„æœåŠ¡
            exit(1)

    if API_KEY:
        print(f"API Key è®¤è¯å·²å¯ç”¨ã€‚è¯·åœ¨è¯·æ±‚å¤´ä¸­æä¾› 'Authorization: Bearer YOUR_API_KEY'")
    else:
        print("API Key è®¤è¯å·²ç¦ç”¨ï¼Œä»»ä½•è¯·æ±‚éƒ½å°†è¢«æ¥å—ã€‚")


    print(f"ğŸš€ æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print(f"API ç«¯ç‚¹: POST http://{host}:{port}/v1/audio/transcriptions")
    print(f"æœåŠ¡å°†ä½¿ç”¨ {threads} ä¸ªçº¿ç¨‹è¿è¡Œã€‚")
    serve(app, host=host, port=port, threads=threads)