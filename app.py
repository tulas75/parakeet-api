import os,sys,json,math

# è®¾ç½®ç¯å¢ƒå˜é‡æ¥è§£å†³numbaç¼“å­˜é—®é¢˜
os.environ['NUMBA_CACHE_DIR'] = '/tmp/numba_cache'
os.environ['NUMBA_DISABLE_JIT'] = '0'

host = '0.0.0.0'
port = 5092
threads = 4
# é»˜è®¤æŒ‰ç…§Nåˆ†é’Ÿå°†éŸ³è§†é¢‘è£åˆ‡ä¸ºå¤šæ®µï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚ç°åœ¨å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ CHUNK_MINITE æ¥è°ƒæ•´ã€‚
# 8Gæ˜¾å­˜å»ºè®®è®¾ç½®ä¸º 10-15 åˆ†é’Ÿä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
CHUNK_MINITE = int(os.environ.get('CHUNK_MINITE', '10'))
# æœåŠ¡é—²ç½®Nåˆ†é’Ÿåè‡ªåŠ¨å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜ï¼Œè®¾ç½®ä¸º0åˆ™ç¦ç”¨
IDLE_TIMEOUT_MINUTES = int(os.environ.get('IDLE_TIMEOUT_MINUTES', '30'))
# æ‡’åŠ è½½å¼€å…³ï¼Œé»˜è®¤ä¸º trueã€‚è®¾ç½®ä¸º 'false' å¯åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹ã€‚
ENABLE_LAZY_LOAD = os.environ.get('ENABLE_LAZY_LOAD', 'true').lower() not in ['false', '0', 'f']
# Whisper å…¼å®¹çš„ API Keyã€‚å¦‚æœç•™ç©ºï¼Œåˆ™ä¸è¿›è¡Œèº«ä»½éªŒè¯ã€‚
API_KEY = os.environ.get('API_KEY', None)
import shutil
import uuid
import subprocess
import datetime
import threading
import time
from werkzeug.utils import secure_filename

from flask import Flask, request, jsonify, Response
from waitress import serve
from pathlib import Path
# ROOT_DIR is not needed in Docker environment
os.environ['HF_ENDPOINT']='https://hf-mirror.com'
# HF_HOME is set in the Dockerfile
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = 'true'
# PATH for ffmpeg is handled by the Docker image's system PATH

import nemo.collections.asr as nemo_asr  # type: ignore
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import gc
import psutil

# --- å…¨å±€è®¾ç½®ä¸æ¨¡å‹çŠ¶æ€ ---
asr_model = None
last_request_time = None
model_lock = threading.Lock()
cuda_available = False  # å…¨å±€CUDAå…¼å®¹æ€§æ ‡å¿—

# æ˜¾å­˜ä¼˜åŒ–é…ç½®
AGGRESSIVE_MEMORY_CLEANUP = os.environ.get('AGGRESSIVE_MEMORY_CLEANUP', 'true').lower() in ['true', '1', 't']
ENABLE_GRADIENT_CHECKPOINTING = os.environ.get('ENABLE_GRADIENT_CHECKPOINTING', 'true').lower() in ['true', '1', 't']
MAX_CHUNK_MEMORY_MB = int(os.environ.get('MAX_CHUNK_MEMORY_MB', '1500'))
FORCE_CLEANUP_THRESHOLD = float(os.environ.get('FORCE_CLEANUP_THRESHOLD', '0.8'))

# Tensor Core ä¼˜åŒ–é…ç½®
ENABLE_TENSOR_CORE = os.environ.get('ENABLE_TENSOR_CORE', 'true').lower() in ['true', '1', 't']
ENABLE_CUDNN_BENCHMARK = os.environ.get('ENABLE_CUDNN_BENCHMARK', 'true').lower() in ['true', '1', 't']
TENSOR_CORE_PRECISION = os.environ.get('TENSOR_CORE_PRECISION', 'highest')  # highest, high, medium

# å¥å­å®Œæ•´æ€§ä¼˜åŒ–é…ç½®
ENABLE_OVERLAP_CHUNKING = os.environ.get('ENABLE_OVERLAP_CHUNKING', 'true').lower() in ['true', '1', 't']
CHUNK_OVERLAP_SECONDS = float(os.environ.get('CHUNK_OVERLAP_SECONDS', '30'))  # é‡å æ—¶é•¿
SENTENCE_BOUNDARY_THRESHOLD = float(os.environ.get('SENTENCE_BOUNDARY_THRESHOLD', '0.5'))  # å¥å­è¾¹ç•Œæ£€æµ‹é˜ˆå€¼


# ç¡®ä¿ä¸´æ—¶ä¸Šä¼ ç›®å½•å­˜åœ¨
if not os.path.exists('/app/temp_uploads'):
    os.makedirs('/app/temp_uploads')

def setup_tensor_core_optimization():
    """é…ç½®Tensor Coreä¼˜åŒ–è®¾ç½®"""
    global cuda_available
    if not cuda_available:
        print("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡Tensor Coreä¼˜åŒ–é…ç½®")
        return
    
    print("æ­£åœ¨é…ç½® Tensor Core ä¼˜åŒ–...")
    
    try:
        # å¯ç”¨ cuDNN benchmark æ¨¡å¼
        if ENABLE_CUDNN_BENCHMARK:
            cudnn.benchmark = True
            cudnn.deterministic = False  # ä¸ºäº†æ€§èƒ½ï¼Œå…è®¸éç¡®å®šæ€§
            print("âœ… cuDNN benchmark å·²å¯ç”¨")
        else:
            cudnn.benchmark = False
            cudnn.deterministic = True
            print("âŒ cuDNN benchmark å·²ç¦ç”¨ï¼ˆç¡®å®šæ€§æ¨¡å¼ï¼‰")
        
        # å¯ç”¨ cuDNN å…è®¸ TensorCore
        if ENABLE_TENSOR_CORE:
            cudnn.allow_tf32 = True  # å…è®¸TF32ï¼ˆA100ç­‰æ”¯æŒï¼‰
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("âœ… Tensor Core (TF32) å·²å¯ç”¨")
        else:
            cudnn.allow_tf32 = False
            torch.backends.cuda.matmul.allow_tf32 = False
            torch.backends.cudnn.allow_tf32 = False
            print("âŒ Tensor Core å·²ç¦ç”¨")
        
        # è®¾ç½® Tensor Core ç²¾åº¦ç­–ç•¥
        if TENSOR_CORE_PRECISION == 'highest':
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
            print("âœ… è®¾ç½®ä¸ºæœ€é«˜ç²¾åº¦æ¨¡å¼")
        elif TENSOR_CORE_PRECISION == 'high':
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
            print("âœ… è®¾ç½®ä¸ºé«˜ç²¾åº¦æ¨¡å¼")
        else:  # medium
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
            print("âœ… è®¾ç½®ä¸ºä¸­ç­‰ç²¾åº¦æ¨¡å¼")
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥ä»¥ä¼˜åŒ– Tensor Core ä½¿ç”¨
        torch.cuda.set_per_process_memory_fraction(0.95)  # ä½¿ç”¨95%çš„æ˜¾å­˜
        print("âœ… GPU å†…å­˜åˆ†é…ç­–ç•¥å·²ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸ Tensor Coreä¼˜åŒ–é…ç½®å¤±è´¥: {e}")

def get_tensor_core_info():
    """è·å– Tensor Core æ”¯æŒä¿¡æ¯"""
    global cuda_available
    if not cuda_available:
        return "N/A - CUDAä¸å¯ç”¨"
    
    try:
        device = torch.cuda.get_device_properties(0)
        major, minor = device.major, device.minor
        
        # æ£€æµ‹ Tensor Core æ”¯æŒ
        if major >= 7:  # V100, T4, RTX 20/30/40ç³»åˆ—ç­‰
            if major == 7:
                return f"âœ… Tensor Core 1.0 (è®¡ç®—èƒ½åŠ› {major}.{minor})"
            elif major == 8:
                if minor >= 0:
                    return f"âœ… Tensor Core 2.0 + TF32 (è®¡ç®—èƒ½åŠ› {major}.{minor})"
                else:
                    return f"âœ… Tensor Core 2.0 (è®¡ç®—èƒ½åŠ› {major}.{minor})"
            elif major >= 9:
                return f"âœ… Tensor Core 3.0+ (è®¡ç®—èƒ½åŠ› {major}.{minor})"
        elif major >= 6:  # P100ç­‰
            return f"âš ï¸ æœ‰é™Tensor Coreæ”¯æŒ (è®¡ç®—èƒ½åŠ› {major}.{minor})"
        else:
            return f"âŒ ä¸æ”¯æŒTensor Core (è®¡ç®—èƒ½åŠ› {major}.{minor})"
        
        return f"æœªçŸ¥ (è®¡ç®—èƒ½åŠ› {major}.{minor})"
    except Exception as e:
        return f"âŒ è·å–GPUä¿¡æ¯å¤±è´¥: {e}"

def optimize_tensor_operations():
    """ä¼˜åŒ–å¼ é‡æ“ä½œä»¥æ›´å¥½åœ°åˆ©ç”¨ Tensor Core"""
    global cuda_available
    if not cuda_available:
        print("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡Tensor Coreé¢„çƒ­")
        return
    
    try:
        # è®¾ç½®ä¼˜åŒ–çš„ CUDA æµ
        torch.cuda.set_sync_debug_mode(0)  # ç¦ç”¨åŒæ­¥è°ƒè¯•ä»¥æå‡æ€§èƒ½
        
        # é¢„çƒ­GPUï¼Œç¡®ä¿Tensor Coreæ­£ç¡®æ¿€æ´»
        # åˆ›å»ºä¸€äº›å¯¹é½åˆ°8/16å€æ•°çš„çŸ©é˜µè¿›è¡Œé¢„çƒ­
        device = torch.cuda.current_device()
        dummy_a = torch.randn(128, 128, device=device, dtype=torch.float16)
        dummy_b = torch.randn(128, 128, device=device, dtype=torch.float16)
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•é¢„çƒ­Tensor Core
        with torch.cuda.amp.autocast():
            _ = torch.matmul(dummy_a, dummy_b)
        
        torch.cuda.synchronize()
        del dummy_a, dummy_b
        torch.cuda.empty_cache()
        print("âœ… Tensor Core é¢„çƒ­å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ Tensor Core é¢„çƒ­å¤±è´¥: {e}")

def detect_sentence_boundaries(text: str) -> list:
    """æ£€æµ‹å¥å­è¾¹ç•Œï¼Œè¿”å›å¥å­ç»“æŸä½ç½®åˆ—è¡¨"""
    import re
    
    # ä¸­è‹±æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰
    sentence_endings = re.finditer(r'[.!?ã€‚ï¼ï¼Ÿ]+[\s]*', text)
    boundaries = [match.end() for match in sentence_endings]
    return boundaries

def find_best_split_point(segments: list, target_time: float, tolerance: float = 2.0) -> int:
    """åœ¨ç›®æ ‡æ—¶é—´é™„è¿‘æ‰¾åˆ°æœ€ä½³çš„å¥å­åˆ†å‰²ç‚¹"""
    if not segments:
        return 0
    
    best_index = 0
    min_distance = float('inf')
    
    # å¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡æ—¶é—´çš„å¥å­ç»“æŸç‚¹
    for i, segment in enumerate(segments):
        segment_end = segment.get('end', 0)
        distance = abs(segment_end - target_time)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¥å­ç»“æŸï¼ˆåŒ…å«æ ‡ç‚¹ç¬¦å·ï¼‰
        text = segment.get('segment', '').strip()
        if text and (text.endswith('.') or text.endswith('ã€‚') or 
                     text.endswith('!') or text.endswith('ï¼') or
                     text.endswith('?') or text.endswith('ï¼Ÿ')):
            # å¥å­ç»“æŸç‚¹ï¼Œæƒé‡æ›´é«˜
            distance *= 0.5
        
        if distance < min_distance and distance <= tolerance:
            min_distance = distance
            best_index = i + 1  # è¿”å›ä¸‹ä¸€ä¸ªæ®µè½çš„ç´¢å¼•
    
    return min(best_index, len(segments))

def merge_overlapping_segments(all_segments: list, chunk_boundaries: list, overlap_seconds: float) -> list:
    """åˆå¹¶é‡å åŒºåŸŸçš„segmentsï¼Œå»é™¤é‡å¤å†…å®¹"""
    if not ENABLE_OVERLAP_CHUNKING or len(chunk_boundaries) <= 1:
        return all_segments
    
    merged_segments = []
    current_chunk_segments = []
    current_chunk_index = 0
    
    print(f"å¼€å§‹åˆå¹¶ {len(all_segments)} ä¸ªsegmentsï¼Œchunkè¾¹ç•Œ: {chunk_boundaries}")
    
    for segment in all_segments:
        segment_start = segment['start']
        segment_end = segment['end']
        
        # ç¡®å®šå½“å‰segmentå±äºå“ªä¸ªchunk
        while (current_chunk_index < len(chunk_boundaries) - 1 and 
               segment_start >= chunk_boundaries[current_chunk_index + 1] - overlap_seconds):
            # å¤„ç†å‰ä¸€ä¸ªchunkçš„segments
            if current_chunk_segments:
                # å¤„ç†é‡å åŒºåŸŸ
                overlap_start = chunk_boundaries[current_chunk_index + 1] - overlap_seconds
                processed_segments = process_chunk_segments(
                    current_chunk_segments, overlap_start, overlap_seconds
                )
                merged_segments.extend(processed_segments)
                current_chunk_segments = []
            
            current_chunk_index += 1
        
        current_chunk_segments.append(segment)
    
    # å¤„ç†æœ€åä¸€ä¸ªchunk
    if current_chunk_segments:
        merged_segments.extend(current_chunk_segments)
    
    print(f"åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆ {len(merged_segments)} ä¸ªsegments")
    return merged_segments

def process_chunk_segments(segments: list, overlap_start: float, overlap_seconds: float) -> list:
    """å¤„ç†å•ä¸ªchunkçš„segmentsï¼Œå¤„ç†é‡å åŒºåŸŸ"""
    if not segments:
        return []
    
    processed = []
    overlap_end = overlap_start + overlap_seconds
    
    for segment in segments:
        segment_start = segment['start']
        segment_end = segment['end']
        
        # å¦‚æœsegmentå®Œå…¨åœ¨é‡å åŒºåŸŸä¹‹å‰ï¼Œç›´æ¥æ·»åŠ 
        if segment_end <= overlap_start:
            processed.append(segment)
        # å¦‚æœsegmentè·¨è¶Šé‡å åŒºåŸŸå¼€å§‹ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æˆªæ–­
        elif segment_start < overlap_start < segment_end:
            # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­ä¸­é—´æˆªæ–­
            text = segment.get('segment', '').strip()
            if text and not any(punct in text for punct in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']):
                # åœ¨å¥å­ä¸­é—´ï¼Œä¿ç•™å®Œæ•´segment
                processed.append(segment)
            else:
                # å¯ä»¥å®‰å…¨æˆªæ–­çš„å¥å­ç»“æŸ
                processed.append(segment)
        
    return processed

def create_overlap_chunks(total_duration: float, chunk_duration: float, overlap_seconds: float) -> list:
    """åˆ›å»ºå¸¦é‡å çš„chunkæ—¶é—´æ®µ"""
    chunks = []
    current_start = 0.0
    
    while current_start < total_duration:
        chunk_end = min(current_start + chunk_duration, total_duration)
        
        chunk_info = {
            'start': current_start,
            'end': chunk_end,
            'duration': chunk_end - current_start
        }
        chunks.append(chunk_info)
        
        # ä¸‹ä¸€ä¸ªchunkçš„å¼€å§‹æ—¶é—´ï¼ˆè€ƒè™‘é‡å ï¼‰
        if chunk_end >= total_duration:
            break
            
        current_start = chunk_end - overlap_seconds
        
    print(f"åˆ›å»ºäº† {len(chunks)} ä¸ªé‡å chunks:")
    for i, chunk in enumerate(chunks):
        print(f"  Chunk {i}: {chunk['start']:.1f}s - {chunk['end']:.1f}s (æ—¶é•¿: {chunk['duration']:.1f}s)")
    
    return chunks

def check_cuda_compatibility():
    """æ£€æŸ¥CUDAå…¼å®¹æ€§ï¼Œå¦‚æœä¸å…¼å®¹åˆ™ç¦ç”¨CUDA"""
    global cuda_available
    
    try:
        if not torch.cuda.is_available():
            print("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            cuda_available = False
            return False
        
        # å°è¯•è·å–è®¾å¤‡æ•°é‡æ¥æµ‹è¯•CUDAå…¼å®¹æ€§
        device_count = torch.cuda.device_count()
        if device_count == 0:
            print("æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            cuda_available = False
            return False
            
        # å°è¯•è·å–è®¾å¤‡å±æ€§æ¥è¿›ä¸€æ­¥æµ‹è¯•å…¼å®¹æ€§
        device_props = torch.cuda.get_device_properties(0)
        print(f"âœ… æ£€æµ‹åˆ°å…¼å®¹çš„GPU: {device_props.name}")
        cuda_available = True
        return True
    except RuntimeError as e:
        if "forward compatibility was attempted on non supported HW" in str(e):
            print("âš ï¸ CUDAå…¼å®¹æ€§é”™è¯¯: GPUç¡¬ä»¶ä¸æ”¯æŒå½“å‰CUDAç‰ˆæœ¬")
            print("è¿™é€šå¸¸æ˜¯å› ä¸ºä¸»æœºçš„GPUé©±åŠ¨ç‰ˆæœ¬è¿‡æ—§ï¼Œä¸æ”¯æŒå®¹å™¨ä¸­çš„CUDA 12.3ç‰ˆæœ¬")
            print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        elif "CUDA" in str(e):
            print(f"âš ï¸ CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
            print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        else:
            print(f"âš ï¸ æœªçŸ¥CUDAé”™è¯¯: {e}")
            print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        
        cuda_available = False
        return False
    except Exception as e:
        print(f"âš ï¸ GPUå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
        print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        cuda_available = False
        return False

def get_gpu_memory_usage():
    """è·å–GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    global cuda_available
    if not cuda_available:
        return 0, 0, 0
    
    try:
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        return allocated, reserved, total
    except Exception as e:
        print(f"âš ï¸ è·å–GPUå†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return 0, 0, 0

def aggressive_memory_cleanup():
    """æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†å‡½æ•°"""
    global cuda_available
    if cuda_available:
        try:
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()
            # åŒæ­¥æ‰€æœ‰CUDAæ“ä½œ
            torch.cuda.synchronize()
            # é‡ç½®å³°å€¼å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
        except Exception as e:
            print(f"âš ï¸ CUDAæ¸…ç†æ“ä½œå¤±è´¥: {e}")
    
    # å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
    for _ in range(3):
        gc.collect()
    
    if cuda_available:
        try:
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

def should_force_cleanup():
    """æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶æ¸…ç†æ˜¾å­˜"""
    global cuda_available
    if not cuda_available:
        return False
    
    allocated, reserved, total = get_gpu_memory_usage()
    usage_ratio = allocated / total if total > 0 else 0
    return usage_ratio > FORCE_CLEANUP_THRESHOLD

def optimize_model_for_inference(model):
    """ä¼˜åŒ–æ¨¡å‹ä»¥å‡å°‘æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨"""
    if model is None:
        return model
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if ENABLE_GRADIENT_CHECKPOINTING and hasattr(model, 'encoder'):
        try:
            if hasattr(model.encoder, 'use_gradient_checkpointing'):
                model.encoder.use_gradient_checkpointing = True
            elif hasattr(model.encoder, 'gradient_checkpointing'):
                model.encoder.gradient_checkpointing = True
        except Exception as e:
            print(f"æ— æ³•å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: {e}")
    
    # ç¦ç”¨è‡ªåŠ¨æ±‚å¯¼ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦æ¢¯åº¦ï¼‰
    for param in model.parameters():
        param.requires_grad = False
    
    return model

def create_streaming_config():
    """åˆ›å»ºæµå¼å¤„ç†é…ç½®ä»¥å‡å°‘æ˜¾å­˜å ç”¨"""
    return {
        'batch_size': 1,  # å•æ‰¹å¤„ç†å‡å°‘æ˜¾å­˜å ç”¨
        'num_workers': 0,  # é¿å…å¤šè¿›ç¨‹å¸¦æ¥çš„é¢å¤–å†…å­˜å¼€é”€
        'pin_memory': False,  # ä¸ä½¿ç”¨é”é¡µå†…å­˜ä»¥èŠ‚çœç³»ç»Ÿå†…å­˜
        'drop_last': False,
        'persistent_workers': False  # ä¸ä¿æŒworkerè¿›ç¨‹
    }

def load_model_if_needed():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œåˆ™è¿›è¡ŒåŠ è½½ã€‚"""
    global asr_model, cuda_available
    # ä½¿ç”¨é”ç¡®ä¿å¤šçº¿ç¨‹ç¯å¢ƒä¸‹æ¨¡å‹åªè¢«åŠ è½½ä¸€æ¬¡
    with model_lock:
        if asr_model is None:
            print("="*50)
            print("æ¨¡å‹å½“å‰æœªåŠ è½½ï¼Œæ­£åœ¨ä»ç£ç›˜åŠ è½½...")
            print("æ¨¡å‹åç§°: nvidia/parakeet-tdt-0.6b-v2")
            try:
                # é¦–å…ˆæ£€æŸ¥CUDAå…¼å®¹æ€§
                cuda_available = check_cuda_compatibility()
                
                # ç¡®ä¿numbaç¼“å­˜ç›®å½•å­˜åœ¨
                numba_cache_dir = os.environ.get('NUMBA_CACHE_DIR', '/tmp/numba_cache')
                if not os.path.exists(numba_cache_dir):
                    os.makedirs(numba_cache_dir, exist_ok=True)
                    os.chmod(numba_cache_dir, 0o777)
                
                model_path = "/app/models/parakeet-tdt-0.6b-v2.nemo"
                if not os.path.exists(model_path):
                    raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}ï¼Œè¯·ç¡®è®¤ models æ–‡ä»¶å¤¹å·²æ­£ç¡®æŒ‚è½½ã€‚")

                # æ£€æŸ¥æ–‡ä»¶æƒé™
                if not os.access(model_path, os.R_OK):
                    raise PermissionError(f"æ— æ³•è¯»å–æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™ã€‚")

                if cuda_available:
                    print(f"âœ… æ£€æµ‹åˆ°å…¼å®¹çš„CUDAç¯å¢ƒï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿå¹¶å¼€å¯åŠç²¾åº¦(FP16)ä¼˜åŒ–ã€‚")
                    
                    # è®¾ç½® Tensor Core ä¼˜åŒ–
                    setup_tensor_core_optimization()
                    optimize_tensor_operations()
                    
                    # æ˜¾ç¤º GPU å’Œ Tensor Core ä¿¡æ¯
                    device_info = torch.cuda.get_device_properties(0)
                    print(f"GPU: {device_info.name}")
                    print(f"Tensor Core æ”¯æŒ: {get_tensor_core_info()}")
                    
                    # å…ˆåœ¨CPUä¸ŠåŠ è½½æ¨¡å‹ï¼Œç„¶åè½¬ç§»åˆ°GPUå¹¶å¯ç”¨FP16
                    loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=model_path, map_location=torch.device('cpu'))
                    loaded_model = loaded_model.cuda()
                    loaded_model = loaded_model.half()
                    
                    # åº”ç”¨æ¨ç†ä¼˜åŒ–
                    loaded_model = optimize_model_for_inference(loaded_model)
                    
                    # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                    allocated, reserved, total = get_gpu_memory_usage()
                    print(f"æ¨¡å‹åŠ è½½åæ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)")
                else:
                    print("ğŸ”„ ä½¿ç”¨ CPU æ¨¡å¼è¿è¡Œã€‚")
                    print("æ³¨æ„: CPUæ¨¡å¼ä¸‹æ¨ç†é€Ÿåº¦ä¼šè¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨å…¼å®¹çš„GPUã€‚")
                    loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=model_path)
                    loaded_model = optimize_model_for_inference(loaded_model)
                
                asr_model = loaded_model
                print("âœ… NeMo ASR æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                print("="*50)
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("="*50)
                import traceback
                traceback.print_exc()
                # å‘ä¸ŠæŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿æ¥å£å¯ä»¥æ•è·å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
                raise e
    return asr_model

def unload_model():
    """ä»å†…å­˜/æ˜¾å­˜ä¸­å¸è½½æ¨¡å‹ã€‚"""
    global asr_model, last_request_time, cuda_available
    with model_lock:
        if asr_model is not None:
            print(f"æ¨¡å‹é—²ç½®è¶…è¿‡ {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿï¼Œæ­£åœ¨ä»å†…å­˜ä¸­å¸è½½...")
            
            # æ˜¾ç¤ºå¸è½½å‰çš„æ˜¾å­˜ä½¿ç”¨
            if cuda_available:
                allocated_before, _, total = get_gpu_memory_usage()
                print(f"å¸è½½å‰æ˜¾å­˜ä½¿ç”¨: {allocated_before:.2f}GB / {total:.2f}GB")
            
            asr_model = None
            aggressive_memory_cleanup()
            
            # æ˜¾ç¤ºå¸è½½åçš„æ˜¾å­˜ä½¿ç”¨
            if cuda_available:
                allocated_after, _, total = get_gpu_memory_usage()
                print(f"å¸è½½åæ˜¾å­˜ä½¿ç”¨: {allocated_after:.2f}GB / {total:.2f}GB")
                print(f"é‡Šæ”¾æ˜¾å­˜: {allocated_before - allocated_after:.2f}GB")
            
            last_request_time = None # é‡ç½®è®¡æ—¶å™¨ï¼Œé˜²æ­¢é‡å¤å¸è½½
            print("âœ… æ¨¡å‹å·²æˆåŠŸå¸è½½ã€‚")

def model_cleanup_checker():
    """åå°çº¿ç¨‹ï¼Œå‘¨æœŸæ€§æ£€æŸ¥æ¨¡å‹æ˜¯å¦é—²ç½®è¿‡ä¹…å¹¶æ‰§è¡Œå¸è½½ã€‚"""
    while True:
        # æ¯ 60 ç§’æ£€æŸ¥ä¸€æ¬¡
        time.sleep(60)
        if asr_model is not None and last_request_time is not None:
            idle_duration = (datetime.datetime.now() - last_request_time).total_seconds()
            if idle_duration > IDLE_TIMEOUT_MINUTES * 60:
                unload_model()


# --- Flask åº”ç”¨åˆå§‹åŒ– ---
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = '/app/temp_uploads'
app.config['MAX_CONTENT_LENGTH'] = 2000 * 1024 * 1024  

# --- è¾…åŠ©å‡½æ•° ---
def get_audio_duration(file_path: str) -> float:
    """ä½¿ç”¨ ffprobe è·å–éŸ³é¢‘æ–‡ä»¶çš„æ—¶é•¿ï¼ˆç§’ï¼‰"""
    command = [
        'ffprobe',
        '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        file_path
    ]
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        return float(result.stdout)
    except (subprocess.CalledProcessError, ValueError) as e:
        print(f"æ— æ³•è·å–æ–‡ä»¶ '{file_path}' çš„æ—¶é•¿: {e}")
        return 0.0

def format_srt_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º SRT æ—¶é—´æˆ³æ ¼å¼ HH:MM:SS,ms"""
    delta = datetime.timedelta(seconds=seconds)
    # æ ¼å¼åŒ–ä¸º 0:00:05.123000
    s = str(delta)
    # åˆ†å‰²ç§’å’Œå¾®ç§’
    if '.' in s:
        parts = s.split('.')
        integer_part = parts[0]
        fractional_part = parts[1][:3] # å–å‰ä¸‰ä½æ¯«ç§’
    else:
        integer_part = s
        fractional_part = "000"

    # å¡«å……å°æ—¶ä½
    if len(integer_part.split(':')) == 2:
        integer_part = "0:" + integer_part
    
    return f"{integer_part},{fractional_part}"


def segments_to_srt(segments: list) -> str:
    """å°† NeMo çš„åˆ†æ®µæ—¶é—´æˆ³è½¬æ¢ä¸º SRT æ ¼å¼å­—ç¬¦ä¸²"""
    srt_content = []
    for i, segment in enumerate(segments):
        start_time = format_srt_time(segment['start'])
        end_time = format_srt_time(segment['end'])
        text = segment['segment'].strip()
        
        if text: # ä»…æ·»åŠ æœ‰å†…å®¹çš„å­—å¹•
            srt_content.append(str(i + 1))
            srt_content.append(f"{start_time} --> {end_time}")
            srt_content.append(text)
            srt_content.append("") # ç©ºè¡Œåˆ†éš”
            
    return "\n".join(srt_content)

# --- Flask è·¯ç”± ---

@app.route('/v1/audio/transcriptions', methods=['POST'])
def transcribe_audio():
    """
    å…¼å®¹ OpenAI çš„è¯­éŸ³è¯†åˆ«æ¥å£ï¼Œæ”¯æŒé•¿éŸ³é¢‘åˆ†ç‰‡å¤„ç†ã€‚
    """
    # --- -1. API Key è®¤è¯ ---
    if API_KEY:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({"error": "Authorization header is missing or invalid. It must be in 'Bearer <key>' format."}), 401
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != API_KEY:
            return jsonify({"error": "Invalid API key."}), 401

    # --- 0. ç¡®ä¿æ¨¡å‹åŠ è½½å¹¶æ›´æ–°æ—¶é—´æˆ³ ---
    try:
        # å¦‚æœæ‡’åŠ è½½å¯ç”¨ï¼Œåˆ™æŒ‰éœ€åŠ è½½ï¼›å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„å…¨å±€æ¨¡å‹
        local_asr_model = load_model_if_needed() if ENABLE_LAZY_LOAD else asr_model
        if not local_asr_model:
             # æ­¤æƒ…å†µæ¶µç›–äº†æ‡’åŠ è½½å¤±è´¥å’Œé¢„åŠ è½½å¤±è´¥ä¸¤ç§åœºæ™¯
             return jsonify({"error": "æ¨¡å‹åŠ è½½å¤±è´¥æˆ–å°šæœªåŠ è½½ï¼Œæ— æ³•å¤„ç†è¯·æ±‚"}), 500
    except Exception as e:
        return jsonify({"error": f"æ¨¡å‹åŠ è½½æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"}), 500
    
    # å¦‚æœå¯ç”¨äº†æ‡’åŠ è½½ï¼Œåˆ™æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
    if ENABLE_LAZY_LOAD:
        global last_request_time
        last_request_time = datetime.datetime.now()


    # --- 1. åŸºæœ¬æ ¡éªŒ ---
    if 'file' not in request.files:
        return jsonify({"error": "è¯·æ±‚ä¸­æœªæ‰¾åˆ°æ–‡ä»¶éƒ¨åˆ†"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    if not shutil.which('ffmpeg'):
        return jsonify({"error": "FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿ PATH ä¸­"}), 500
    if not shutil.which('ffprobe'):
        return jsonify({"error": "ffprobe æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿ PATH ä¸­"}), 500

    # è·å–è¯·æ±‚å‚æ•°
    model_name = request.form.get('model', 'whisper-1')
    response_format = request.form.get('response_format', 'json')  # æ”¯æŒ json, text, srt, verbose_json, vtt
    language = request.form.get('language', None)
    prompt = request.form.get('prompt', None)
    temperature = float(request.form.get('temperature', 0))
    
    print(f"æ¥æ”¶åˆ°è¯·æ±‚ï¼Œæ¨¡å‹: '{model_name}', å“åº”æ ¼å¼: '{response_format}'")

    original_filename = secure_filename(file.filename)
    unique_id = str(uuid.uuid4())
    temp_original_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_{original_filename}")
    target_wav_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}.wav")
    
    # ç”¨äºæ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶çš„åˆ—è¡¨
    temp_files_to_clean = []

    try:
        # --- 2. ä¿å­˜å¹¶ç»Ÿä¸€è½¬æ¢ä¸º 16k å•å£°é“ WAV ---
        file.save(temp_original_path)
        temp_files_to_clean.append(temp_original_path)
        
        print(f"[{unique_id}] æ­£åœ¨å°† '{original_filename}' è½¬æ¢ä¸ºæ ‡å‡† WAV æ ¼å¼...")
        ffmpeg_command = [
            'ffmpeg', '-y', '-i', temp_original_path,
            '-ac', '1', '-ar', '16000', target_wav_path
        ]
        result = subprocess.run(ffmpeg_command, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"FFmpeg é”™è¯¯: {result.stderr}")
            return jsonify({"error": "æ–‡ä»¶è½¬æ¢å¤±è´¥", "details": result.stderr}), 500
        temp_files_to_clean.append(target_wav_path)

        # --- 3. éŸ³é¢‘åˆ‡ç‰‡ (Chunking) ---
        # åŠ¨æ€è°ƒæ•´chunkå¤§å°åŸºäºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if cuda_available:
            allocated, _, total = get_gpu_memory_usage()
            memory_usage_ratio = allocated / total if total > 0 else 0
            
            if memory_usage_ratio > 0.6:  # å¦‚æœæ˜¾å­˜ä½¿ç”¨è¶…è¿‡60%
                # å‡å°‘chunkå¤§å°ä»¥é™ä½æ˜¾å­˜å‹åŠ›
                adjusted_chunk_minutes = max(3, CHUNK_MINITE - 2)
                print(f"[{unique_id}] æ˜¾å­˜ä½¿ç”¨è¾ƒé«˜({memory_usage_ratio*100:.1f}%)ï¼Œè°ƒæ•´chunkå¤§å°ä» {CHUNK_MINITE} åˆ†é’Ÿåˆ° {adjusted_chunk_minutes} åˆ†é’Ÿ")
                CHUNK_DURATION_SECONDS = adjusted_chunk_minutes * 60
            else:
                CHUNK_DURATION_SECONDS = CHUNK_MINITE * 60
        else:
            # CPUæ¨¡å¼ä¸‹ä½¿ç”¨è¾ƒå°çš„chunkä»¥é¿å…å†…å­˜ä¸è¶³
            cpu_chunk_minutes = max(3, CHUNK_MINITE // 2)  # CPUæ¨¡å¼å‡åŠchunkå¤§å°
            print(f"[{unique_id}] CPUæ¨¡å¼ï¼Œè°ƒæ•´chunkå¤§å°åˆ° {cpu_chunk_minutes} åˆ†é’Ÿ")
            CHUNK_DURATION_SECONDS = cpu_chunk_minutes * 60
            
        total_duration = get_audio_duration(target_wav_path)
        if total_duration == 0:
            return jsonify({"error": "æ— æ³•å¤„ç†æ—¶é•¿ä¸º0çš„éŸ³é¢‘"}), 400

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡ç‰‡ï¼Œå¦‚æœéŸ³é¢‘æ—¶é•¿å°äºåˆ‡ç‰‡é˜ˆå€¼ï¼Œåˆ™ç›´æ¥å¤„ç†
        if total_duration <= CHUNK_DURATION_SECONDS:
            print(f"[{unique_id}] æ–‡ä»¶æ€»æ—¶é•¿: {total_duration:.2f}s. å°äºåˆ‡ç‰‡é˜ˆå€¼({CHUNK_DURATION_SECONDS}s)ï¼Œæ— éœ€åˆ‡ç‰‡ã€‚")
            chunk_paths = [target_wav_path]
            chunk_info_list = [{'start': 0, 'end': total_duration, 'duration': total_duration}]
            num_chunks = 1
        else:
            # ä½¿ç”¨é‡å åˆ†å‰²ç­–ç•¥
            if ENABLE_OVERLAP_CHUNKING:
                print(f"[{unique_id}] å¯ç”¨é‡å åˆ†å‰²æ¨¡å¼ï¼Œé‡å æ—¶é•¿: {CHUNK_OVERLAP_SECONDS}s")
                chunk_info_list = create_overlap_chunks(total_duration, CHUNK_DURATION_SECONDS, CHUNK_OVERLAP_SECONDS)
            else:
                # ä¼ ç»Ÿç¡¬åˆ†å‰²
                num_chunks = math.ceil(total_duration / CHUNK_DURATION_SECONDS)
                chunk_info_list = []
                for i in range(num_chunks):
                    start_time = i * CHUNK_DURATION_SECONDS
                    end_time = min(start_time + CHUNK_DURATION_SECONDS, total_duration)
                    chunk_info_list.append({
                        'start': start_time,
                        'end': end_time, 
                        'duration': end_time - start_time
                    })
            
            chunk_paths = []
            num_chunks = len(chunk_info_list)
            print(f"[{unique_id}] æ–‡ä»¶æ€»æ—¶é•¿: {total_duration:.2f}s. å°†åˆ‡åˆ†ä¸º {num_chunks} ä¸ªç‰‡æ®µã€‚")
            
            for i, chunk_info in enumerate(chunk_info_list):
                chunk_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_chunk_{i}.wav")
                chunk_paths.append(chunk_path)
                temp_files_to_clean.append(chunk_path)
                
                start_time = chunk_info['start']
                duration = chunk_info['duration']
                
                print(f"[{unique_id}] æ­£åœ¨åˆ›å»ºåˆ‡ç‰‡ {i+1}/{num_chunks} ({start_time:.1f}s - {chunk_info['end']:.1f}s)...")
                chunk_command = [
                    'ffmpeg', '-y', '-i', target_wav_path,
                    '-ss', str(start_time),
                    '-t', str(duration),
                    '-c', 'copy',
                    chunk_path
                ]
                result = subprocess.run(chunk_command, capture_output=True, text=True)
                if result.returncode != 0:
                    print(f"[{unique_id}] âš ï¸ åˆ›å»ºåˆ‡ç‰‡ {i+1} æ—¶å‡ºç°è­¦å‘Š: {result.stderr}")
                    # ç»§ç»­å¤„ç†ï¼Œä¸ä¸­æ–­
            
        # --- 4. å¾ªç¯è½¬å½•å¹¶åˆå¹¶ç»“æœ ---
        all_segments = []
        all_words = []
        chunk_boundaries = []

        for i, (chunk_path, chunk_info) in enumerate(zip(chunk_paths, chunk_info_list)):
            print(f"[{unique_id}] æ­£åœ¨è½¬å½•åˆ‡ç‰‡ {i+1}/{num_chunks}...")
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœè¿‡é«˜åˆ™å¼ºåˆ¶æ¸…ç†
            if should_force_cleanup():
                print(f"[{unique_id}] æ˜¾å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†...")
                aggressive_memory_cleanup()
            
            # æ˜¾ç¤ºå½“å‰æ˜¾å­˜/å†…å­˜ä½¿ç”¨
            if cuda_available:
                allocated, _, total = get_gpu_memory_usage()
                print(f"[{unique_id}] å¤„ç†åˆ‡ç‰‡ {i+1} å‰æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.2f}GB")
            else:
                # æ˜¾ç¤ºCPUå†…å­˜ä½¿ç”¨
                memory = psutil.virtual_memory()
                print(f"[{unique_id}] å¤„ç†åˆ‡ç‰‡ {i+1} å‰å†…å­˜ä½¿ç”¨: {memory.used/1024**3:.2f}GB / {memory.total/1024**3:.2f}GB ({memory.percent:.1f}%)")
            
            # å¯¹å½“å‰åˆ‡ç‰‡è¿›è¡Œè½¬å½•
            # ä½¿ç”¨ with torch.cuda.amp.autocast() åœ¨åŠç²¾åº¦ä¸‹è¿è¡Œæ¨ç†
            with torch.no_grad():  # ç¡®ä¿ä¸è®¡ç®—æ¢¯åº¦
                if cuda_available:
                    with torch.cuda.amp.autocast(dtype=torch.float16):
                        output = local_asr_model.transcribe([chunk_path], timestamps=True)
                else:
                    # CPUæ¨¡å¼ä¸‹ç›´æ¥è½¬å½•
                    output = local_asr_model.transcribe([chunk_path], timestamps=True)

            # ç«‹å³è¿›è¡Œå†…å­˜æ¸…ç†
            if AGGRESSIVE_MEMORY_CLEANUP:
                aggressive_memory_cleanup()
            else:
                if cuda_available:
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass
                gc.collect()
            
            # è®°å½•chunkè¾¹ç•Œç”¨äºåç»­åˆå¹¶
            chunk_start_offset = chunk_info['start']
            chunk_boundaries.append(chunk_start_offset)
            
            if output and output[0].timestamp:
                # ä¿®æ­£å¹¶æ”¶é›† segment æ—¶é—´æˆ³ï¼ˆä½¿ç”¨chunkåœ¨åŸéŸ³é¢‘ä¸­çš„çœŸå®èµ·å§‹æ—¶é—´ï¼‰
                if 'segment' in output[0].timestamp:
                    for seg in output[0].timestamp['segment']:
                        seg['start'] += chunk_start_offset
                        seg['end'] += chunk_start_offset
                        all_segments.append(seg)
                
                # ä¿®æ­£å¹¶æ”¶é›† word æ—¶é—´æˆ³
                if 'word' in output[0].timestamp:
                     for word in output[0].timestamp['word']:
                        word['start'] += chunk_start_offset
                        word['end'] += chunk_start_offset
                        all_words.append(word)
            
            # ç«‹å³åˆ é™¤å·²å¤„ç†çš„chunkæ–‡ä»¶ä»¥èŠ‚çœç£ç›˜ç©ºé—´å’Œå†…å­˜
            if num_chunks > 1 and os.path.exists(chunk_path):
                try:
                    os.remove(chunk_path)
                    temp_files_to_clean.remove(chunk_path)
                    print(f"[{unique_id}] å·²åˆ é™¤å¤„ç†å®Œæˆçš„åˆ‡ç‰‡æ–‡ä»¶: chunk_{i}")
                except Exception as e:
                    print(f"[{unique_id}] åˆ é™¤åˆ‡ç‰‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        print(f"[{unique_id}] æ‰€æœ‰åˆ‡ç‰‡è½¬å½•å®Œæˆï¼Œæ­£åœ¨åˆå¹¶ç»“æœã€‚")
        
        # --- 4.5. å¤„ç†é‡å åŒºåŸŸå¹¶åˆå¹¶segments ---
        if ENABLE_OVERLAP_CHUNKING and len(chunk_boundaries) > 1:
            print(f"[{unique_id}] å¤„ç†é‡å åŒºåŸŸï¼Œå»é™¤é‡å¤å†…å®¹...")
            all_segments = merge_overlapping_segments(all_segments, chunk_boundaries, CHUNK_OVERLAP_SECONDS)
            print(f"[{unique_id}] é‡å å¤„ç†å®Œæˆï¼Œæœ€ç»ˆsegmentsæ•°é‡: {len(all_segments)}")

        # --- 5. æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º ---
        if not all_segments:
            return jsonify({"error": "è½¬å½•å¤±è´¥ï¼Œæ¨¡å‹æœªè¿”å›ä»»ä½•æœ‰æ•ˆå†…å®¹"}), 500

        # æ„å»ºå®Œæ•´çš„è½¬å½•æ–‡æœ¬
        full_text = " ".join([seg['segment'].strip() for seg in all_segments if seg['segment'].strip()])
        
        # æ ¹æ® response_format è¿”å›ä¸åŒæ ¼å¼
        if response_format == 'text':
            return Response(full_text, mimetype='text/plain')
        elif response_format == 'srt':
            srt_result = segments_to_srt(all_segments)
            return Response(srt_result, mimetype='text/plain')
        elif response_format == 'vtt':
            vtt_result = segments_to_vtt(all_segments)
            return Response(vtt_result, mimetype='text/plain')
        elif response_format == 'verbose_json':
            # è¯¦ç»†çš„ JSON æ ¼å¼ï¼ŒåŒ…å«æ›´å¤šä¿¡æ¯
            response_data = {
                "task": "transcribe",
                "language": language or "en",
                "duration": total_duration,
                "text": full_text,
                "segments": [
                    {
                        "id": i,
                        "seek": int(seg['start'] * 100),  # è½¬æ¢ä¸º centiseconds
                        "start": seg['start'],
                        "end": seg['end'],
                        "text": seg['segment'].strip(),
                        "tokens": [],  # NeMo ä¸æä¾› tokensï¼Œç•™ç©º
                        "temperature": temperature,
                        "avg_logprob": -0.5,  # æ¨¡æ‹Ÿå€¼
                        "compression_ratio": 1.0,  # æ¨¡æ‹Ÿå€¼
                        "no_speech_prob": 0.0,  # æ¨¡æ‹Ÿå€¼
                        "words": [
                            {
                                "word": word['word'],
                                "start": word['start'],
                                "end": word['end'],
                                "probability": 0.9  # æ¨¡æ‹Ÿå€¼
                            }
                            for word in all_words 
                            if word['start'] >= seg['start'] and word['end'] <= seg['end']
                        ] if all_words else []
                    }
                    for i, seg in enumerate(all_segments) if seg['segment'].strip()
                ]
            }
            return jsonify(response_data)
        else:
            # é»˜è®¤ JSON æ ¼å¼ (response_format == 'json')
            response_data = {
                "text": full_text
            }
            return jsonify(response_data)

    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", "details": str(e)}), 500
    finally:
        # --- 6. æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ ---
        print(f"[{unique_id}] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for f_path in temp_files_to_clean:
            if os.path.exists(f_path):
                os.remove(f_path)
        print(f"[{unique_id}] ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
        
        # --- 7. å¼ºåˆ¶æ¸…ç†å†…å­˜ï¼Œé¿å…ç´¯ç§¯ ---
        print(f"[{unique_id}] æ‰§è¡Œæœ€ç»ˆå†…å­˜æ¸…ç†...")
        if cuda_available:
            allocated_before, _, total = get_gpu_memory_usage()
            print(f"[{unique_id}] æ¸…ç†å‰æ˜¾å­˜ä½¿ç”¨: {allocated_before:.2f}GB / {total:.2f}GB")
        else:
            memory_before = psutil.virtual_memory()
            print(f"[{unique_id}] æ¸…ç†å‰å†…å­˜ä½¿ç”¨: {memory_before.used/1024**3:.2f}GB / {memory_before.total/1024**3:.2f}GB")
        
        aggressive_memory_cleanup()
        
        if cuda_available:
            allocated_after, _, total = get_gpu_memory_usage()
            print(f"[{unique_id}] æ¸…ç†åæ˜¾å­˜ä½¿ç”¨: {allocated_after:.2f}GB / {total:.2f}GB")
            if allocated_before > 0:
                print(f"[{unique_id}] é‡Šæ”¾æ˜¾å­˜: {allocated_before - allocated_after:.2f}GB")
        else:
            memory_after = psutil.virtual_memory()
            print(f"[{unique_id}] æ¸…ç†åå†…å­˜ä½¿ç”¨: {memory_after.used/1024**3:.2f}GB / {memory_after.total/1024**3:.2f}GB")
        print(f"[{unique_id}] å†…å­˜æ¸…ç†å®Œæˆã€‚")


def segments_to_vtt(segments: list) -> str:
    """å°† NeMo çš„åˆ†æ®µæ—¶é—´æˆ³è½¬æ¢ä¸º VTT æ ¼å¼å­—ç¬¦ä¸²"""
    vtt_content = ["WEBVTT", ""]
    
    for i, segment in enumerate(segments):
        start_time = format_vtt_time(segment['start'])
        end_time = format_vtt_time(segment['end'])
        text = segment['segment'].strip()
        
        if text:  # ä»…æ·»åŠ æœ‰å†…å®¹çš„å­—å¹•
            vtt_content.append(f"{start_time} --> {end_time}")
            vtt_content.append(text)
            vtt_content.append("")  # ç©ºè¡Œåˆ†éš”
            
    return "\n".join(vtt_content)


def format_vtt_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º VTT æ—¶é—´æˆ³æ ¼å¼ HH:MM:SS.mmm"""
    delta = datetime.timedelta(seconds=seconds)
    # æ ¼å¼åŒ–ä¸º 0:00:05.123000
    s = str(delta)
    # åˆ†å‰²ç§’å’Œå¾®ç§’
    if '.' in s:
        parts = s.split('.')
        integer_part = parts[0]
        fractional_part = parts[1][:3]  # å–å‰ä¸‰ä½æ¯«ç§’
    else:
        integer_part = s
        fractional_part = "000"

    # å¡«å……å°æ—¶ä½
    if len(integer_part.split(':')) == 2:
        integer_part = "0:" + integer_part
    
    return f"{integer_part}.{fractional_part}"

# --- Waitress æœåŠ¡å™¨å¯åŠ¨ ---
if __name__ == '__main__':
    
    # æ ¹æ®æ˜¯å¦å¯ç”¨æ‡’åŠ è½½æ¥å†³å®šæ˜¯é¢„åŠ è½½æ¨¡å‹è¿˜æ˜¯å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
    if ENABLE_LAZY_LOAD:
        print("æ‡’åŠ è½½æ¨¡å¼å·²å¯ç”¨ã€‚æ¨¡å‹å°†åœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶åŠ è½½ã€‚")
        # å¯åŠ¨åå°çº¿ç¨‹æ¥ç›‘æ§å’Œå¸è½½é—²ç½®çš„æ¨¡å‹
        if IDLE_TIMEOUT_MINUTES > 0:
            print(f"å°†å¯ç”¨æ¨¡å‹è‡ªåŠ¨å¸è½½åŠŸèƒ½ï¼Œé—²ç½®è¶…æ—¶: {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿã€‚")
            cleanup_thread = threading.Thread(target=model_cleanup_checker, daemon=True)
            cleanup_thread.start()
        else:
            print("æ¨¡å‹è‡ªåŠ¨å¸è½½åŠŸèƒ½å·²ç¦ç”¨ (IDLE_TIMEOUT_MINUTES=0)ã€‚")
    else:
        # æ‡’åŠ è½½è¢«ç¦ç”¨ï¼Œåœ¨å¯åŠ¨æ—¶ç›´æ¥åŠ è½½æ¨¡å‹
        print("æ‡’åŠ è½½æ¨¡å¼å·²ç¦ç”¨ï¼Œæ­£åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹...")
        try:
            load_model_if_needed()
        except Exception as e:
            print(f"âŒ å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            # é¢„åŠ è½½å¤±è´¥æ—¶é€€å‡ºï¼Œä»¥é¿å…è¿è¡Œä¸€ä¸ªæŸåçš„æœåŠ¡
            exit(1)

    if API_KEY:
        print(f"API Key è®¤è¯å·²å¯ç”¨ã€‚è¯·åœ¨è¯·æ±‚å¤´ä¸­æä¾› 'Authorization: Bearer YOUR_API_KEY'")
    else:
        print("API Key è®¤è¯å·²ç¦ç”¨ï¼Œä»»ä½•è¯·æ±‚éƒ½å°†è¢«æ¥å—ã€‚")


    print(f"ğŸš€ æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print(f"API ç«¯ç‚¹: POST http://{host}:{port}/v1/audio/transcriptions")
    print(f"æœåŠ¡å°†ä½¿ç”¨ {threads} ä¸ªçº¿ç¨‹è¿è¡Œã€‚")
    print("")
    print("=== æ˜¾å­˜ä¼˜åŒ–é…ç½® ===")
    print(f"æ¿€è¿›æ˜¾å­˜æ¸…ç†: {'å¯ç”¨' if AGGRESSIVE_MEMORY_CLEANUP else 'ç¦ç”¨'}")
    print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {'å¯ç”¨' if ENABLE_GRADIENT_CHECKPOINTING else 'ç¦ç”¨'}")
    print(f"å¼ºåˆ¶æ¸…ç†é˜ˆå€¼: {FORCE_CLEANUP_THRESHOLD*100:.0f}%")
    print(f"æœ€å¤§chunkå†…å­˜: {MAX_CHUNK_MEMORY_MB}MB")
    print(f"é»˜è®¤chunkæ—¶é•¿: {CHUNK_MINITE} åˆ†é’Ÿ")
    # åˆå§‹åŒ–CUDAå…¼å®¹æ€§æ£€æŸ¥
    print("æ­£åœ¨æ£€æŸ¥CUDAå…¼å®¹æ€§...")
    cuda_available = check_cuda_compatibility()
    
    if cuda_available:
        _, _, total_memory = get_gpu_memory_usage()
        print(f"GPUæ€»æ˜¾å­˜: {total_memory:.1f}GB")
    else:
        memory = psutil.virtual_memory()
        print(f"ç³»ç»Ÿå†…å­˜: {memory.total/1024**3:.1f}GB")
    print("=" * 25)
    print("")
    print("=== Tensor Core é…ç½® ===")
    print(f"Tensor Core: {'å¯ç”¨' if ENABLE_TENSOR_CORE else 'ç¦ç”¨'}")
    print(f"cuDNN Benchmark: {'å¯ç”¨' if ENABLE_CUDNN_BENCHMARK else 'ç¦ç”¨'}")
    print(f"ç²¾åº¦æ¨¡å¼: {TENSOR_CORE_PRECISION}")
    if cuda_available:
        print(f"GPUæ”¯æŒ: {get_tensor_core_info()}")
    else:
        print("GPUæ”¯æŒ: N/A - CUDAä¸å¯ç”¨æˆ–ä¸å…¼å®¹")
    print("=" * 25)
    print("")
    print("=== å¥å­å®Œæ•´æ€§ä¼˜åŒ– ===")
    print(f"é‡å åˆ†å‰²: {'å¯ç”¨' if ENABLE_OVERLAP_CHUNKING else 'ç¦ç”¨'}")
    if ENABLE_OVERLAP_CHUNKING:
        print(f"é‡å æ—¶é•¿: {CHUNK_OVERLAP_SECONDS}s")
        print(f"è¾¹ç•Œé˜ˆå€¼: {SENTENCE_BOUNDARY_THRESHOLD}")
    print("=" * 25)
    serve(app, host=host, port=port, threads=threads)