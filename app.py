import os,sys,json,math

# è®¾ç½®ç¯å¢ƒå˜é‡æ¥è§£å†³numbaç¼“å­˜é—®é¢˜
os.environ['NUMBA_CACHE_DIR'] = '/tmp/numba_cache'
os.environ['NUMBA_DISABLE_JIT'] = '0'

# è®¾ç½®matplotlibé…ç½®ç›®å½•ï¼Œé¿å…æƒé™é—®é¢˜
# ä¼˜å…ˆä½¿ç”¨å¯åŠ¨è„šæœ¬è®¾ç½®çš„ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å¤‡ç”¨ç›®å½•
if 'MPLCONFIGDIR' not in os.environ:
    os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib_config'
    os.makedirs('/tmp/matplotlib_config', exist_ok=True)
    os.chmod('/tmp/matplotlib_config', 0o777)
else:
    # ç¡®ä¿å·²è®¾ç½®çš„ç›®å½•å­˜åœ¨ä¸”æœ‰æ­£ç¡®æƒé™
    mpl_dir = os.environ['MPLCONFIGDIR']
    try:
        os.makedirs(mpl_dir, exist_ok=True)
        os.chmod(mpl_dir, 0o755)
    except (PermissionError, OSError):
        # å¦‚æœæ— æ³•åˆ›å»ºæˆ–è®¾ç½®æƒé™ï¼Œå›é€€åˆ°tmpç›®å½•
        os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib_config'
        os.makedirs('/tmp/matplotlib_config', exist_ok=True)
        os.chmod('/tmp/matplotlib_config', 0o777)

host = '0.0.0.0'
port = 5092
threads = 4
# é»˜è®¤æŒ‰ç…§Nåˆ†é’Ÿå°†éŸ³è§†é¢‘è£åˆ‡ä¸ºå¤šæ®µï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚ç°åœ¨å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ CHUNK_MINITE æ¥è°ƒæ•´ã€‚
# 8Gæ˜¾å­˜å»ºè®®è®¾ç½®ä¸º 10-15 åˆ†é’Ÿä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
CHUNK_MINITE = int(os.environ.get('CHUNK_MINITE', '10'))
# æœåŠ¡é—²ç½®Nåˆ†é’Ÿåè‡ªåŠ¨å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜ï¼Œè®¾ç½®ä¸º0åˆ™ç¦ç”¨
IDLE_TIMEOUT_MINUTES = int(os.environ.get('IDLE_TIMEOUT_MINUTES', '30'))
# æ‡’åŠ è½½å¼€å…³ï¼Œé»˜è®¤ä¸º trueã€‚è®¾ç½®ä¸º 'false' å¯åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹ã€‚
ENABLE_LAZY_LOAD = os.environ.get('ENABLE_LAZY_LOAD', 'true').lower() not in ['false', '0', 'f']
# Whisper å…¼å®¹çš„ API Keyã€‚å¦‚æœç•™ç©ºï¼Œåˆ™ä¸è¿›è¡Œèº«ä»½éªŒè¯ã€‚
API_KEY = os.environ.get('API_KEY', None)
import shutil
from typing import Any, Dict
import uuid
import subprocess
import datetime
import threading
import time
from werkzeug.utils import secure_filename

from flask import Flask, request, jsonify, Response
from waitress import serve
from pathlib import Path
# ROOT_DIR is not needed in Docker environment
# ä»…å½“æœªæ˜¾å¼é…ç½®æ—¶æ‰è®¾ç½® HF é•œåƒï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
if 'HF_ENDPOINT' not in os.environ:
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# HF_HOME is set in the Dockerfile
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = 'true'
# PATH for ffmpeg is handled by the Docker image's system PATH

# å‡å°‘ PyTorch CUDA åˆ†é…ç¢ç‰‡ï¼Œé™ä½ OOM å‡ ç‡ï¼ˆå¯é€šè¿‡å¤–éƒ¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
if 'PYTORCH_CUDA_ALLOC_CONF' not in os.environ:
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'

import nemo.collections.asr as nemo_asr  # type: ignore
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import gc
import psutil
try:
    # huggingface_hub may not be present in the editor environment; import defensively
    from huggingface_hub import HfApi, hf_hub_download  # type: ignore
except Exception:
    # Provide fallbacks so static checkers and runtime in minimal environments won't crash.
    HfApi = None  # type: ignore
    def hf_hub_download(*args, **kwargs):
        raise RuntimeError("huggingface_hub is not installed")

# --- å…¨å±€è®¾ç½®ä¸æ¨¡å‹çŠ¶æ€ ---
asr_model = None
last_request_time = None
model_lock = threading.Lock()
cuda_available = False  # å…¨å±€CUDAå…¼å®¹æ€§æ ‡å¿—

# å—æ”¯æŒçš„è¯­è¨€ï¼ˆISO 639-1ï¼Œä¸¤å­—æ¯å°å†™ï¼‰ï¼ŒåŸºäº parakeet-tdt-0.6b-v3 å…¬å‘Š
SUPPORTED_LANG_CODES = {
    'bg','hr','cs','da','nl','en','et','fi','fr','de','el','hu','it','lv','lt','mt','pl','pt','ro','sk','sl','es','sv','ru','uk'
}

# è¯­è¨€è‡ªåŠ¨æ‹’ç»ï¼ˆå½“æœªæ˜¾å¼ä¼ å…¥ language æ—¶ï¼Œå…ˆå¯¹çŸ­ç‰‡æ®µåšè¯­è¨€åˆåˆ¤ï¼›è‹¥ä¸å—æ”¯æŒåˆ™ç›´æ¥è¿”å› Whisper é£æ ¼é”™è¯¯ï¼‰
ENABLE_AUTO_LANGUAGE_REJECTION = os.environ.get('ENABLE_AUTO_LANGUAGE_REJECTION', 'true').lower() in ['true', '1', 't']
LID_CLIP_SECONDS = int(os.environ.get('LID_CLIP_SECONDS', '45'))

# æ¨ç†å¹¶å‘æ§åˆ¶ï¼ˆé¿å…å¤šè¯·æ±‚åŒæ—¶å ç”¨æ˜¾å­˜å¯¼è‡´ OOMï¼‰
MAX_CONCURRENT_INFERENCES = int(os.environ.get('MAX_CONCURRENT_INFERENCES', '1'))
inference_semaphore = threading.Semaphore(MAX_CONCURRENT_INFERENCES)

# æ˜¾å­˜ä¼˜åŒ–é…ç½®
AGGRESSIVE_MEMORY_CLEANUP = os.environ.get('AGGRESSIVE_MEMORY_CLEANUP', 'true').lower() in ['true', '1', 't']
ENABLE_GRADIENT_CHECKPOINTING = os.environ.get('ENABLE_GRADIENT_CHECKPOINTING', 'true').lower() in ['true', '1', 't']
MAX_CHUNK_MEMORY_MB = int(os.environ.get('MAX_CHUNK_MEMORY_MB', '1500'))
FORCE_CLEANUP_THRESHOLD = float(os.environ.get('FORCE_CLEANUP_THRESHOLD', '0.8'))

# é—²ç½®æ—¶èµ„æºä¼˜åŒ–é…ç½®
IDLE_MEMORY_CLEANUP_INTERVAL = int(os.environ.get('IDLE_MEMORY_CLEANUP_INTERVAL', '120'))  # é—²ç½®æ—¶å†…å­˜æ¸…ç†é—´éš”(ç§’)ï¼Œé»˜è®¤2åˆ†é’Ÿ
IDLE_DEEP_CLEANUP_THRESHOLD = int(os.environ.get('IDLE_DEEP_CLEANUP_THRESHOLD', '600'))  # æ·±åº¦æ¸…ç†é˜ˆå€¼(ç§’)ï¼Œé»˜è®¤10åˆ†é’Ÿ
ENABLE_IDLE_CPU_OPTIMIZATION = os.environ.get('ENABLE_IDLE_CPU_OPTIMIZATION', 'true').lower() in ['true', '1', 't']
IDLE_MONITORING_INTERVAL = int(os.environ.get('IDLE_MONITORING_INTERVAL', '30'))  # é—²ç½®ç›‘æ§é—´éš”(ç§’)ï¼Œé»˜è®¤30ç§’
# è¶…çº§æ¿€è¿›å†…å­˜ä¼˜åŒ–é…ç½®
ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION = os.environ.get('ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION', 'true').lower() in ['true', '1', 't']
IMMEDIATE_CLEANUP_AFTER_REQUEST = os.environ.get('IMMEDIATE_CLEANUP_AFTER_REQUEST', 'true').lower() in ['true', '1', 't']
MEMORY_USAGE_ALERT_THRESHOLD_GB = float(os.environ.get('MEMORY_USAGE_ALERT_THRESHOLD_GB', '6.0'))  # å†…å­˜ä½¿ç”¨è¶…è¿‡6GBæ—¶å‘Šè­¦å¹¶å¼ºåˆ¶æ¸…ç†
AUTO_MODEL_UNLOAD_THRESHOLD_MINUTES = int(os.environ.get('AUTO_MODEL_UNLOAD_THRESHOLD_MINUTES', '10'))  # è‡ªåŠ¨å¸è½½æ¨¡å‹é˜ˆå€¼ï¼Œé»˜è®¤10åˆ†é’Ÿ

# Tensor Core ä¼˜åŒ–é…ç½®
ENABLE_TENSOR_CORE = os.environ.get('ENABLE_TENSOR_CORE', 'true').lower() in ['true', '1', 't']
ENABLE_CUDNN_BENCHMARK = os.environ.get('ENABLE_CUDNN_BENCHMARK', 'true').lower() in ['true', '1', 't']
TENSOR_CORE_PRECISION = os.environ.get('TENSOR_CORE_PRECISION', 'highest')  # highest, high, medium
GPU_MEMORY_FRACTION = float(os.environ.get('GPU_MEMORY_FRACTION', '0.95'))  # è¿›ç¨‹å…è®¸ä½¿ç”¨çš„æ˜¾å­˜æ¯”ä¾‹

# å¥å­å®Œæ•´æ€§ä¼˜åŒ–é…ç½®
ENABLE_OVERLAP_CHUNKING = os.environ.get('ENABLE_OVERLAP_CHUNKING', 'true').lower() in ['true', '1', 't']
CHUNK_OVERLAP_SECONDS = float(os.environ.get('CHUNK_OVERLAP_SECONDS', '30'))  # é‡å æ—¶é•¿
SENTENCE_BOUNDARY_THRESHOLD = float(os.environ.get('SENTENCE_BOUNDARY_THRESHOLD', '0.5'))  # å¥å­è¾¹ç•Œæ£€æµ‹é˜ˆå€¼


# é™éŸ³å¯¹é½åˆ‡ç‰‡ä¸å‰å¤„ç†é…ç½®
ENABLE_SILENCE_ALIGNED_CHUNKING = os.environ.get('ENABLE_SILENCE_ALIGNED_CHUNKING', 'true').lower() in ['true', '1', 't']
SILENCE_THRESHOLD_DB = os.environ.get('SILENCE_THRESHOLD_DB', '-38dB')  # ffmpeg silencedetect å™ªå£°é˜ˆå€¼
MIN_SILENCE_DURATION = float(os.environ.get('MIN_SILENCE_DURATION', '0.35'))  # è®¤ä¸ºæ˜¯é™éŸ³çš„æœ€å°æ—¶é•¿(ç§’)
SILENCE_MAX_SHIFT_SECONDS = float(os.environ.get('SILENCE_MAX_SHIFT_SECONDS', '2.0'))  # ç›®æ ‡åˆ†å‰²ç‚¹é™„è¿‘å…è®¸å‘é™éŸ³å¯¹é½çš„æœ€å¤§åç§»(ç§’)

ENABLE_FFMPEG_DENOISE = os.environ.get('ENABLE_FFMPEG_DENOISE', 'false').lower() in ['true', '1', 't']
# åˆç†çš„é»˜è®¤å»å™ª/å‡è¡¡/åŠ¨æ€èŒƒå›´è®¾ç½®ï¼Œå°½å¯èƒ½æ¸©å’Œï¼Œé¿å…è¿‡æ‹Ÿåˆ
DENOISE_FILTER = os.environ.get(
    'DENOISE_FILTER',
    'afftdn=nf=-25,highpass=f=50,lowpass=f=8000,dynaudnorm=m=7:s=5'
)

# è§£ç ç­–ç•¥ï¼ˆè‹¥æ¨¡å‹æ”¯æŒï¼‰
DECODING_STRATEGY = os.environ.get('DECODING_STRATEGY', 'greedy')  # å¯é€‰: greedy, beam
RNNT_BEAM_SIZE = int(os.environ.get('RNNT_BEAM_SIZE', '4'))

# Nemo è½¬å†™è¿è¡Œæ—¶é…ç½®ï¼ˆæ‰¹é‡ä¸DataLoaderï¼‰
TRANSCRIBE_BATCH_SIZE = int(os.environ.get('TRANSCRIBE_BATCH_SIZE', '1'))
TRANSCRIBE_NUM_WORKERS = int(os.environ.get('TRANSCRIBE_NUM_WORKERS', '0'))

# å­—å¹•åå¤„ç†é…ç½®ï¼ˆé˜²æ­¢å­—å¹•æ˜¾ç¤ºæ—¶é—´è¿‡çŸ­ï¼‰
MERGE_SHORT_SUBTITLES = os.environ.get('MERGE_SHORT_SUBTITLES', 'true').lower() in ['true', '1', 't']
MIN_SUBTITLE_DURATION_SECONDS = float(os.environ.get('MIN_SUBTITLE_DURATION_SECONDS', '1.5'))
SHORT_SUBTITLE_MERGE_MAX_GAP_SECONDS = float(os.environ.get('SHORT_SUBTITLE_MERGE_MAX_GAP_SECONDS', '0.3'))
SHORT_SUBTITLE_MIN_CHARS = int(os.environ.get('SHORT_SUBTITLE_MIN_CHARS', '6'))
SUBTITLE_MIN_GAP_SECONDS = float(os.environ.get('SUBTITLE_MIN_GAP_SECONDS', '0.06'))

# é•¿å­—å¹•æ‹†åˆ†ä¸æ¢è¡Œï¼ˆå¯é€‰ï¼‰
# - å°†è¿‡é•¿/è¿‡ä¹…çš„å­—å¹•æ‹†ä¸ºå¤šæ¡ï¼›åŒæ—¶å¯¹æ¯æ¡å­—å¹•å†…æ–‡æœ¬è¿›è¡Œæ¢è¡Œï¼Œä¾¿äºè§‚çœ‹
SPLIT_LONG_SUBTITLES = os.environ.get('SPLIT_LONG_SUBTITLES', 'true').lower() in ['true', '1', 't']
MAX_SUBTITLE_DURATION_SECONDS = float(os.environ.get('MAX_SUBTITLE_DURATION_SECONDS', '6.0'))
MAX_SUBTITLE_CHARS_PER_SEGMENT = int(os.environ.get('MAX_SUBTITLE_CHARS_PER_SEGMENT', '84'))  # çº¦ä¸¤è¡Œï¼Œæ¯è¡Œ~42
PREFERRED_LINE_LENGTH = int(os.environ.get('PREFERRED_LINE_LENGTH', '42'))
MAX_SUBTITLE_LINES = int(os.environ.get('MAX_SUBTITLE_LINES', '2'))
# è‹¥ä¸º trueï¼Œå°è¯•ä½¿ç”¨è¯çº§æ—¶é—´æˆ³è¿›è¡Œæ›´ç²¾ç¡®çš„æ‹†åˆ†ï¼ˆæ¨¡å‹è‹¥æœªè¿”å›wordsåˆ™è‡ªåŠ¨å›é€€ï¼‰
ENABLE_WORD_TIMESTAMPS_FOR_SPLIT = os.environ.get('ENABLE_WORD_TIMESTAMPS_FOR_SPLIT', 'false').lower() in ['true', '1', 't']
# é€šè¿‡æ ‡ç‚¹ä¼˜å…ˆåˆ‡åˆ†ï¼Œé€—å·/å¥å·/é—®å·/æ„Ÿå¹å·/åˆ†å·ç­‰
SUBTITLE_SPLIT_PUNCTUATION = os.environ.get('SUBTITLE_SPLIT_PUNCTUATION', 'ã€‚ï¼ï¼Ÿ!?.,;ï¼›ï¼Œ,')

# ç®€åŒ–é…ç½®ï¼šé¢„è®¾ä¸GPUæ˜¾å­˜ï¼ˆGBï¼‰
PRESET = os.environ.get('PRESET', 'balanced').lower()  # speed | balanced | quality | simple(=balanced)
GPU_VRAM_GB_ENV = os.environ.get('GPU_VRAM_GB', '').strip()


# ç¡®ä¿ä¸´æ—¶ä¸Šä¼ ç›®å½•å­˜åœ¨
if not os.path.exists('/app/temp_uploads'):
    os.makedirs('/app/temp_uploads')

def setup_tensor_core_optimization():
    """é…ç½®Tensor Coreä¼˜åŒ–è®¾ç½®"""
    global cuda_available
    if not cuda_available:
        print("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡Tensor Coreä¼˜åŒ–é…ç½®")
        return
    
    print("æ­£åœ¨é…ç½® Tensor Core ä¼˜åŒ–...")
    
    try:
        # å¯ç”¨ cuDNN benchmark æ¨¡å¼
        if ENABLE_CUDNN_BENCHMARK:
            cudnn.benchmark = True
            cudnn.deterministic = False  # ä¸ºäº†æ€§èƒ½ï¼Œå…è®¸éç¡®å®šæ€§
            print("âœ… cuDNN benchmark å·²å¯ç”¨")
        else:
            cudnn.benchmark = False
            cudnn.deterministic = True
            print("âŒ cuDNN benchmark å·²ç¦ç”¨ï¼ˆç¡®å®šæ€§æ¨¡å¼ï¼‰")
        
        # å¯ç”¨ cuDNN å…è®¸ TensorCore
        if ENABLE_TENSOR_CORE:
            cudnn.allow_tf32 = True  # å…è®¸TF32ï¼ˆA100ç­‰æ”¯æŒï¼‰
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("âœ… Tensor Core (TF32) å·²å¯ç”¨")
        else:
            cudnn.allow_tf32 = False
            torch.backends.cuda.matmul.allow_tf32 = False
            torch.backends.cudnn.allow_tf32 = False
            print("âŒ Tensor Core å·²ç¦ç”¨")
        
        # è®¾ç½® Tensor Core ç²¾åº¦ç­–ç•¥
        if TENSOR_CORE_PRECISION == 'highest':
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
            print("âœ… è®¾ç½®ä¸ºæœ€é«˜ç²¾åº¦æ¨¡å¼")
        elif TENSOR_CORE_PRECISION == 'high':
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
            print("âœ… è®¾ç½®ä¸ºé«˜ç²¾åº¦æ¨¡å¼")
        else:  # medium
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
            print("âœ… è®¾ç½®ä¸ºä¸­ç­‰ç²¾åº¦æ¨¡å¼")
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥ä»¥ä¼˜åŒ– Tensor Core ä½¿ç”¨
        try:
            torch.cuda.set_per_process_memory_fraction(GPU_MEMORY_FRACTION)
            print(f"âœ… GPU å†…å­˜åˆ†é…æ¯”ä¾‹: {GPU_MEMORY_FRACTION*100:.0f}%")
        except Exception as e:
            print(f"âš ï¸ è®¾ç½®å†…å­˜åˆ†é…æ¯”ä¾‹å¤±è´¥: {e}")
        print("âœ… GPU å†…å­˜åˆ†é…ç­–ç•¥å·²ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸ Tensor Coreä¼˜åŒ–é…ç½®å¤±è´¥: {e}")

def get_tensor_core_info():
    """è·å– Tensor Core æ”¯æŒä¿¡æ¯"""
    global cuda_available
    if not cuda_available:
        return "N/A - CUDAä¸å¯ç”¨"
    
    try:
        device = torch.cuda.get_device_properties(0)
        major, minor = device.major, device.minor
        
        # æ£€æµ‹ Tensor Core æ”¯æŒ
        if major >= 7:  # V100, T4, RTX 20/30/40ç³»åˆ—ç­‰
            if major == 7:
                return f"âœ… Tensor Core 1.0 (è®¡ç®—èƒ½åŠ› {major}.{minor})"
            elif major == 8:
                if minor >= 0:
                    return f"âœ… Tensor Core 2.0 + TF32 (è®¡ç®—èƒ½åŠ› {major}.{minor})"
                else:
                    return f"âœ… Tensor Core 2.0 (è®¡ç®—èƒ½åŠ› {major}.{minor})"
            elif major >= 9:
                return f"âœ… Tensor Core 3.0+ (è®¡ç®—èƒ½åŠ› {major}.{minor})"
        elif major >= 6:  # P100ç­‰
            return f"âš ï¸ æœ‰é™Tensor Coreæ”¯æŒ (è®¡ç®—èƒ½åŠ› {major}.{minor})"
        else:
            return f"âŒ ä¸æ”¯æŒTensor Core (è®¡ç®—èƒ½åŠ› {major}.{minor})"
        
        return f"æœªçŸ¥ (è®¡ç®—èƒ½åŠ› {major}.{minor})"
    except Exception as e:
        return f"âŒ è·å–GPUä¿¡æ¯å¤±è´¥: {e}"

def optimize_tensor_operations():
    """ä¼˜åŒ–å¼ é‡æ“ä½œä»¥æ›´å¥½åœ°åˆ©ç”¨ Tensor Core"""
    global cuda_available
    if not cuda_available:
        print("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡Tensor Coreé¢„çƒ­")
        return
    
    try:
        # è®¾ç½®ä¼˜åŒ–çš„ CUDA æµ
        torch.cuda.set_sync_debug_mode(0)  # ç¦ç”¨åŒæ­¥è°ƒè¯•ä»¥æå‡æ€§èƒ½
        
        # é¢„çƒ­GPUï¼Œç¡®ä¿Tensor Coreæ­£ç¡®æ¿€æ´»
        # åˆ›å»ºä¸€äº›å¯¹é½åˆ°8/16å€æ•°çš„çŸ©é˜µè¿›è¡Œé¢„çƒ­
        device = torch.cuda.current_device()
        dummy_a = torch.randn(128, 128, device=device, dtype=torch.float16)
        dummy_b = torch.randn(128, 128, device=device, dtype=torch.float16)
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•é¢„çƒ­Tensor Core
        with torch.cuda.amp.autocast():
            _ = torch.matmul(dummy_a, dummy_b)
        
        torch.cuda.synchronize()
        del dummy_a, dummy_b
        torch.cuda.empty_cache()
        print("âœ… Tensor Core é¢„çƒ­å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ Tensor Core é¢„çƒ­å¤±è´¥: {e}")

def detect_sentence_boundaries(text: str) -> list:
    """æ£€æµ‹å¥å­è¾¹ç•Œï¼Œè¿”å›å¥å­ç»“æŸä½ç½®åˆ—è¡¨"""
    import re
    
    # ä¸­è‹±æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰
    sentence_endings = re.finditer(r'[.!?ã€‚ï¼ï¼Ÿ]+[\s]*', text)
    boundaries = [match.end() for match in sentence_endings]
    return boundaries

def find_best_split_point(segments: list, target_time: float, tolerance: float = 2.0) -> int:
    """åœ¨ç›®æ ‡æ—¶é—´é™„è¿‘æ‰¾åˆ°æœ€ä½³çš„å¥å­åˆ†å‰²ç‚¹"""
    if not segments:
        return 0
    
    best_index = 0
    min_distance = float('inf')
    
    # å¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡æ—¶é—´çš„å¥å­ç»“æŸç‚¹
    for i, segment in enumerate(segments):
        segment_end = segment.get('end', 0)
        distance = abs(segment_end - target_time)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¥å­ç»“æŸï¼ˆåŒ…å«æ ‡ç‚¹ç¬¦å·ï¼‰
        text = segment.get('segment', '').strip()
        if text and (text.endswith('.') or text.endswith('ã€‚') or 
                     text.endswith('!') or text.endswith('ï¼') or
                     text.endswith('?') or text.endswith('ï¼Ÿ')):
            # å¥å­ç»“æŸç‚¹ï¼Œæƒé‡æ›´é«˜
            distance *= 0.5
        
        if distance < min_distance and distance <= tolerance:
            min_distance = distance
            best_index = i + 1  # è¿”å›ä¸‹ä¸€ä¸ªæ®µè½çš„ç´¢å¼•
    
    return min(best_index, len(segments))

def merge_overlapping_segments(all_segments: list, chunk_boundaries: list, overlap_seconds: float) -> list:
    """åˆå¹¶é‡å åŒºåŸŸçš„segmentsï¼Œå»é™¤é‡å¤å†…å®¹"""
    if not ENABLE_OVERLAP_CHUNKING or len(chunk_boundaries) <= 1:
        return all_segments
    
    # ç®€åŒ–å¹¶æ›´é²æ£’ï¼šæŒ‰æ—¶é—´æ’åºï¼Œç„¶ååŸºäºé‡å çª—å£å†…å»é‡åŒæ–‡æ®µè½
    if not all_segments:
        return []
    all_segments_sorted = sorted(all_segments, key=lambda s: (s.get('start', 0.0), s.get('end', 0.0)))
    merged = []
    for seg in all_segments_sorted:
        text = seg.get('segment', '').strip()
        if not text:
            continue
        if not merged:
            merged.append(seg)
            continue
        prev = merged[-1]
        # è‹¥æ—¶é—´ä¸Šé«˜åº¦é‡å ï¼Œä¸”æ–‡æœ¬é«˜ç›¸ä¼¼ï¼ˆæˆ–å®Œå…¨ç›¸åŒï¼‰ï¼Œåˆ™ä¿ç•™æ›´é•¿/ç½®ä¿¡åº¦æ›´é«˜çš„ä¸€æ¡
        overlap = min(prev['end'], seg['end']) - max(prev['start'], seg['start'])
        window = overlap_seconds * 0.9 if overlap_seconds else 0.0
        def normalized(t: str) -> str:
            return ''.join(t.split()).lower()
        same_text = normalized(prev.get('segment', '')) == normalized(text)
        if overlap > 0 and overlap >= min(prev['end'] - prev['start'], seg['end'] - seg['start']) * 0.5:
            if same_text or overlap >= window:
                # é€‰æ‹©æ—¶é—´æ›´é•¿çš„æ®µè½
                if (prev['end'] - prev['start']) >= (seg['end'] - seg['start']):
                    # å¯èƒ½æ‰©å±•å°¾éƒ¨
                    prev['end'] = max(prev['end'], seg['end'])
                else:
                    merged[-1] = seg
                continue
        # å¦åˆ™ç›´æ¥è¿½åŠ 
        merged.append(seg)
    print(f"åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆ {len(merged)} ä¸ªsegments")
    return merged

def enforce_min_subtitle_duration(
    segments: list,
    min_duration: float,
    merge_max_gap: float,
    min_chars: int,
    min_gap: float,
) -> list:
    """å¯¹è½¬å†™çš„ segments è¿›è¡Œåå¤„ç†ï¼Œé¿å…å­—å¹•æ˜¾ç¤ºæ—¶é—´è¿‡çŸ­ï¼š
    1) å°è¯•å°†è¿‡çŸ­æˆ–æ–‡æœ¬è¿‡å°‘çš„ç›¸é‚»æ®µåˆå¹¶ï¼ˆä¸¤æ®µé—´éš™ä¸è¶…è¿‡ merge_max_gapï¼‰ã€‚
    2) è‹¥ä»çŸ­äº min_durationï¼Œå°½é‡å°†å½“å‰æ®µçš„ç»“æŸæ—¶é—´å»¶é•¿åˆ° min_durationï¼Œä½†ä¸ä¸ä¸‹ä¸€æ®µé‡å ï¼ˆé¢„ç•™ min_gapï¼‰ã€‚

    segments: [{'start': float, 'end': float, 'segment': str}, ...]
    è¿”å›ï¼šå¤„ç†åçš„ segmentsï¼ˆæŒ‰å¼€å§‹æ—¶é—´æ’åºï¼Œä¸”ä¸é‡å ï¼‰
    """
    if not segments:
        return []

    # æŒ‰å¼€å§‹æ—¶é—´æ’åºï¼Œæ·±æ‹·è´ä»¥å…ä¿®æ”¹åŸå¯¹è±¡
    segments_sorted = sorted(
        [
            {
                'start': float(s.get('start', 0.0)),
                'end': float(s.get('end', 0.0)),
                'segment': str(s.get('segment', '')),
            }
            for s in segments
        ],
        key=lambda s: (s['start'], s['end'])
    )

    result: list = []
    i = 0
    n = len(segments_sorted)

    while i < n:
        current = segments_sorted[i]
        current_text = str(current.get('segment', '')).strip()

        # å°è¯•å‰å‘åˆå¹¶ï¼Œç›´åˆ°æ»¡è¶³æœ€çŸ­æ—¶é•¿æˆ–æ— å¯åˆå¹¶å¯¹è±¡
        while MERGE_SHORT_SUBTITLES:
            duration = max(0.0, float(current.get('end', 0.0)) - float(current.get('start', 0.0)))
            too_short = duration < min_duration or len(current_text) <= min_chars
            if not too_short or i + 1 >= n:
                break
            next_seg = segments_sorted[i + 1]
            gap = max(0.0, float(next_seg.get('start', 0.0)) - float(current.get('end', 0.0)))
            if gap > merge_max_gap:
                break
            # åˆå¹¶åˆ° current
            next_text = str(next_seg.get('segment', '')).strip()
            current['end'] = max(float(current.get('end', 0.0)), float(next_seg.get('end', 0.0)))
            current_text = (current_text + ' ' + next_text).strip()
            current['segment'] = current_text
            i += 1  # åå¹¶ä¸‹ä¸€æ®µ
        # åˆå¹¶å®Œæˆåï¼Œå¦‚ä»çŸ­åˆ™å°è¯•å»¶é•¿ï¼Œä½†ä¸å¾—ä¸ä¸‹ä¸€æ®µé‡å 
        duration = max(0.0, float(current.get('end', 0.0)) - float(current.get('start', 0.0)))
        if duration < float(min_duration):
            desired_end = float(current.get('start', 0.0)) + float(min_duration)
            if i + 1 < n:
                next_start = float(segments_sorted[i + 1].get('start', 0.0))
                safe_end = max(float(current.get('end', 0.0)), min(desired_end, next_start - float(min_gap)))
                # åªæœ‰åœ¨ä¸ä¼šå¯¼è‡´éæ³•åŒºé—´æ—¶æ‰æ›´æ–°
                if safe_end > float(current.get('start', 0.0)):
                    current['end'] = safe_end
            else:
                # å·²æ˜¯æœ€åä¸€æ®µï¼Œç›´æ¥å»¶é•¿
                current['end'] = desired_end

        result.append(current)
        i += 1

    # æœ€åå†ä¿è¯ä¸é‡å ä¸å•è°ƒé€’å¢
    cleaned: list = []
    for seg in result:
        if not cleaned:
            cleaned.append(seg)
            continue
        prev = cleaned[-1]
        if seg['start'] < prev['end']:
            seg['start'] = prev['end'] + min_gap
            if seg['start'] > seg['end']:
                seg['start'] = seg['end']
        cleaned.append(seg)

    return cleaned

def process_chunk_segments(segments: list, overlap_start: float, overlap_seconds: float) -> list:
    """å¤„ç†å•ä¸ªchunkçš„segmentsï¼Œå¤„ç†é‡å åŒºåŸŸ"""
    if not segments:
        return []
    
    processed = []
    overlap_end = overlap_start + overlap_seconds
    
    for segment in segments:
        segment_start = segment['start']
        segment_end = segment['end']
        
        # å¦‚æœsegmentå®Œå…¨åœ¨é‡å åŒºåŸŸä¹‹å‰ï¼Œç›´æ¥æ·»åŠ 
        if segment_end <= overlap_start:
            processed.append(segment)
        # å¦‚æœsegmentè·¨è¶Šé‡å åŒºåŸŸå¼€å§‹ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æˆªæ–­
        elif segment_start < overlap_start < segment_end:
            # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­ä¸­é—´æˆªæ–­
            text = segment.get('segment', '').strip()
            if text and not any(punct in text for punct in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']):
                # åœ¨å¥å­ä¸­é—´ï¼Œä¿ç•™å®Œæ•´segment
                processed.append(segment)
            else:
                # å¯ä»¥å®‰å…¨æˆªæ–­çš„å¥å­ç»“æŸ
                processed.append(segment)
        
    return processed

def create_overlap_chunks(total_duration: float, chunk_duration: float, overlap_seconds: float) -> list:
    """åˆ›å»ºå¸¦é‡å çš„chunkæ—¶é—´æ®µ"""
    chunks = []
    current_start = 0.0
    
    while current_start < total_duration:
        chunk_end = min(current_start + chunk_duration, total_duration)
        
        chunk_info = {
            'start': current_start,
            'end': chunk_end,
            'duration': chunk_end - current_start
        }
        chunks.append(chunk_info)
        
        # ä¸‹ä¸€ä¸ªchunkçš„å¼€å§‹æ—¶é—´ï¼ˆè€ƒè™‘é‡å ï¼‰
        if chunk_end >= total_duration:
            break
            
        current_start = chunk_end - overlap_seconds
        
    print(f"åˆ›å»ºäº† {len(chunks)} ä¸ªé‡å chunks:")
    for i, chunk in enumerate(chunks):
        print(f"  Chunk {i}: {chunk['start']:.1f}s - {chunk['end']:.1f}s (æ—¶é•¿: {chunk['duration']:.1f}s)")
    
    return chunks

def check_cuda_compatibility():
    """æ£€æŸ¥CUDAå…¼å®¹æ€§ï¼Œå¦‚æœä¸å…¼å®¹åˆ™ç¦ç”¨CUDA"""
    global cuda_available
    
    try:
        if not torch.cuda.is_available():
            print("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            cuda_available = False
            return False
        
        # å°è¯•è·å–è®¾å¤‡æ•°é‡æ¥æµ‹è¯•CUDAå…¼å®¹æ€§
        device_count = torch.cuda.device_count()
        if device_count == 0:
            print("æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            cuda_available = False
            return False
            
        # å°è¯•è·å–è®¾å¤‡å±æ€§æ¥è¿›ä¸€æ­¥æµ‹è¯•å…¼å®¹æ€§
        device_props = torch.cuda.get_device_properties(0)
        print(f"âœ… æ£€æµ‹åˆ°å…¼å®¹çš„GPU: {device_props.name}")
        cuda_available = True
        return True
    except RuntimeError as e:
        if "forward compatibility was attempted on non supported HW" in str(e):
            print("âš ï¸ CUDAå…¼å®¹æ€§é”™è¯¯: GPUç¡¬ä»¶ä¸æ”¯æŒå½“å‰CUDAç‰ˆæœ¬")
            print("è¿™é€šå¸¸æ˜¯å› ä¸ºä¸»æœºçš„GPUé©±åŠ¨ç‰ˆæœ¬è¿‡æ—§ï¼Œä¸æ”¯æŒå®¹å™¨ä¸­çš„CUDA 13.x è¿è¡Œæ—¶")
            print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        elif "CUDA" in str(e):
            print(f"âš ï¸ CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
            print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        else:
            print(f"âš ï¸ æœªçŸ¥CUDAé”™è¯¯: {e}")
            print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        
        cuda_available = False
        return False
    except Exception as e:
        print(f"âš ï¸ GPUå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
        print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼è¿è¡Œ")
        cuda_available = False
        return False

def get_gpu_memory_usage():
    """è·å–GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    global cuda_available
    if not cuda_available:
        return 0, 0, 0
    
    try:
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        return allocated, reserved, total
    except Exception as e:
        print(f"âš ï¸ è·å–GPUå†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return 0, 0, 0

def aggressive_memory_cleanup():
    """æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†å‡½æ•°"""
    global cuda_available
    if cuda_available:
        try:
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()
            # åŒæ­¥æ‰€æœ‰CUDAæ“ä½œ
            torch.cuda.synchronize()
            # é‡ç½®å³°å€¼å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
        except Exception as e:
            print(f"âš ï¸ CUDAæ¸…ç†æ“ä½œå¤±è´¥: {e}")
    
    # å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
    for _ in range(3):
        gc.collect()
    
    if cuda_available:
        try:
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

def ultra_aggressive_memory_cleanup():
    """è¶…çº§æ¿€è¿›çš„å†…å­˜æ¸…ç†å‡½æ•° - ç”¨äºå¤„ç†é«˜å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    global cuda_available
    print("ğŸ”¥ æ‰§è¡Œè¶…çº§æ¿€è¿›å†…å­˜æ¸…ç†...")
    
    # è®°å½•æ¸…ç†å‰çš„å†…å­˜ä½¿ç”¨
    if cuda_available:
        allocated_before, reserved_before, total = get_gpu_memory_usage()
        print(f"æ¸…ç†å‰æ˜¾å­˜ä½¿ç”¨: {allocated_before:.2f}GB / {total:.2f}GB ({allocated_before/total*100:.1f}%)")
    else:
        memory_before = psutil.virtual_memory()
        print(f"æ¸…ç†å‰å†…å­˜ä½¿ç”¨: {memory_before.used/1024**3:.2f}GB / {memory_before.total/1024**3:.2f}GB ({memory_before.percent:.1f}%)")
    
    # æ‰§è¡Œæ ‡å‡†çš„æ¿€è¿›æ¸…ç†
    aggressive_memory_cleanup()
    
    # è¶…çº§æ¿€è¿›çš„CUDAæ¸…ç†
    if cuda_available:
        try:
            # å¼ºåˆ¶åŒæ­¥æ‰€æœ‰CUDAæµ
            torch.cuda.synchronize()
            
            # è®¾ç½®ç©ºä¸Šä¸‹æ–‡ä»¥é‡Šæ”¾æ›´å¤šå†…å­˜
            torch.cuda.empty_cache()
            
            # å¤šè½®å¼ºåˆ¶æ¸…ç©ºCUDAç¼“å­˜
            for round_num in range(5):
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                if round_num < 4:  # åœ¨è½®æ¬¡é—´è¿›è¡Œåƒåœ¾å›æ”¶
                    gc.collect()
            
            # é‡ç½®æ‰€æœ‰CUDAç»Ÿè®¡å’ŒçŠ¶æ€
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
            
            # å°è¯•é‡ç½®CUDAä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæ”¯æŒï¼‰
            try:
                if hasattr(torch.cuda, 'reset_memory_stats'):
                    torch.cuda.reset_memory_stats()
                if hasattr(torch.cuda, 'ipc_collect'):
                    torch.cuda.ipc_collect()
            except Exception:
                pass
                
        except Exception as e:
            print(f"âš ï¸ è¶…çº§æ¿€è¿›CUDAæ¸…ç†å¤±è´¥: {e}")
    
    # è¶…å¼ºåŠ›çš„Pythonåƒåœ¾å›æ”¶
    print("æ‰§è¡Œå¼ºåŠ›åƒåœ¾å›æ”¶...")
    for round_num in range(8):
        collected = gc.collect()
        if collected > 0:
            print(f"åƒåœ¾å›æ”¶è½®æ¬¡ {round_num + 1}: å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
    
    # å¼ºåˆ¶è¿è¡Œæ‰€æœ‰ç»ˆç»“å™¨
    try:
        import weakref
        weakref.finalize._run_finalizers()
    except Exception:
        pass
    
    # è®°å½•æ¸…ç†åçš„å†…å­˜ä½¿ç”¨
    if cuda_available:
        allocated_after, reserved_after, total = get_gpu_memory_usage()
        saved_memory = allocated_before - allocated_after
        print(f"æ¸…ç†åæ˜¾å­˜ä½¿ç”¨: {allocated_after:.2f}GB / {total:.2f}GB ({allocated_after/total*100:.1f}%)")
        print(f"âœ… è¶…çº§æ¿€è¿›æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾æ˜¾å­˜: {saved_memory:.2f}GB")
    else:
        memory_after = psutil.virtual_memory()
        print(f"æ¸…ç†åå†…å­˜ä½¿ç”¨: {memory_after.used/1024**3:.2f}GB / {memory_after.total/1024**3:.2f}GB ({memory_after.percent:.1f}%)")
        print(f"âœ… è¶…çº§æ¿€è¿›æ¸…ç†å®Œæˆ")

def idle_deep_memory_cleanup():
    """é—²ç½®æ—¶æ·±åº¦å†…å­˜æ¸…ç†å‡½æ•°"""
    global cuda_available
    print("ğŸ§¹ æ‰§è¡Œé—²ç½®æ—¶æ·±åº¦å†…å­˜æ¸…ç†...")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¶…çº§æ¿€è¿›æ¸…ç†
    needs_ultra_cleanup = False
    if cuda_available:
        allocated, _, total = get_gpu_memory_usage()
        if allocated > MEMORY_USAGE_ALERT_THRESHOLD_GB:
            needs_ultra_cleanup = True
            print(f"âš ï¸ æ˜¾å­˜ä½¿ç”¨({allocated:.2f}GB)è¶…è¿‡è­¦å‘Šé˜ˆå€¼({MEMORY_USAGE_ALERT_THRESHOLD_GB:.1f}GB)ï¼Œå¯ç”¨è¶…çº§æ¿€è¿›æ¸…ç†")
    else:
        memory = psutil.virtual_memory()
        memory_gb = memory.used / 1024**3
        if memory_gb > MEMORY_USAGE_ALERT_THRESHOLD_GB:
            needs_ultra_cleanup = True
            print(f"âš ï¸ å†…å­˜ä½¿ç”¨({memory_gb:.2f}GB)è¶…è¿‡è­¦å‘Šé˜ˆå€¼({MEMORY_USAGE_ALERT_THRESHOLD_GB:.1f}GB)ï¼Œå¯ç”¨è¶…çº§æ¿€è¿›æ¸…ç†")
    
    if needs_ultra_cleanup and ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION:
        ultra_aggressive_memory_cleanup()
    else:
        # æ‰§è¡Œæ ‡å‡†çš„æ¿€è¿›æ¸…ç†
        aggressive_memory_cleanup()
        
        # é¢å¤–çš„æ·±åº¦æ¸…ç†æªæ–½
        if cuda_available:
            try:
                # å¤šæ¬¡æ¸…ç©ºCUDAç¼“å­˜ä»¥ç¡®ä¿å½»åº•
                for _ in range(3):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                # é‡ç½®æ‰€æœ‰å†…å­˜ç»Ÿè®¡
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.reset_accumulated_memory_stats()
            except Exception as e:
                print(f"âš ï¸ æ·±åº¦CUDAæ¸…ç†å¤±è´¥: {e}")
        
        # æ›´å¼ºåŠ›çš„åƒåœ¾å›æ”¶
        for _ in range(5):
            gc.collect()
        
        allocated, reserved, total = get_gpu_memory_usage()
        print(f"âœ… æ·±åº¦æ¸…ç†å®Œæˆï¼Œå½“å‰æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.2f}GB")
    
    # å°è¯•è®¾ç½®ä½ä¼˜å…ˆçº§ (ä»…åœ¨æ”¯æŒçš„ç³»ç»Ÿä¸Š)
    if ENABLE_IDLE_CPU_OPTIMIZATION:
        try:
            import os
            import psutil
            current_process = psutil.Process()
            # è®¾ç½®ä¸ºä½ä¼˜å…ˆçº§ (ä»…åœ¨é—²ç½®æ—¶)
            if hasattr(psutil, 'BELOW_NORMAL_PRIORITY_CLASS'):
                current_process.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
            elif hasattr(current_process, 'nice'):
                current_process.nice(10)  # è®¾ç½®ä¸ºä½ä¼˜å…ˆçº§
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
            pass

def immediate_post_request_cleanup():
    """è¯·æ±‚å®Œæˆåç«‹å³æ‰§è¡Œçš„å†…å­˜æ¸…ç†"""
    if not IMMEDIATE_CLEANUP_AFTER_REQUEST:
        return
    
    print("ğŸ§½ æ‰§è¡Œè¯·æ±‚åå³æ—¶æ¸…ç†...")
    global cuda_available
    
    if cuda_available:
        try:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        except Exception:
            pass
    
    # å¿«é€Ÿåƒåœ¾å›æ”¶
    gc.collect()

def check_memory_usage_and_cleanup():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µå¹¶åœ¨å¿…è¦æ—¶è§¦å‘æ¸…ç†"""
    global cuda_available
    
    if cuda_available:
        allocated, _, total = get_gpu_memory_usage()
        if allocated > MEMORY_USAGE_ALERT_THRESHOLD_GB:
            print(f"ğŸš¨ æ˜¾å­˜ä½¿ç”¨è¿‡é«˜({allocated:.2f}GB > {MEMORY_USAGE_ALERT_THRESHOLD_GB:.1f}GB)ï¼Œç«‹å³æ‰§è¡Œæ¸…ç†")
            if ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION:
                ultra_aggressive_memory_cleanup()
            else:
                aggressive_memory_cleanup()
            return True
    else:
        memory = psutil.virtual_memory()
        memory_gb = memory.used / 1024**3
        if memory_gb > MEMORY_USAGE_ALERT_THRESHOLD_GB:
            print(f"ğŸš¨ å†…å­˜ä½¿ç”¨è¿‡é«˜({memory_gb:.2f}GB > {MEMORY_USAGE_ALERT_THRESHOLD_GB:.1f}GB)ï¼Œç«‹å³æ‰§è¡Œæ¸…ç†")
            if ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION:
                ultra_aggressive_memory_cleanup()
            else:
                aggressive_memory_cleanup()
            return True
    
    return False

def should_force_cleanup():
    """æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶æ¸…ç†æ˜¾å­˜"""
    global cuda_available
    if not cuda_available:
        return False
    
    allocated, reserved, total = get_gpu_memory_usage()
    usage_ratio = allocated / total if total > 0 else 0
    return usage_ratio > FORCE_CLEANUP_THRESHOLD

def optimize_model_for_inference(model):
    """ä¼˜åŒ–æ¨¡å‹ä»¥å‡å°‘æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨"""
    if model is None:
        return model
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if ENABLE_GRADIENT_CHECKPOINTING and hasattr(model, 'encoder'):
        try:
            if hasattr(model.encoder, 'use_gradient_checkpointing'):
                model.encoder.use_gradient_checkpointing = True
            elif hasattr(model.encoder, 'gradient_checkpointing'):
                model.encoder.gradient_checkpointing = True
        except Exception as e:
            print(f"æ— æ³•å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: {e}")
    
    # ç¦ç”¨è‡ªåŠ¨æ±‚å¯¼ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦æ¢¯åº¦ï¼‰
    for param in model.parameters():
        param.requires_grad = False
    
    return model

def create_streaming_config():
    """åˆ›å»ºæµå¼å¤„ç†é…ç½®ä»¥å‡å°‘æ˜¾å­˜å ç”¨"""
    return {
        'batch_size': 1,  # å•æ‰¹å¤„ç†å‡å°‘æ˜¾å­˜å ç”¨
        'num_workers': 0,  # é¿å…å¤šè¿›ç¨‹å¸¦æ¥çš„é¢å¤–å†…å­˜å¼€é”€
        'pin_memory': False,  # ä¸ä½¿ç”¨é”é¡µå†…å­˜ä»¥èŠ‚çœç³»ç»Ÿå†…å­˜
        'drop_last': False,
        'persistent_workers': False  # ä¸ä¿æŒworkerè¿›ç¨‹
    }

def load_model_if_needed():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œåˆ™è¿›è¡ŒåŠ è½½ã€‚"""
    global asr_model, cuda_available
    # ä½¿ç”¨é”ç¡®ä¿å¤šçº¿ç¨‹ç¯å¢ƒä¸‹æ¨¡å‹åªè¢«åŠ è½½ä¸€æ¬¡
    with model_lock:
        if asr_model is None:
            print("="*50)
            print("æ¨¡å‹å½“å‰æœªåŠ è½½ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            # æ–°æ¨¡å‹é»˜è®¤ï¼šv3ï¼›æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
            model_id = os.environ.get('MODEL_ID', 'nvidia/parakeet-tdt-0.6b-v3').strip()
            model_local_path_env = os.environ.get('MODEL_LOCAL_PATH', '').strip()
            print(f"é¦–é€‰æ¨¡å‹: {model_id}")
            try:
                # é¦–å…ˆæ£€æŸ¥CUDAå…¼å®¹æ€§
                cuda_available = check_cuda_compatibility()
                
                # ç¡®ä¿numbaç¼“å­˜ç›®å½•å­˜åœ¨
                numba_cache_dir = os.environ.get('NUMBA_CACHE_DIR', '/tmp/numba_cache')
                if not os.path.exists(numba_cache_dir):
                    os.makedirs(numba_cache_dir, exist_ok=True)
                    os.chmod(numba_cache_dir, 0o777)
                
                # æœ¬åœ°ä¼˜å…ˆç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ MODEL_LOCAL_PATH ï¼›å¦åˆ™å°è¯•å¸¸è§æ–‡ä»¶åï¼›å¦åˆ™èµ° HF è‡ªåŠ¨ä¸‹è½½
                candidate_local_paths = []
                if model_local_path_env:
                    candidate_local_paths.append(model_local_path_env)
                # æ–°ç‰ˆ v3 é»˜è®¤æ–‡ä»¶åï¼ˆè‹¥ç”¨æˆ·æ‰‹åŠ¨ä¸‹è½½ .nemoï¼‰
                candidate_local_paths.append("/app/models/parakeet-tdt-0.6b-v3.nemo")
                # å…¼å®¹æ—§ç‰ˆ v2 æ–‡ä»¶åï¼ˆå‘åå…¼å®¹ï¼‰
                candidate_local_paths.append("/app/models/parakeet-tdt-0.6b-v2.nemo")

                model_path = next((p for p in candidate_local_paths if os.path.exists(p)), None)

                if cuda_available:
                    print(f"âœ… æ£€æµ‹åˆ°å…¼å®¹çš„CUDAç¯å¢ƒï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿå¹¶å¼€å¯åŠç²¾åº¦(FP16)ä¼˜åŒ–ã€‚")
                    
                    # è®¾ç½® Tensor Core ä¼˜åŒ–
                    setup_tensor_core_optimization()
                    optimize_tensor_operations()
                    
                    # æ˜¾ç¤º GPU å’Œ Tensor Core ä¿¡æ¯
                    device_info = torch.cuda.get_device_properties(0)
                    print(f"GPU: {device_info.name}")
                    print(f"Tensor Core æ”¯æŒ: {get_tensor_core_info()}")
                    
                    # å…ˆåœ¨CPUä¸ŠåŠ è½½æ¨¡å‹ï¼Œç„¶åè½¬ç§»åˆ°GPUå¹¶å¯ç”¨FP16
                    if model_path:
                        # æœ¬åœ° .nemo
                        # æ£€æŸ¥æ–‡ä»¶æƒé™
                        if not os.access(model_path, os.R_OK):
                            raise PermissionError(f"æ— æ³•è¯»å–æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™ã€‚")
                        print(f"ä»æœ¬åœ° .nemo æ¢å¤: {model_path}")
                        loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=model_path, map_location=torch.device('cpu'))
                    else:
                        # ä» HF è‡ªåŠ¨ä¸‹è½½æˆ–å°è¯•ç›´æ¥æŠ“å– .nemo æ–‡ä»¶åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•
                        print(f"å°è¯•ä» Hugging Face è·å–æ¨¡å‹æ–‡ä»¶: {model_id}")
                        os.makedirs('/app/models', exist_ok=True)
                        downloaded_path = None
                        try:
                            if HfApi is None:
                                raise RuntimeError("huggingface_hub not available")
                            api = HfApi()
                            repo_files = api.list_repo_files(model_id)
                            nemo_files = [f for f in repo_files if f.endswith('.nemo')]
                            if nemo_files:
                                target_fname = nemo_files[0]
                                print(f"å‘ç°è¿œç«¯ .nemo æ–‡ä»¶: {target_fname}ï¼Œå¼€å§‹ä¸‹è½½...")
                                downloaded_path = hf_hub_download(repo_id=model_id, filename=target_fname, cache_dir='/app/models')
                                print(f"å·²ä¸‹è½½æ¨¡å‹åˆ°: {downloaded_path}")
                            else:
                                print("è¿œç«¯ä»“åº“æœªå‘ç° .nemo æ–‡ä»¶ï¼Œå›é€€åˆ° NeMo.from_pretrained() æ–¹æ³•åŠ è½½")
                        except Exception as e:
                            print(f"å°è¯•ä» Hugging Face è·å– .nemo å¤±è´¥: {e}")

                        if downloaded_path and os.path.exists(downloaded_path):
                            loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=downloaded_path, map_location=torch.device('cpu'))
                        else:
                            print(f"ä½¿ç”¨ NeMo çš„ from_pretrained åŠ è½½æ¨¡å‹: {model_id}")
                            loaded_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_id)
                    loaded_model = loaded_model.cuda()
                    loaded_model = loaded_model.half()
                    
                    # åº”ç”¨æ¨ç†ä¼˜åŒ–
                    loaded_model = optimize_model_for_inference(loaded_model)
                    
                    # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                    allocated, reserved, total = get_gpu_memory_usage()
                    print(f"æ¨¡å‹åŠ è½½åæ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)")
                else:
                    print("ğŸ”„ ä½¿ç”¨ CPU æ¨¡å¼è¿è¡Œã€‚")
                    print("æ³¨æ„: CPUæ¨¡å¼ä¸‹æ¨ç†é€Ÿåº¦ä¼šè¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨å…¼å®¹çš„GPUã€‚")
                    if model_path:
                        # æœ¬åœ° .nemo
                        if not os.access(model_path, os.R_OK):
                            raise PermissionError(f"æ— æ³•è¯»å–æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™ã€‚")
                        print(f"ä»æœ¬åœ° .nemo æ¢å¤: {model_path}")
                        loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=model_path)
                    else:
                        # ä» HF è‡ªåŠ¨ä¸‹è½½æˆ–å°è¯•ç›´æ¥æŠ“å– .nemo æ–‡ä»¶åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•ï¼ˆCPU åˆ†æ”¯ï¼‰
                        print(f"å°è¯•ä» Hugging Face è·å–æ¨¡å‹æ–‡ä»¶: {model_id}")
                        os.makedirs('/app/models', exist_ok=True)
                        downloaded_path = None
                        try:
                            if HfApi is None:
                                raise RuntimeError("huggingface_hub not available")
                            api = HfApi()
                            repo_files = api.list_repo_files(model_id)
                            nemo_files = [f for f in repo_files if f.endswith('.nemo')]
                            if nemo_files:
                                target_fname = nemo_files[0]
                                print(f"å‘ç°è¿œç«¯ .nemo æ–‡ä»¶: {target_fname}ï¼Œå¼€å§‹ä¸‹è½½...")
                                downloaded_path = hf_hub_download(repo_id=model_id, filename=target_fname, cache_dir='/app/models')
                                print(f"å·²ä¸‹è½½æ¨¡å‹åˆ°: {downloaded_path}")
                            else:
                                print("è¿œç«¯ä»“åº“æœªå‘ç° .nemo æ–‡ä»¶ï¼Œå›é€€åˆ° NeMo.from_pretrained() æ–¹æ³•åŠ è½½")
                        except Exception as e:
                            print(f"å°è¯•ä» Hugging Face è·å– .nemo å¤±è´¥: {e}")

                        if downloaded_path and os.path.exists(downloaded_path):
                            loaded_model = nemo_asr.models.ASRModel.restore_from(restore_path=downloaded_path)
                        else:
                            print(f"ä½¿ç”¨ NeMo çš„ from_pretrained åŠ è½½æ¨¡å‹: {model_id}")
                            loaded_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_id)
                    loaded_model = optimize_model_for_inference(loaded_model)
                
                # é…ç½®è§£ç ç­–ç•¥ï¼ˆè‹¥æ¨¡å‹æ”¯æŒï¼‰
                try:
                    configure_decoding_strategy(loaded_model)
                except Exception as e:
                    print(f"âš ï¸ é…ç½®è§£ç ç­–ç•¥å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤è§£ç : {e}")

                asr_model = loaded_model
                print("âœ… NeMo ASR æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                print("="*50)
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("="*50)
                import traceback
                traceback.print_exc()
                # å‘ä¸ŠæŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿æ¥å£å¯ä»¥æ•è·å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
                raise e
    return asr_model

def predownload_model_artifacts():
    """åœ¨åå°ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œä½†ä¸åŠ è½½åˆ°å†…å­˜ã€‚
    è¿™ä¸ªå‡½æ•°ç”¨äºåœ¨å¯ç”¨æ‡’åŠ è½½æ—¶æå‰æŠŠå¤§æ–‡ä»¶æ‹‰å–åˆ° `/app/models`ï¼Œä»¥ç¼©çŸ­åç»­é¦–æ¬¡åŠ è½½å»¶æ—¶ã€‚
    """
    try:
        model_id = os.environ.get('MODEL_ID', 'nvidia/parakeet-tdt-0.6b-v3').strip()
        model_local_path_env = os.environ.get('MODEL_LOCAL_PATH', '').strip()
        print(f"[predownload] å¯åŠ¨æ¨¡å‹é¢„ä¸‹è½½æ£€æŸ¥: {model_id}")

        # æœ¬åœ°ä¼˜å…ˆï¼šå¦‚æœå·²å­˜åœ¨æœ¬åœ°æ–‡ä»¶åˆ™æ— éœ€ä¸‹è½½
        candidate_local_paths = []
        if model_local_path_env:
            candidate_local_paths.append(model_local_path_env)
        candidate_local_paths.append('/app/models/parakeet-tdt-0.6b-v3.nemo')
        candidate_local_paths.append('/app/models/parakeet-tdt-0.6b-v2.nemo')
        for p in candidate_local_paths:
            if p and os.path.exists(p):
                print(f"[predownload] å‘ç°æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼Œæ— éœ€ä¸‹è½½: {p}")
                return

        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs('/app/models', exist_ok=True)

        # å°è¯•ä½¿ç”¨ huggingface_hub ä¸‹è½½è¿œç«¯ .nemo æ–‡ä»¶ï¼ˆä»…ä¸‹è½½ï¼Œä¸æ¢å¤/åŠ è½½ï¼‰
        if HfApi is None:
            print("[predownload] huggingface_hub ä¸å¯ç”¨ï¼Œè·³è¿‡é¢„ä¸‹è½½")
            return

        try:
            api = HfApi()
            repo_files = api.list_repo_files(model_id)
            nemo_files = [f for f in repo_files if f.endswith('.nemo')]
            if not nemo_files:
                print(f"[predownload] è¿œç«¯ä»“åº“æœªå‘ç° .nemo æ–‡ä»¶: {model_id}ï¼Œè·³è¿‡é¢„ä¸‹è½½")
                return
            target_fname = nemo_files[0]
            print(f"[predownload] å‘ç°è¿œç«¯ .nemo æ–‡ä»¶: {target_fname}ï¼Œå¼€å§‹ä¸‹è½½åˆ° /app/models ...")
            try:
                downloaded_path = hf_hub_download(repo_id=model_id, filename=target_fname, cache_dir='/app/models')
                if downloaded_path and os.path.exists(downloaded_path):
                    print(f"[predownload] å·²ä¸‹è½½æ¨¡å‹æ–‡ä»¶: {downloaded_path}")
                else:
                    print(f"[predownload] ä¸‹è½½è¿”å›è·¯å¾„æ— æ•ˆæˆ–ä¸å­˜åœ¨: {downloaded_path}")
            except Exception as e:
                print(f"[predownload] hf_hub_download å¤±è´¥: {e}")
        except Exception as e:
            print(f"[predownload] æŸ¥è¯¢è¿œç«¯ä»“åº“æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
    except Exception as e:
        print(f"[predownload] é¢„ä¸‹è½½çº¿ç¨‹å¼‚å¸¸: {e}")

def unload_model():
    """ä»å†…å­˜/æ˜¾å­˜ä¸­å¸è½½æ¨¡å‹ã€‚"""
    global asr_model, last_request_time, cuda_available
    with model_lock:
        if asr_model is not None:
            print(f"æ¨¡å‹é—²ç½®è¶…è¿‡ {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿï¼Œæ­£åœ¨ä»å†…å­˜ä¸­å¸è½½...")
            
            # æ˜¾ç¤ºå¸è½½å‰çš„æ˜¾å­˜ä½¿ç”¨
            if cuda_available:
                allocated_before, _, total = get_gpu_memory_usage()
                print(f"å¸è½½å‰æ˜¾å­˜ä½¿ç”¨: {allocated_before:.2f}GB / {total:.2f}GB")
            
            asr_model = None
            
            # å¸è½½åç«‹å³æ‰§è¡Œæ·±åº¦æ¸…ç†
            idle_deep_memory_cleanup()
            
            # æ˜¾ç¤ºå¸è½½åçš„æ˜¾å­˜ä½¿ç”¨
            if cuda_available:
                allocated_after, _, total = get_gpu_memory_usage()
                print(f"å¸è½½åæ˜¾å­˜ä½¿ç”¨: {allocated_after:.2f}GB / {total:.2f}GB")
                print(f"é‡Šæ”¾æ˜¾å­˜: {allocated_before - allocated_after:.2f}GB")
            
            last_request_time = None # é‡ç½®è®¡æ—¶å™¨ï¼Œé˜²æ­¢é‡å¤å¸è½½
            print("âœ… æ¨¡å‹å·²æˆåŠŸå¸è½½å¹¶å®Œæˆæ·±åº¦æ¸…ç†ã€‚")

def model_cleanup_checker():
    """åå°çº¿ç¨‹ï¼Œå‘¨æœŸæ€§æ£€æŸ¥æ¨¡å‹æ˜¯å¦é—²ç½®è¿‡ä¹…å¹¶æ‰§è¡Œå¸è½½ã€‚"""
    last_cleanup_time = datetime.datetime.now()
    
    while True:
        # æ ¹æ®ç³»ç»ŸçŠ¶æ€è‡ªé€‚åº”è°ƒæ•´æ£€æŸ¥é—´éš”
        current_time = datetime.datetime.now()
        
        # åŸºç¡€ç›‘æ§é—´éš” - ä½¿ç”¨æ›´çŸ­çš„é—´éš”ä»¥ä¾¿æ›´é¢‘ç¹æ£€æŸ¥
        sleep_interval = IDLE_MONITORING_INTERVAL
        
        # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µå¹¶åœ¨éœ€è¦æ—¶å¼ºåˆ¶æ¸…ç†
        if check_memory_usage_and_cleanup():
            last_cleanup_time = current_time
        
        if asr_model is not None and last_request_time is not None:
            idle_duration = (current_time - last_request_time).total_seconds()
            
            # ä½¿ç”¨æ›´çŸ­çš„æ¨¡å‹å¸è½½é˜ˆå€¼
            model_unload_threshold = min(IDLE_TIMEOUT_MINUTES * 60, AUTO_MODEL_UNLOAD_THRESHOLD_MINUTES * 60)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½æ¨¡å‹
            if idle_duration > model_unload_threshold:
                print(f"æ¨¡å‹é—²ç½® {idle_duration/60:.1f} åˆ†é’Ÿï¼Œè¶…è¿‡é˜ˆå€¼ {model_unload_threshold/60:.1f} åˆ†é’Ÿ")
                unload_model()
                # æ¨¡å‹å¸è½½åç«‹å³æ‰§è¡Œæ·±åº¦æ¸…ç†
                idle_deep_memory_cleanup()
                last_cleanup_time = current_time
            
            # æ ¹æ®é—²ç½®æ—¶é—´è°ƒæ•´æ£€æŸ¥é¢‘ç‡
            elif idle_duration > IDLE_DEEP_CLEANUP_THRESHOLD:
                # é•¿æ—¶é—´é—²ç½®æ—¶ï¼Œé™ä½æ£€æŸ¥é¢‘ç‡ä½†æ‰§è¡Œæ·±åº¦æ¸…ç†
                sleep_interval = max(60, IDLE_MONITORING_INTERVAL * 2)  # æœ€å°‘1åˆ†é’Ÿé—´éš”
                if (current_time - last_cleanup_time).total_seconds() > IDLE_MEMORY_CLEANUP_INTERVAL:
                    print(f"æ‰§è¡Œå®šæœŸæ·±åº¦æ¸…ç† (é—²ç½® {idle_duration/60:.1f} åˆ†é’Ÿ)")
                    idle_deep_memory_cleanup()
                    last_cleanup_time = current_time
            
            elif idle_duration > IDLE_MEMORY_CLEANUP_INTERVAL:
                # ä¸­ç­‰é—²ç½®æ—¶é—´ï¼Œæ‰§è¡Œè½»é‡æ¸…ç†
                if (current_time - last_cleanup_time).total_seconds() > IDLE_MEMORY_CLEANUP_INTERVAL:
                    print(f"æ‰§è¡Œå®šæœŸå†…å­˜æ¸…ç† (é—²ç½® {idle_duration/60:.1f} åˆ†é’Ÿ)")
                    if AGGRESSIVE_MEMORY_CLEANUP and should_force_cleanup():
                        print("ğŸ§¹ æ‰§è¡Œé—²ç½®æœŸé—´å†…å­˜æ¸…ç†...")
                        aggressive_memory_cleanup()
                    else:
                        # å³ä½¿ä¸éœ€è¦å¼ºåˆ¶æ¸…ç†ï¼Œä¹Ÿè¿›è¡ŒåŸºç¡€æ¸…ç†
                        if cuda_available:
                            try:
                                torch.cuda.empty_cache()
                            except Exception:
                                pass
                        gc.collect()
                    last_cleanup_time = current_time
            
            # å³ä½¿åœ¨çŸ­æœŸé—²ç½®æ—¶ä¹Ÿè¿›è¡Œæœ€åŸºæœ¬çš„æ¸…ç†
            elif idle_duration > 60:  # é—²ç½®è¶…è¿‡1åˆ†é’Ÿ
                if (current_time - last_cleanup_time).total_seconds() > 120:  # æ¯2åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                    if cuda_available:
                        try:
                            torch.cuda.empty_cache()
                        except Exception:
                            pass
                    gc.collect()
                    last_cleanup_time = current_time
        
        else:
            # æ¨¡å‹æœªåŠ è½½æˆ–æœªæœ‰è¯·æ±‚æ—¶ï¼Œä½¿ç”¨è¾ƒé•¿çš„æ£€æŸ¥é—´éš”å¹¶å®šæœŸæ¸…ç†
            sleep_interval = max(60, IDLE_MONITORING_INTERVAL * 2)  # å‡å°‘åˆ°æœ€å°‘1åˆ†é’Ÿé—´éš”
            if (current_time - last_cleanup_time).total_seconds() > IDLE_MEMORY_CLEANUP_INTERVAL:
                print("æ‰§è¡Œæ— æ¨¡å‹çŠ¶æ€ä¸‹çš„å®šæœŸæ¸…ç†")
                aggressive_memory_cleanup()
                last_cleanup_time = current_time
        
        time.sleep(sleep_interval)


# --- Flask åº”ç”¨åˆå§‹åŒ– ---
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = '/app/temp_uploads'
app.config['MAX_CONTENT_LENGTH'] = 2000 * 1024 * 1024  

# --- è¾…åŠ©å‡½æ•° ---
def get_audio_duration(file_path: str) -> float:
    """ä½¿ç”¨ ffprobe è·å–éŸ³é¢‘æ–‡ä»¶çš„æ—¶é•¿ï¼ˆç§’ï¼‰"""
    command = [
        'ffprobe',
        '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        file_path
    ]
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        return float(result.stdout)
    except (subprocess.CalledProcessError, ValueError) as e:
        print(f"æ— æ³•è·å–æ–‡ä»¶ '{file_path}' çš„æ—¶é•¿: {e}")
        return 0.0

def format_srt_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º SRT æ—¶é—´æˆ³æ ¼å¼ HH:MM:SS,ms"""
    delta = datetime.timedelta(seconds=seconds)
    # æ ¼å¼åŒ–ä¸º 0:00:05.123000
    s = str(delta)
    # åˆ†å‰²ç§’å’Œå¾®ç§’
    if '.' in s:
        parts = s.split('.')
        integer_part = parts[0]
        fractional_part = parts[1][:3] # å–å‰ä¸‰ä½æ¯«ç§’
    else:
        integer_part = s
        fractional_part = "000"

    # å¡«å……å°æ—¶ä½
    if len(integer_part.split(':')) == 2:
        integer_part = "0:" + integer_part
    
    return f"{integer_part},{fractional_part}"


def segments_to_srt(segments: list) -> str:
    """å°† NeMo çš„åˆ†æ®µæ—¶é—´æˆ³è½¬æ¢ä¸º SRT æ ¼å¼å­—ç¬¦ä¸²"""
    srt_content = []
    for i, segment in enumerate(segments):
        start_time = format_srt_time(segment['start'])
        end_time = format_srt_time(segment['end'])
        text = segment['segment'].strip()
        if text and PREFERRED_LINE_LENGTH > 0:
            text = wrap_text_for_display(
                text,
                preferred_line_length=PREFERRED_LINE_LENGTH,
                max_lines=MAX_SUBTITLE_LINES,
            )
        
        if text: # ä»…æ·»åŠ æœ‰å†…å®¹çš„å­—å¹•
            srt_content.append(str(i + 1))
            srt_content.append(f"{start_time} --> {end_time}")
            srt_content.append(text)
            srt_content.append("") # ç©ºè¡Œåˆ†éš”
            
    return "\n".join(srt_content)


def parse_ffmpeg_silence_log(ffmpeg_stderr: str) -> list:
    """è§£æ ffmpeg silencedetect è¾“å‡ºï¼Œè¿”å›é™éŸ³åŒºé—´ [(start, end), ...]ã€‚"""
    import re
    silence_starts = []
    silence_intervals = []
    # silencedetect è¾“å‡ºç¤ºä¾‹:
    # [silencedetect @ 0x...] silence_start: 12.345
    # [silencedetect @ 0x...] silence_end: 13.789 | silence_duration: 1.444
    start_re = re.compile(r"silence_start:\s*([0-9.]+)")
    end_re = re.compile(r"silence_end:\s*([0-9.]+)")
    for line in ffmpeg_stderr.splitlines():
        m = start_re.search(line)
        if m:
            silence_starts.append(float(m.group(1)))
            continue
        m = end_re.search(line)
        if m and silence_starts:
            start = silence_starts.pop(0)
            end = float(m.group(1))
            silence_intervals.append((start, end))
    return silence_intervals


def find_nearest_silence(target_time: float, silence_intervals: list, max_shift: float) -> float:
    """åœ¨ target_time é™„è¿‘æŸ¥æ‰¾æœ€è¿‘çš„é™éŸ³è¾¹ç•Œï¼Œè¿”å›å»ºè®®çš„åˆ‡ç‰‡å¼€å§‹æ—¶é—´ã€‚è‹¥æœªæ‰¾åˆ°åˆé€‚é™éŸ³ç‚¹ï¼Œåˆ™è¿”å› target_timeã€‚"""
    if not silence_intervals:
        return target_time
    best_time = target_time
    best_dist = max_shift + 1.0
    for start, end in silence_intervals:
        for edge in (start, end):
            dist = abs(edge - target_time)
            if dist < best_dist and dist <= max_shift:
                best_dist = dist
                best_time = edge
    return best_time


def detect_silences_with_ffmpeg(source_wav: str) -> list:
    """ä½¿ç”¨ ffmpeg silencedetect æ£€æµ‹é™éŸ³åŒºé—´ã€‚"""
    command = [
        'ffmpeg', '-hide_banner', '-nostats', '-i', source_wav,
        '-af', f'silencedetect=noise={SILENCE_THRESHOLD_DB}:d={MIN_SILENCE_DURATION}',
        '-f', 'null', '-' 
    ]
    result = subprocess.run(command, capture_output=True, text=True)
    # æ— è®ºè¿”å›ç å¦‚ä½•ï¼Œstderr éƒ½åŒ…å« silencedetect è¾“å‡º
    return parse_ffmpeg_silence_log(result.stderr)

# --- Flask è·¯ç”± ---

@app.route('/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ç”¨äºDockerå¥åº·æ£€æŸ¥å’ŒæœåŠ¡ç›‘æ§
    """
    try:
        current_time = datetime.datetime.now()
        # æ£€æŸ¥åŸºæœ¬æœåŠ¡çŠ¶æ€
        health_status: Dict[str, Any] = {
            "status": "healthy",
            "timestamp": current_time.isoformat(),
            "service": "parakeet-api",
            "version": "1.0.0"
        }
        
        # æ£€æŸ¥CUDAçŠ¶æ€
        global cuda_available
        if cuda_available:
            try:
                allocated, reserved, total = get_gpu_memory_usage()
                health_status["gpu"] = {
                    "available": True,
                    "memory_allocated_gb": round(allocated, 2),
                    "memory_reserved_gb": round(reserved, 2),
                    "memory_total_gb": round(total, 2),
                    "memory_usage_percent": round((allocated/total)*100, 1) if total > 0 else 0,
                    "memory_reserved_percent": round((reserved/total)*100, 1) if total > 0 else 0
                }
            except Exception as e:
                health_status["gpu"] = {
                    "available": True,
                    "error": str(e)
                }
        else:
            health_status["gpu"] = {
                "available": False,
                "mode": "cpu"
            }
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€å’Œé—²ç½®ä¿¡æ¯
        model_info = {
            "loaded": asr_model is not None,
            "lazy_load": ENABLE_LAZY_LOAD
        }
        
        if last_request_time is not None:
            idle_seconds = (current_time - last_request_time).total_seconds()
            model_info["last_request_time"] = last_request_time.isoformat()
            model_info["idle_duration_seconds"] = round(idle_seconds, 1)
            model_info["idle_duration_minutes"] = round(idle_seconds / 60, 1)
            
            # æ·»åŠ é—²ç½®çŠ¶æ€åˆ†ç±»
            if idle_seconds > IDLE_TIMEOUT_MINUTES * 60:
                model_info["idle_status"] = "ready_for_unload"
            elif idle_seconds > IDLE_DEEP_CLEANUP_THRESHOLD:
                model_info["idle_status"] = "deep_idle"
            elif idle_seconds > IDLE_MEMORY_CLEANUP_INTERVAL:
                model_info["idle_status"] = "idle"
            else:
                model_info["idle_status"] = "active"
        else:
            model_info["idle_status"] = "no_requests" if asr_model is not None else "unloaded"
        
        health_status["model"] = model_info
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨
        memory = psutil.virtual_memory()
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
        except:
            cpu_percent = 0.0
            
        health_status["system"] = {
            "memory_usage_percent": memory.percent,
            "memory_available_gb": round(memory.available / 1024**3, 2),
            "memory_total_gb": round(memory.total / 1024**3, 2),
            "cpu_usage_percent": round(cpu_percent, 1)
        }
        
        # æ·»åŠ èµ„æºä¼˜åŒ–é…ç½®çŠ¶æ€
        health_status["optimization"] = {
            "aggressive_memory_cleanup": AGGRESSIVE_MEMORY_CLEANUP,
            "idle_timeout_minutes": IDLE_TIMEOUT_MINUTES,
            "idle_memory_cleanup_interval": IDLE_MEMORY_CLEANUP_INTERVAL,
            "idle_deep_cleanup_threshold": IDLE_DEEP_CLEANUP_THRESHOLD,
            "enable_idle_cpu_optimization": ENABLE_IDLE_CPU_OPTIMIZATION,
            "force_cleanup_threshold": FORCE_CLEANUP_THRESHOLD,
            "enable_aggressive_idle_optimization": ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION,
            "immediate_cleanup_after_request": IMMEDIATE_CLEANUP_AFTER_REQUEST,
            "memory_usage_alert_threshold_gb": MEMORY_USAGE_ALERT_THRESHOLD_GB,
            "auto_model_unload_threshold_minutes": AUTO_MODEL_UNLOAD_THRESHOLD_MINUTES,
            "idle_monitoring_interval": IDLE_MONITORING_INTERVAL
        }
        
        return jsonify(health_status), 200
        
    except Exception as e:
        error_status = {
            "status": "unhealthy",
            "timestamp": datetime.datetime.now().isoformat(),
            "error": str(e)
        }
        return jsonify(error_status), 500

@app.route('/health/simple', methods=['GET'])
def simple_health_check():
    """
    ç®€å•å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ä»…è¿”å›HTTP 200çŠ¶æ€
    """
    return "OK", 200

@app.route('/v1/audio/transcriptions', methods=['POST'])
def transcribe_audio():
    """
    å…¼å®¹ OpenAI çš„è¯­éŸ³è¯†åˆ«æ¥å£ï¼Œæ”¯æŒé•¿éŸ³é¢‘åˆ†ç‰‡å¤„ç†ã€‚
    """
    # --- -1. API Key è®¤è¯ ---
    if API_KEY:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({"error": "Authorization header is missing or invalid. It must be in 'Bearer <key>' format."}), 401
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != API_KEY:
            return jsonify({"error": "Invalid API key."}), 401

    # --- 0. ç¡®ä¿æ¨¡å‹åŠ è½½å¹¶æ›´æ–°æ—¶é—´æˆ³ ---
    try:
        # å¦‚æœæ‡’åŠ è½½å¯ç”¨ï¼Œåˆ™æŒ‰éœ€åŠ è½½ï¼›å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„å…¨å±€æ¨¡å‹
        local_asr_model = load_model_if_needed() if ENABLE_LAZY_LOAD else asr_model
        if not local_asr_model:
             # æ­¤æƒ…å†µæ¶µç›–äº†æ‡’åŠ è½½å¤±è´¥å’Œé¢„åŠ è½½å¤±è´¥ä¸¤ç§åœºæ™¯
             return jsonify({"error": "æ¨¡å‹åŠ è½½å¤±è´¥æˆ–å°šæœªåŠ è½½ï¼Œæ— æ³•å¤„ç†è¯·æ±‚"}), 500
    except Exception as e:
        return jsonify({"error": f"æ¨¡å‹åŠ è½½æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"}), 500
    
    # å¦‚æœå¯ç”¨äº†æ‡’åŠ è½½ï¼Œåˆ™æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
    if ENABLE_LAZY_LOAD:
        global last_request_time
        last_request_time = datetime.datetime.now()


    # --- 1. åŸºæœ¬æ ¡éªŒ ---
    if 'file' not in request.files:
        return jsonify({"error": "è¯·æ±‚ä¸­æœªæ‰¾åˆ°æ–‡ä»¶éƒ¨åˆ†"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    if not shutil.which('ffmpeg'):
        return jsonify({"error": "FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿ PATH ä¸­"}), 500
    if not shutil.which('ffprobe'):
        return jsonify({"error": "ffprobe æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿ PATH ä¸­"}), 500

    # è·å–è¯·æ±‚å‚æ•°
    model_name = request.form.get('model', 'whisper-1')
    response_format = request.form.get('response_format', 'json')  # æ”¯æŒ json, text, srt, verbose_json, vtt
    language = request.form.get('language', None)
    prompt = request.form.get('prompt', None)
    temperature = float(request.form.get('temperature', 0))
    
    print(f"æ¥æ”¶åˆ°è¯·æ±‚ï¼Œæ¨¡å‹: '{model_name}', å“åº”æ ¼å¼: '{response_format}', è¯­è¨€: '{language}'")

    # --- 0.5 è¯­è¨€ç™½åå•æ ¡éªŒï¼ˆWhisper å…¼å®¹è¡Œä¸ºï¼‰---
    # è‹¥å®¢æˆ·ç«¯æ˜¾å¼ä¼ å…¥ languageï¼Œæˆ‘ä»¬åªæ¥å—å—æ”¯æŒçš„ 25 ç§è¯­è¨€ï¼Œå¦åˆ™ç›´æ¥æ‹’ç»
    detected_language = None  # ç”¨äºå­˜å‚¨è‡ªåŠ¨æ£€æµ‹çš„è¯­è¨€
    if language:
        lang_norm = str(language).strip().lower().replace('_', '-')
        # å…¼å®¹åƒ "en-US" è¿™ç§åŒºåŸŸç ï¼šåªå–ä¸»è¯­è¨€éƒ¨åˆ†
        primary = lang_norm.split('-')[0]
        if primary not in SUPPORTED_LANG_CODES:
            # ä¸ Whisper çš„é£æ ¼ä¿æŒä¸€è‡´ï¼šè¿”å› 400ï¼Œå¹¶åœ¨ message ä¸­æç¤ºä¸æ”¯æŒ
            return jsonify({
                "error": {
                    "message": f"Unsupported language: {language}",
                    "type": "invalid_request_error",
                    "param": "language",
                    "code": "unsupported_language"
                }
            }), 400

    original_filename = secure_filename(str(file.filename or 'uploaded_file'))
    unique_id = str(uuid.uuid4())
    temp_original_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_{original_filename}")
    target_wav_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}.wav")
    
    # ç”¨äºæ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶çš„åˆ—è¡¨
    temp_files_to_clean = []

    try:
        # --- 2. ä¿å­˜å¹¶ç»Ÿä¸€è½¬æ¢ä¸º 16k å•å£°é“ WAV ---
        file.save(temp_original_path)
        temp_files_to_clean.append(temp_original_path)
        
        print(f"[{unique_id}] æ­£åœ¨å°† '{original_filename}' è½¬æ¢ä¸ºæ ‡å‡† WAV æ ¼å¼...")
        # å¯é€‰å‰å¤„ç†æ»¤æ³¢å™¨
        ffmpeg_filters = []
        if ENABLE_FFMPEG_DENOISE:
            ffmpeg_filters.append(DENOISE_FILTER)
        ffmpeg_command = [
            'ffmpeg', '-y', '-vn', '-sn', '-dn', '-i', temp_original_path,
            '-ac', '1', '-ar', '16000'
        ]
        if ffmpeg_filters:
            ffmpeg_command += ['-af', ','.join(ffmpeg_filters)]
        ffmpeg_command += [target_wav_path]
        result = subprocess.run(ffmpeg_command, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"FFmpeg é”™è¯¯: {result.stderr}")
            return jsonify({"error": "æ–‡ä»¶è½¬æ¢å¤±è´¥", "details": result.stderr}), 500
        temp_files_to_clean.append(target_wav_path)

        # --- 2.5 è‡ªåŠ¨è¯­è¨€æ£€æµ‹å’ŒéªŒè¯ï¼ˆæœªæ˜¾å¼ä¼  language æ—¶ï¼‰---
        if not language:
            try:
                lid_clip_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_lid.wav")
                temp_files_to_clean.append(lid_clip_path)
                # å–çŸ­ç‰‡æ®µè¿›è¡Œå¿«é€Ÿè½¬å†™
                clip_seconds = max(5, int(LID_CLIP_SECONDS))
                probe_dur = get_audio_duration(target_wav_path)
                if probe_dur > 0:
                    clip_seconds = min(clip_seconds, int(math.ceil(probe_dur)))
                clip_cmd = [
                    'ffmpeg', '-y', '-i', target_wav_path,
                    '-t', str(clip_seconds),
                    '-ac', '1', '-ar', '16000',
                    lid_clip_path
                ]
                _res = subprocess.run(clip_cmd, capture_output=True, text=True)
                if _res.returncode == 0 and os.path.exists(lid_clip_path):
                    # ä»…æ–‡æœ¬æ¨ç†ï¼ˆä¸å¼€æ—¶é—´æˆ³ï¼Œé™ä½å¼€é”€ï¼‰
                    with inference_semaphore:
                        lid_out = safe_transcribe(
                            local_asr_model,
                            lid_clip_path,
                            need_timestamps=False,
                            batch_size=1,
                            num_workers=0,
                        )
                    # æå–æ–‡æœ¬
                    lid_text = ""
                    if isinstance(lid_out, list) and lid_out:
                        first = lid_out[0]
                        try:
                            if hasattr(first, 'text') and first.text:
                                lid_text = str(first.text)
                            elif hasattr(first, 'segment') and first.segment:
                                lid_text = str(first.segment)
                            else:
                                lid_text = str(first)
                        except Exception:
                            lid_text = str(first)

                    # ç”¨è½»é‡æ–‡æœ¬è¯­è¨€è¯†åˆ«åšè¯­è¨€æ£€æµ‹
                    if lid_text and lid_text.strip():
                        try:
                            try:
                                from langdetect import detect  # type: ignore
                            except Exception:
                                detect = None  # type: ignore
                            detected = None
                            if detect is not None:
                                detected = detect(lid_text)
                            # è‹¥èƒ½æ£€æµ‹åˆ°è¯­è¨€
                            if detected:
                                det_primary = str(detected).strip().lower().split('-')[0]
                                if det_primary:
                                    if det_primary in SUPPORTED_LANG_CODES:
                                        # æ£€æµ‹åˆ°æ”¯æŒçš„è¯­è¨€ï¼Œå­˜å‚¨ç”¨äºåç»­ä½¿ç”¨
                                        detected_language = det_primary
                                        print(f"[{unique_id}] è‡ªåŠ¨æ£€æµ‹åˆ°è¯­è¨€: {detected_language}")
                                    elif ENABLE_AUTO_LANGUAGE_REJECTION:
                                        # æ£€æµ‹åˆ°ä¸æ”¯æŒçš„è¯­è¨€ä¸”å¯ç”¨äº†è‡ªåŠ¨æ‹’ç»
                                        return jsonify({
                                            "error": {
                                                "message": f"Unsupported language: {detected}",
                                                "type": "invalid_request_error",
                                                "param": "language",
                                                "code": "unsupported_language"
                                            }
                                        }), 400
                                    else:
                                        # æ£€æµ‹åˆ°ä¸æ”¯æŒçš„è¯­è¨€ä½†æœªå¯ç”¨è‡ªåŠ¨æ‹’ç»ï¼Œé»˜è®¤ä¸ºè‹±è¯­
                                        detected_language = "en"
                                        print(f"[{unique_id}] æ£€æµ‹åˆ°ä¸æ”¯æŒçš„è¯­è¨€ {detected}ï¼Œé»˜è®¤ä½¿ç”¨è‹±è¯­")
                        except Exception as _e:
                            # æ£€æµ‹å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œé»˜è®¤ä½¿ç”¨è‹±è¯­
                            print(f"[{unique_id}] è¯­è¨€è‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨è‹±è¯­: {_e}")
                            detected_language = "en"
                    else:
                        # æ— æ³•æå–æ–‡æœ¬ï¼Œé»˜è®¤ä½¿ç”¨è‹±è¯­
                        print(f"[{unique_id}] æ— æ³•æå–æ–‡æœ¬è¿›è¡Œè¯­è¨€æ£€æµ‹ï¼Œé»˜è®¤ä½¿ç”¨è‹±è¯­")
                        detected_language = "en"
            except Exception as _e:
                print(f"[{unique_id}] è‡ªåŠ¨è¯­è¨€æ£€æµ‹é˜¶æ®µå¼‚å¸¸ï¼Œé»˜è®¤ä½¿ç”¨è‹±è¯­: {_e}")
                detected_language = "en"

        # --- 3. éŸ³é¢‘åˆ‡ç‰‡ (Chunking) ---
        # åŠ¨æ€è°ƒæ•´chunkå¤§å°åŸºäºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        heavy_ts_request = response_format in ['srt', 'vtt', 'verbose_json']
        if cuda_available:
            allocated, _, total = get_gpu_memory_usage()
            memory_usage_ratio = allocated / total if total > 0 else 0
            
            if memory_usage_ratio > 0.6:  # å¦‚æœæ˜¾å­˜ä½¿ç”¨è¶…è¿‡60%
                # å‡å°‘chunkå¤§å°ä»¥é™ä½æ˜¾å­˜å‹åŠ›
                adjusted_chunk_minutes = max(3, CHUNK_MINITE - 2)
                print(f"[{unique_id}] æ˜¾å­˜ä½¿ç”¨è¾ƒé«˜({memory_usage_ratio*100:.1f}%)ï¼Œè°ƒæ•´chunkå¤§å°ä» {CHUNK_MINITE} åˆ†é’Ÿåˆ° {adjusted_chunk_minutes} åˆ†é’Ÿ")
                CHUNK_DURATION_SECONDS = adjusted_chunk_minutes * 60
            else:
                CHUNK_DURATION_SECONDS = CHUNK_MINITE * 60
            # ä¸º â‰¤8~12GB æ˜¾å­˜è®¾å¤‡æˆ–éœ€è¦æ—¶é—´æˆ³çš„è¯·æ±‚è®¾ç½®æ›´ä¿å®ˆçš„ä¸Šé™ï¼Œé¿å…æ³¨æ„åŠ›çŸ©é˜µ OOM
            try:
                vram_gb = total
                cap_env = os.environ.get('CHUNK_SECONDS_CAP', '').strip()
                if cap_env:
                    cap_sec = int(float(cap_env))
                else:
                    if vram_gb <= 8.5:
                        cap_sec = 180 if heavy_ts_request else 240
                    elif vram_gb <= 12.0:
                        cap_sec = 300 if heavy_ts_request else 480
                    else:
                        cap_sec = 600
                if CHUNK_DURATION_SECONDS > cap_sec:
                    print(f"[{unique_id}] åŸºäºGPUæ˜¾å­˜({vram_gb:.1f}GB){'ä¸”éœ€æ—¶é—´æˆ³' if heavy_ts_request else ''}ï¼Œé™åˆ¶chunkæ—¶é•¿ä¸º {cap_sec}s")
                    CHUNK_DURATION_SECONDS = cap_sec
            except Exception:
                pass
        else:
            # CPUæ¨¡å¼ä¸‹ä½¿ç”¨è¾ƒå°çš„chunkä»¥é¿å…å†…å­˜ä¸è¶³
            cpu_chunk_minutes = max(3, CHUNK_MINITE // 2)  # CPUæ¨¡å¼å‡åŠchunkå¤§å°
            print(f"[{unique_id}] CPUæ¨¡å¼ï¼Œè°ƒæ•´chunkå¤§å°åˆ° {cpu_chunk_minutes} åˆ†é’Ÿ")
            CHUNK_DURATION_SECONDS = cpu_chunk_minutes * 60
            # CPU æ¨¡å¼ä¹Ÿè®¾ç½®ä¸Šé™ï¼Œå°¤å…¶åœ¨éœ€è¦æ—¶é—´æˆ³æ—¶
            try:
                cap_env = os.environ.get('CHUNK_SECONDS_CAP', '').strip()
                cap_sec = int(float(cap_env)) if cap_env else (180 if heavy_ts_request else 240)
                if CHUNK_DURATION_SECONDS > cap_sec:
                    print(f"[{unique_id}] CPUæ¨¡å¼é™åˆ¶chunkæ—¶é•¿ä¸º {cap_sec}s")
                    CHUNK_DURATION_SECONDS = cap_sec
            except Exception:
                pass
            
        total_duration = get_audio_duration(target_wav_path)
        if total_duration == 0:
            return jsonify({"error": "æ— æ³•å¤„ç†æ—¶é•¿ä¸º0çš„éŸ³é¢‘"}), 400

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡ç‰‡ï¼Œå¦‚æœéŸ³é¢‘æ—¶é•¿å°äºåˆ‡ç‰‡é˜ˆå€¼ï¼Œåˆ™ç›´æ¥å¤„ç†
        if total_duration <= CHUNK_DURATION_SECONDS:
            print(f"[{unique_id}] æ–‡ä»¶æ€»æ—¶é•¿: {total_duration:.2f}s. å°äºåˆ‡ç‰‡é˜ˆå€¼({CHUNK_DURATION_SECONDS}s)ï¼Œæ— éœ€åˆ‡ç‰‡ã€‚")
            chunk_paths = [target_wav_path]
            chunk_info_list = [{'start': 0, 'end': total_duration, 'duration': total_duration}]
            num_chunks = 1
        else:
            # ä½¿ç”¨é‡å åˆ†å‰²ç­–ç•¥
            if ENABLE_OVERLAP_CHUNKING:
                print(f"[{unique_id}] å¯ç”¨é‡å åˆ†å‰²æ¨¡å¼ï¼Œé‡å æ—¶é•¿: {CHUNK_OVERLAP_SECONDS}s")
                chunk_info_list = create_overlap_chunks(total_duration, CHUNK_DURATION_SECONDS, CHUNK_OVERLAP_SECONDS)
            else:
                # ä¼ ç»Ÿç¡¬åˆ†å‰²
                num_chunks = math.ceil(total_duration / CHUNK_DURATION_SECONDS)
                chunk_info_list = []
                for i in range(num_chunks):
                    start_time = i * CHUNK_DURATION_SECONDS
                    end_time = min(start_time + CHUNK_DURATION_SECONDS, total_duration)
                    chunk_info_list.append({
                        'start': start_time,
                        'end': end_time, 
                        'duration': end_time - start_time
                    })
            
            chunk_paths = []
            num_chunks = len(chunk_info_list)
            print(f"[{unique_id}] æ–‡ä»¶æ€»æ—¶é•¿: {total_duration:.2f}s. å°†åˆ‡åˆ†ä¸º {num_chunks} ä¸ªç‰‡æ®µã€‚")
            
            # è‹¥å¯ç”¨é™éŸ³å¯¹é½ï¼Œåˆ™é¢„å…ˆæ£€æµ‹é™éŸ³åŒºé—´
            silence_intervals = []
            if ENABLE_SILENCE_ALIGNED_CHUNKING and total_duration > CHUNK_DURATION_SECONDS:
                print(f"[{unique_id}] æ£€æµ‹é™éŸ³åŒºé—´ç”¨äºåˆ†å‰²å¯¹é½: noise={SILENCE_THRESHOLD_DB}, min_dur={MIN_SILENCE_DURATION}s")
                silence_intervals = detect_silences_with_ffmpeg(target_wav_path)
                print(f"[{unique_id}] å…±æ£€æµ‹åˆ° {len(silence_intervals)} æ®µé™éŸ³åŒºé—´")

            for i, chunk_info in enumerate(chunk_info_list):
                chunk_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{unique_id}_chunk_{i}.wav")
                chunk_paths.append(chunk_path)
                temp_files_to_clean.append(chunk_path)
                
                start_time = chunk_info['start']
                # å°†åˆ‡ç‰‡å¼€å§‹å¯¹é½åˆ°æœ€è¿‘é™éŸ³è¾¹ç•Œï¼ˆä¸è¶…è¿‡æœ€å¤§åç§»ï¼‰
                if ENABLE_SILENCE_ALIGNED_CHUNKING and silence_intervals:
                    aligned_start = find_nearest_silence(start_time, silence_intervals, SILENCE_MAX_SHIFT_SECONDS)
                    if aligned_start != start_time:
                        print(f"[{unique_id}] åˆ‡ç‰‡{i+1} å¼€å§‹æ—¶é—´ {start_time:.2f}s å¯¹é½è‡³é™éŸ³ {aligned_start:.2f}s")
                        # åŒæ—¶è°ƒæ•´è¯¥chunkçš„ç»“æŸï¼Œä¿æŒ duration ä¸å˜
                        shift = aligned_start - start_time
                        start_time = max(0.0, aligned_start)
                        chunk_info['start'] = start_time
                        chunk_info['end'] = min(total_duration, chunk_info['end'] + shift)
                        chunk_info['duration'] = chunk_info['end'] - chunk_info['start']
                duration = chunk_info['duration']
                
                print(f"[{unique_id}] æ­£åœ¨åˆ›å»ºåˆ‡ç‰‡ {i+1}/{num_chunks} ({start_time:.1f}s - {chunk_info['end']:.1f}s)...")
                chunk_command = [
                    'ffmpeg', '-y', '-i', target_wav_path,
                    '-ss', str(start_time),
                    '-t', str(duration),
                    '-c', 'copy',
                    chunk_path
                ]
                result = subprocess.run(chunk_command, capture_output=True, text=True)
                if result.returncode != 0:
                    print(f"[{unique_id}] âš ï¸ åˆ›å»ºåˆ‡ç‰‡ {i+1} æ—¶å‡ºç°è­¦å‘Š: {result.stderr}")
                    # ç»§ç»­å¤„ç†ï¼Œä¸ä¸­æ–­
            
        # --- 4. å¾ªç¯è½¬å½•å¹¶åˆå¹¶ç»“æœ ---
        all_segments = []
        all_words = []
        chunk_boundaries = []
        # ä»…åœ¨éœ€è¦ SRT/VTT/verbose_json æ—¶è¯·æ±‚æ—¶é—´æˆ³ï¼Œå‡å°‘æ˜¾å­˜ä¸è®¡ç®—
        need_timestamps = response_format in ['srt', 'vtt', 'verbose_json']
        # å½“éœ€è¦è¿›è¡Œé•¿å­—å¹•åˆ‡åˆ†ä¸”å¯ç”¨äº†åŸºäºè¯æ—¶é—´æˆ³çš„åˆ‡åˆ†æ—¶ï¼Œä¹Ÿå°è¯•æ”¶é›†è¯çº§æ—¶é—´æˆ³
        collect_word_timestamps = (response_format == 'verbose_json') or (SPLIT_LONG_SUBTITLES and ENABLE_WORD_TIMESTAMPS_FOR_SPLIT)
        full_text_parts = []  # å½“ä¸éœ€è¦æ—¶é—´æˆ³æ—¶ï¼Œç›´æ¥æ”¶é›†æ–‡æœ¬

        for i, (chunk_path, chunk_info) in enumerate(zip(chunk_paths, chunk_info_list)):
            print(f"[{unique_id}] æ­£åœ¨è½¬å½•åˆ‡ç‰‡ {i+1}/{num_chunks}...")
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœè¿‡é«˜åˆ™å¼ºåˆ¶æ¸…ç†
            if should_force_cleanup():
                print(f"[{unique_id}] æ˜¾å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†...")
                aggressive_memory_cleanup()
            
            # æ˜¾ç¤ºå½“å‰æ˜¾å­˜/å†…å­˜ä½¿ç”¨
            if cuda_available:
                allocated, _, total = get_gpu_memory_usage()
                print(f"[{unique_id}] å¤„ç†åˆ‡ç‰‡ {i+1} å‰æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.2f}GB")
            else:
                # æ˜¾ç¤ºCPUå†…å­˜ä½¿ç”¨
                memory = psutil.virtual_memory()
                print(f"[{unique_id}] å¤„ç†åˆ‡ç‰‡ {i+1} å‰å†…å­˜ä½¿ç”¨: {memory.used/1024**3:.2f}GB / {memory.total/1024**3:.2f}GB ({memory.percent:.1f}%)")
            
            # å¯¹å½“å‰åˆ‡ç‰‡è¿›è¡Œè½¬å½•
            # ä½¿ç”¨ with torch.cuda.amp.autocast() åœ¨åŠç²¾åº¦ä¸‹è¿è¡Œæ¨ç†
            # æ¨ç†æ¨¡å¼è¿›ä¸€æ­¥é™ä½å†…å­˜/å¼€é”€ï¼Œå¹¶å‘æ§åˆ¶é¿å… OOM
            with inference_semaphore:
                output = safe_transcribe(
                    local_asr_model,
                    chunk_path,
                    need_timestamps=need_timestamps,
                    batch_size=TRANSCRIBE_BATCH_SIZE,
                    num_workers=TRANSCRIBE_NUM_WORKERS,
                )

            # ç«‹å³è¿›è¡Œå†…å­˜æ¸…ç†
            if AGGRESSIVE_MEMORY_CLEANUP:
                aggressive_memory_cleanup()
            else:
                if cuda_available:
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass
                gc.collect()
            
            # è®°å½•chunkè¾¹ç•Œç”¨äºåç»­åˆå¹¶
            chunk_start_offset = chunk_info['start']
            chunk_boundaries.append(chunk_start_offset)
            
            if need_timestamps:
                if output and getattr(output[0], 'timestamp', None):
                    # ä¿®æ­£å¹¶æ”¶é›† segment æ—¶é—´æˆ³
                    if 'segment' in output[0].timestamp:
                        for seg in output[0].timestamp['segment']:
                            seg['start'] += chunk_start_offset
                            seg['end'] += chunk_start_offset
                            all_segments.append(seg)
                    # ä¿®æ­£å¹¶æ”¶é›† word æ—¶é—´æˆ³ï¼ˆä»…åœ¨ verbose_json éœ€è¦ï¼‰
                    if collect_word_timestamps and 'word' in output[0].timestamp:
                        for word in output[0].timestamp['word']:
                            word['start'] += chunk_start_offset
                            word['end'] += chunk_start_offset
                            all_words.append(word)
                else:
                    # æŸäº›æ¨¡å‹/é…ç½®å¯èƒ½ä¸è¿”å›æ—¶é—´æˆ³ï¼Œå°è¯•ç›´æ¥æ–‡æœ¬å›é€€
                    if isinstance(output, list) and output:
                        full_text_parts.append(str(output[0]))
            else:
                # ä¸éœ€è¦æ—¶é—´æˆ³ï¼Œç›´æ¥å–æ–‡æœ¬
                if isinstance(output, list) and output:
                    # NeMo è¿”å›çš„å…ƒç´ å¯èƒ½æ˜¯ Hypothesis å¯¹è±¡ï¼Œä¼˜å…ˆæå– .text æˆ– .segment å­—æ®µ
                    first = output[0]
                    try:
                        # ä¼˜å…ˆä½¿ç”¨å¸¸è§å±æ€§
                        if hasattr(first, 'text') and first.text:
                            full_text_parts.append(str(first.text))
                        elif hasattr(first, 'segment') and first.segment:
                            full_text_parts.append(str(first.segment))
                        else:
                            full_text_parts.append(str(first))
                    except Exception:
                        full_text_parts.append(str(first))
            
            # é‡Šæ”¾ä¸´æ—¶è¾“å‡ºå¼•ç”¨
            try:
                del output
            except Exception:
                pass
            # ç«‹å³åˆ é™¤å·²å¤„ç†çš„chunkæ–‡ä»¶ä»¥èŠ‚çœç£ç›˜ç©ºé—´å’Œå†…å­˜
            if num_chunks > 1 and os.path.exists(chunk_path):
                try:
                    os.remove(chunk_path)
                    temp_files_to_clean.remove(chunk_path)
                    print(f"[{unique_id}] å·²åˆ é™¤å¤„ç†å®Œæˆçš„åˆ‡ç‰‡æ–‡ä»¶: chunk_{i}")
                except Exception as e:
                    print(f"[{unique_id}] åˆ é™¤åˆ‡ç‰‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        print(f"[{unique_id}] æ‰€æœ‰åˆ‡ç‰‡è½¬å½•å®Œæˆï¼Œæ­£åœ¨åˆå¹¶ç»“æœã€‚")
        
        # --- 4.5. å¤„ç†é‡å åŒºåŸŸå¹¶åˆå¹¶segments ---
        if ENABLE_OVERLAP_CHUNKING and len(chunk_boundaries) > 1:
            print(f"[{unique_id}] å¤„ç†é‡å åŒºåŸŸï¼Œå»é™¤é‡å¤å†…å®¹...")
            all_segments = merge_overlapping_segments(all_segments, chunk_boundaries, CHUNK_OVERLAP_SECONDS)
            print(f"[{unique_id}] é‡å å¤„ç†å®Œæˆï¼Œæœ€ç»ˆsegmentsæ•°é‡: {len(all_segments)}")

        # --- 4.6. å­—å¹•åå¤„ç†ï¼šåˆå¹¶/å»¶é•¿è¿‡çŸ­å­—å¹•ï¼Œé¿å…é—ªçƒ ---
        if MERGE_SHORT_SUBTITLES and all_segments:
            before_cnt = len(all_segments)
            all_segments = enforce_min_subtitle_duration(
                all_segments,
                min_duration=MIN_SUBTITLE_DURATION_SECONDS,
                merge_max_gap=SHORT_SUBTITLE_MERGE_MAX_GAP_SECONDS,
                min_chars=SHORT_SUBTITLE_MIN_CHARS,
                min_gap=SUBTITLE_MIN_GAP_SECONDS,
            )
            print(f"[{unique_id}] å­—å¹•åå¤„ç†å®Œæˆï¼š{before_cnt} -> {len(all_segments)} æ®µï¼ˆæœ€å°æ—¶é•¿ {MIN_SUBTITLE_DURATION_SECONDS}sï¼‰")

        # --- 4.7. é•¿å­—å¹•æ‹†åˆ†ï¼ˆæŒ‰æ—¶é•¿/å­—ç¬¦æ•°é™åˆ¶ï¼‰ ---
        if SPLIT_LONG_SUBTITLES and all_segments:
            before_cnt = len(all_segments)
            all_segments = split_and_wrap_long_subtitles(
                segments=all_segments,
                words=all_words if collect_word_timestamps else None,
                max_duration=MAX_SUBTITLE_DURATION_SECONDS,
                max_chars=MAX_SUBTITLE_CHARS_PER_SEGMENT,
                preferred_line_length=PREFERRED_LINE_LENGTH,
                max_lines=MAX_SUBTITLE_LINES,
                punctuation=SUBTITLE_SPLIT_PUNCTUATION,
            )
            print(f"[{unique_id}] é•¿å­—å¹•æ‹†åˆ†å®Œæˆï¼š{before_cnt} -> {len(all_segments)} æ®µï¼ˆæœ€å¤§æ—¶é•¿ {MAX_SUBTITLE_DURATION_SECONDS}s, æœ€å¤§å­—ç¬¦ {MAX_SUBTITLE_CHARS_PER_SEGMENT}ï¼‰")

        # --- 5. æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º ---
        # å¦‚æœæ—¢æ²¡æœ‰æ—¶é—´æˆ³æ®µï¼Œä¹Ÿæ²¡æœ‰ç›´æ¥æ–‡æœ¬ï¼Œåˆ™è§†ä¸ºå¤±è´¥ï¼›
        # å¦åˆ™å³ä½¿æ²¡æœ‰ segmentsï¼ˆä¾‹å¦‚æ¨¡å‹åªè¿”å›çº¯æ–‡æœ¬ï¼‰ï¼Œä¹Ÿåº”è¿”å›æ–‡æœ¬ç»“æœã€‚
        if not all_segments and not full_text_parts:
            return jsonify({"error": "è½¬å½•å¤±è´¥ï¼Œæ¨¡å‹æœªè¿”å›ä»»ä½•æœ‰æ•ˆå†…å®¹"}), 500

        # æ„å»ºå®Œæ•´çš„è½¬å½•æ–‡æœ¬
        full_text = " ".join([seg['segment'].strip() for seg in all_segments if seg['segment'].strip()])
        
        # æ ¹æ® response_format è¿”å›ä¸åŒæ ¼å¼
        if response_format == 'text':
            if not full_text:
                # å½“æœªå¯ç”¨æ—¶é—´æˆ³ä¸”ç›´æ¥æ”¶é›†æ–‡æœ¬
                full_text = " ".join(full_text_parts) if full_text_parts else ""
            return Response(full_text, mimetype='text/plain')
        elif response_format == 'srt':
            srt_result = segments_to_srt(all_segments)
            return Response(srt_result, mimetype='text/plain')
        elif response_format == 'vtt':
            vtt_result = segments_to_vtt(all_segments)
            return Response(vtt_result, mimetype='text/plain')
        elif response_format == 'verbose_json':
            # è¯¦ç»†çš„ JSON æ ¼å¼ï¼ŒåŒ…å«æ›´å¤šä¿¡æ¯
            response_data = {
                "task": "transcribe",
                "language": language or detected_language or "en",
                "duration": total_duration,
                "text": full_text,
                "segments": [
                    {
                        "id": i,
                        "seek": int(seg['start'] * 100),  # è½¬æ¢ä¸º centiseconds
                        "start": seg['start'],
                        "end": seg['end'],
                        "text": seg['segment'].strip(),
                        "tokens": [],  # NeMo ä¸æä¾› tokensï¼Œç•™ç©º
                        "temperature": temperature,
                        "avg_logprob": -0.5,  # æ¨¡æ‹Ÿå€¼
                        "compression_ratio": 1.0,  # æ¨¡æ‹Ÿå€¼
                        "no_speech_prob": 0.0,  # æ¨¡æ‹Ÿå€¼
                        "words": [
                            {
                                "word": word['word'],
                                "start": word['start'],
                                "end": word['end'],
                                "probability": 0.9  # æ¨¡æ‹Ÿå€¼
                            }
                            for word in all_words 
                            if word['start'] >= seg['start'] and word['end'] <= seg['end']
                        ] if all_words else []
                    }
                    for i, seg in enumerate(all_segments) if seg['segment'].strip()
                ]
            }
            return jsonify(response_data)
        else:
            # é»˜è®¤ JSON æ ¼å¼ (response_format == 'json')
            if not all_segments:
                # å½“æœªå¯ç”¨æ—¶é—´æˆ³ï¼Œtext æ¥è‡ª direct è¾“å‡º
                if not full_text:
                    full_text = " ".join(full_text_parts) if full_text_parts else ""
            response_data = {"text": full_text}
            return jsonify(response_data)

    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", "details": str(e)}), 500
    finally:
        # --- 6. æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ ---
        print(f"[{unique_id}] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for f_path in temp_files_to_clean:
            if os.path.exists(f_path):
                os.remove(f_path)
        print(f"[{unique_id}] ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
        
        # --- 7. ç«‹å³æ‰§è¡Œè¯·æ±‚åæ¸…ç† ---
        immediate_post_request_cleanup()
        
        # --- 8. å¼ºåˆ¶æ¸…ç†å†…å­˜ï¼Œé¿å…ç´¯ç§¯ ---
        print(f"[{unique_id}] æ‰§è¡Œæœ€ç»ˆå†…å­˜æ¸…ç†...")
        if cuda_available:
            allocated_before, _, total = get_gpu_memory_usage()
            print(f"[{unique_id}] æ¸…ç†å‰æ˜¾å­˜ä½¿ç”¨: {allocated_before:.2f}GB / {total:.2f}GB")
        else:
            memory_before = psutil.virtual_memory()
            print(f"[{unique_id}] æ¸…ç†å‰å†…å­˜ä½¿ç”¨: {memory_before.used/1024**3:.2f}GB / {memory_before.total/1024**3:.2f}GB")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¶…çº§æ¿€è¿›æ¸…ç†
        needs_ultra_cleanup = False
        if cuda_available and allocated_before > MEMORY_USAGE_ALERT_THRESHOLD_GB:
            needs_ultra_cleanup = True
        elif not cuda_available and memory_before.used/1024**3 > MEMORY_USAGE_ALERT_THRESHOLD_GB:
            needs_ultra_cleanup = True
        
        if needs_ultra_cleanup and ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION:
            print(f"[{unique_id}] å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ‰§è¡Œè¶…çº§æ¿€è¿›æ¸…ç†")
            ultra_aggressive_memory_cleanup()
        else:
            aggressive_memory_cleanup()
        
        if cuda_available:
            allocated_after, _, total = get_gpu_memory_usage()
            print(f"[{unique_id}] æ¸…ç†åæ˜¾å­˜ä½¿ç”¨: {allocated_after:.2f}GB / {total:.2f}GB")
            if allocated_before > 0:
                print(f"[{unique_id}] é‡Šæ”¾æ˜¾å­˜: {allocated_before - allocated_after:.2f}GB")
        else:
            memory_after = psutil.virtual_memory()
            print(f"[{unique_id}] æ¸…ç†åå†…å­˜ä½¿ç”¨: {memory_after.used/1024**3:.2f}GB / {memory_after.total/1024**3:.2f}GB")
        print(f"[{unique_id}] å†…å­˜æ¸…ç†å®Œæˆã€‚")


def segments_to_vtt(segments: list) -> str:
    """å°† NeMo çš„åˆ†æ®µæ—¶é—´æˆ³è½¬æ¢ä¸º VTT æ ¼å¼å­—ç¬¦ä¸²"""
    vtt_content = ["WEBVTT", ""]
    
    for i, segment in enumerate(segments):
        start_time = format_vtt_time(segment['start'])
        end_time = format_vtt_time(segment['end'])
        text = segment['segment'].strip()
        if text and PREFERRED_LINE_LENGTH > 0:
            text = wrap_text_for_display(
                text,
                preferred_line_length=PREFERRED_LINE_LENGTH,
                max_lines=MAX_SUBTITLE_LINES,
            )
        
        if text:  # ä»…æ·»åŠ æœ‰å†…å®¹çš„å­—å¹•
            vtt_content.append(f"{start_time} --> {end_time}")
            vtt_content.append(text)
            vtt_content.append("")  # ç©ºè¡Œåˆ†éš”
            
    return "\n".join(vtt_content)


def format_vtt_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º VTT æ—¶é—´æˆ³æ ¼å¼ HH:MM:SS.mmm"""
    delta = datetime.timedelta(seconds=seconds)
    # æ ¼å¼åŒ–ä¸º 0:00:05.123000
    s = str(delta)
    # åˆ†å‰²ç§’å’Œå¾®ç§’
    if '.' in s:
        parts = s.split('.')
        integer_part = parts[0]
        fractional_part = parts[1][:3]  # å–å‰ä¸‰ä½æ¯«ç§’
    else:
        integer_part = s
        fractional_part = "000"

    # å¡«å……å°æ—¶ä½
    if len(integer_part.split(':')) == 2:
        integer_part = "0:" + integer_part
    
    return f"{integer_part}.{fractional_part}"


def wrap_text_for_display(text: str, preferred_line_length: int, max_lines: int) -> str:
    """å°†å•è¡Œæ–‡æœ¬æŒ‰å­—æ•°è½¯æ¢è¡Œä¸ºæœ€å¤š max_lines è¡Œï¼Œå°½é‡åœ¨ç©ºæ ¼æˆ–æ ‡ç‚¹å¤„æ–­è¡Œã€‚
    è‹¥æ–‡æœ¬è¶…è¿‡è¡Œæ•°é™åˆ¶ï¼Œåç»­ä»ä¿ç•™ä½†ä¸å¼ºåˆ¶å¢åŠ è¡Œæ•°ï¼ˆSRT/VTTå¯å¤šè¡Œï¼‰ã€‚
    """
    if preferred_line_length <= 0 or max_lines <= 0:
        return text
    import re
    words = re.findall(r"\S+|\s+", text)
    lines = []
    current = ""
    for token in words:
        tentative = current + token
        if len(tentative.strip()) <= preferred_line_length or not current:
            current = tentative
        else:
            lines.append(current.strip())
            current = token.lstrip()
            if len(lines) >= max_lines - 1:
                break
    if current.strip():
        lines.append(current.strip())
    return "\n".join(lines)


def split_and_wrap_long_subtitles(
    segments: list,
    words: list | None,
    max_duration: float,
    max_chars: int,
    preferred_line_length: int,
    max_lines: int,
    punctuation: str,
) -> list:
    """æŒ‰æ—¶é•¿ä¸å­—ç¬¦æ•°å°†è¿‡é•¿å­—å¹•æ‹†åˆ†ä¸ºå¤šæ¡ï¼Œå¹¶å¯¹æ¯æ¡æ–‡æœ¬è¿›è¡Œæ¢è¡Œã€‚
    - è‹¥æä¾› wordsï¼ˆè¯çº§æ—¶é—´æˆ³ï¼‰ï¼Œä¼˜å…ˆåœ¨è¯è¾¹ç•Œæ‹†åˆ†ï¼›å¦åˆ™é€€åŒ–ä¸ºæŒ‰æ ‡ç‚¹/å­—ç¬¦åˆ‡åˆ†ã€‚
    """
    if not segments:
        return []

    # å»ºç«‹æ¯æ®µå†…çš„ words ç´¢å¼•ï¼ˆè‹¥æä¾›ï¼‰
    words_by_range: list[list] = []
    if words:
        for seg in segments:
            start, end = seg.get('start', 0.0), seg.get('end', 0.0)
            seg_words = [w for w in words if w.get('start', 0.0) >= start and w.get('end', 0.0) <= end]
            words_by_range.append(seg_words)
    else:
        words_by_range = [[] for _ in segments]

    import re
    punct_set = set(punctuation)

    def split_points_by_chars(text: str) -> list[int]:
        points: list[int] = []
        last = 0
        while last + max_chars < len(text):
            cut = last + max_chars
            # å°½é‡å‘å·¦å›é€€åˆ°æœ€è¿‘çš„ç©ºæ ¼æˆ–æ ‡ç‚¹
            back = cut
            while back > last and text[back - 1] not in punct_set and not text[back - 1].isspace():
                back -= 1
            if back == last:
                back = cut
            points.append(back)
            last = back
        return points

    new_segments: list = []
    for seg, seg_words in zip(segments, words_by_range):
        start = float(seg.get('start', 0.0))
        end = float(seg.get('end', 0.0))
        text = str(seg.get('segment', '')).strip()
        if not text:
            continue

        duration = max(0.0, end - start)
        too_long_by_time = duration > max_duration
        too_long_by_chars = len(text) > max_chars

        if not too_long_by_time and not too_long_by_chars:
            seg_copy = dict(seg)
            seg_copy['segment'] = wrap_text_for_display(text, preferred_line_length, max_lines)
            new_segments.append(seg_copy)
            continue

        # è®¡ç®—åº”æ‹†åˆ†çš„ç‰‡æ®µæ•°ï¼ˆæ—¶é—´/å­—ç¬¦åŒçº¦æŸï¼‰
        parts_by_time = max(1, int(math.ceil(duration / max_duration))) if max_duration > 0 else 1
        parts_by_chars = max(1, int(math.ceil(len(text) / max_chars))) if max_chars > 0 else 1
        parts = max(parts_by_time, parts_by_chars)

        # åŸºäºè¯æ—¶é—´æˆ³æ‹†åˆ†
        if seg_words:
            total_dur = duration if duration > 0 else 1e-6
            target_bounds = [start + i * (total_dur / parts) for i in range(1, parts)]
            cut_times: list[float] = []
            for tb in target_bounds:
                # æ‰¾åˆ°ç¦» tb æœ€è¿‘çš„è¯è¾¹ç•Œ
                best_t = None
                best_d = 1e9
                for w in seg_words:
                    for edge in (w.get('start', 0.0), w.get('end', 0.0)):
                        d = abs(edge - tb)
                        if d < best_d:
                            best_d = d
                            best_t = edge
                if best_t is not None:
                    cut_times.append(best_t)
            cut_times = sorted(t for t in cut_times if start < t < end)

            # æŒ‰ cut_times åˆ‡ç‰‡
            times = [start] + cut_times + [end]
            # å°†è¯æŒ‰æ—¶é—´æ®µåˆ†æ¡¶ï¼Œå¹¶ç»„è£…æ–‡æœ¬
            for i in range(len(times) - 1):
                s_i, e_i = times[i], times[i + 1]
                sub_words = [w for w in seg_words if w.get('start', 0.0) >= s_i and w.get('end', 0.0) <= e_i]
                sub_text = " ".join(w.get('word', '').strip() for w in sub_words if w.get('word'))
                if not sub_text:
                    # å›é€€åˆ°åŸæ–‡æœ¬çš„åˆ‡ç‰‡ä¼°è®¡
                    ratio_a = (s_i - start) / total_dur
                    ratio_b = (e_i - start) / total_dur
                    a = int(ratio_a * len(text))
                    b = int(ratio_b * len(text))
                    sub_text = text[a:b].strip()
                sub_text = wrap_text_for_display(sub_text, preferred_line_length, max_lines)
                new_segments.append({'start': s_i, 'end': e_i, 'segment': sub_text})
            continue

        # æ— è¯çº§æ—¶é—´æˆ³æ—¶ï¼šæŒ‰å­—ç¬¦ä¸æ ‡ç‚¹è¿‘ä¼¼æ‹†åˆ†
        # å…ˆæŒ‰å­—ç¬¦ä¸Šé™è®¡ç®—æ–­ç‚¹
        points = split_points_by_chars(text) if too_long_by_chars else []
        # åŠ å…¥æ ‡ç‚¹æ–­ç‚¹ï¼ˆå¥æœ«ä¼˜å…ˆï¼‰
        for m in re.finditer(r"[ã€‚ï¼ï¼Ÿ!?.,;ï¼›ï¼Œ]", text):
            idx = m.end()
            # åªåœ¨è¿‡é•¿æ—¶è€ƒè™‘
            if len(text) > max_chars or duration > max_duration:
                points.append(idx)
        points = sorted(set(p for p in points if 0 < p < len(text)))

        # æ ¹æ® points åˆ‡æ–‡æœ¬ï¼Œæ—¶é—´å‡åˆ†
        stops = [0] + points + [len(text)]
        times = [start + i * ((end - start) / (len(stops) - 1)) for i in range(len(stops))]
        for i in range(len(stops) - 1):
            a, b = stops[i], stops[i + 1]
            s_i, e_i = times[i], times[i + 1]
            sub_text = text[a:b].strip()
            if not sub_text:
                continue
            sub_text = wrap_text_for_display(sub_text, preferred_line_length, max_lines)
            new_segments.append({'start': s_i, 'end': e_i, 'segment': sub_text})

    # æœ€ç»ˆä¿è¯æŒ‰å¼€å§‹æ—¶é—´æ’åº
    new_segments.sort(key=lambda s: (s.get('start', 0.0), s.get('end', 0.0)))
    return new_segments

def configure_decoding_strategy(model):
    """é…ç½® NeMo æ¨¡å‹çš„è§£ç ç­–ç•¥ï¼ˆè‹¥æ”¯æŒï¼‰ã€‚
    - å¯¹ RNNT/Conformer-Transducer ç­‰æ¨¡å‹ï¼Œå°è¯•å¼€å¯ beam searchã€‚
    - è‹¥æ¨¡å‹ä¸æ”¯æŒç›¸åº”å±æ€§ï¼Œé™é»˜è·³è¿‡ã€‚
    """
    try:
        if hasattr(model, 'change_decoding_strategy'):
            if DECODING_STRATEGY == 'beam':
                model.change_decoding_strategy(decoding_cfg={
                    'strategy': 'beam',
                    'beam_size': RNNT_BEAM_SIZE,
                })
                print(f"âœ… å¯ç”¨ Beam Searchï¼Œbeam_size={RNNT_BEAM_SIZE}")
            else:
                # åœ¨ä½æ˜¾å­˜ç¯å¢ƒä¸‹ï¼Œç¦ç”¨ CUDA graphs é™ä½ä¸€æ¬¡æ€§æ˜¾å­˜å³°å€¼
                model.change_decoding_strategy(decoding_cfg={
                    'strategy': 'greedy',
                    'allow_cuda_graphs': False,
                    'greedy': {
                        'use_cuda_graph_decoder': False,
                        'max_symbols_per_step': 10,
                        'loop_labels': True,
                    }
                })
                print("âœ… å¯ç”¨ Greedy è§£ç ")
        elif hasattr(model, 'decoder') and hasattr(model.decoder, 'cfg'):
            # å…¼å®¹éƒ¨åˆ†æ¨¡å‹çš„ decoder é…ç½®
            decoder_cfg = getattr(model.decoder, 'cfg')
            if DECODING_STRATEGY == 'beam' and hasattr(decoder_cfg, 'beam_size'):
                decoder_cfg.beam_size = RNNT_BEAM_SIZE
                print(f"âœ… é…ç½® decoder.beam_size={RNNT_BEAM_SIZE}")
            # å…¶ä½™æƒ…å†µæŒ‰é»˜è®¤
    except Exception as e:
        print(f"âš ï¸ è®¾ç½®è§£ç ç­–ç•¥æ—¶å‡ºé”™: {e}")


def safe_transcribe(model, audio_path: str, need_timestamps: bool, batch_size: int, num_workers: int):
    """æ‰§è¡Œä¸€æ¬¡å®‰å…¨çš„è½¬å†™ï¼š
    - ä½¿ç”¨ autocast + inference_mode é™ä½æ˜¾å­˜
    - å¦‚é‡ CUDA OOMï¼Œè‡ªåŠ¨é™çº§ä¸º greedy è§£ç å¹¶é‡è¯•ä¸€æ¬¡
    """
    global DECODING_STRATEGY
    try:
        if cuda_available:
            with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.float16):
                return model.transcribe(
                    [audio_path],
                    timestamps=need_timestamps,
                    batch_size=batch_size,
                    num_workers=num_workers,
                )
        else:
            with torch.inference_mode():
                return model.transcribe(
                    [audio_path],
                    timestamps=need_timestamps,
                    batch_size=batch_size,
                    num_workers=num_workers,
                )
    except RuntimeError as e:
        if 'CUDA out of memory' in str(e) or 'CUDA error' in str(e):
            print("âš ï¸ æ£€æµ‹åˆ° CUDA å†…å­˜ä¸è¶³ï¼Œå°è¯•é™çº§ä¸º greedy è§£ç å¹¶é‡è¯•ä¸€æ¬¡â€¦")
            aggressive_memory_cleanup()
            # è®°å½•åŸç­–ç•¥å¹¶é™çº§
            original_strategy = DECODING_STRATEGY
            try:
                # å¼ºåˆ¶åˆ‡æ¢ä¸º greedy
                os.environ['DECODING_STRATEGY'] = 'greedy'
                DECODING_STRATEGY = 'greedy'
                configure_decoding_strategy(model)
                # é‡è¯•
                if cuda_available:
                    with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.float16):
                        return model.transcribe(
                            [audio_path],
                            timestamps=need_timestamps,
                            batch_size=1,  # è¿›ä¸€æ­¥æ”¶ç¼©æ‰¹é‡
                            num_workers=0,
                        )
                else:
                    with torch.inference_mode():
                        return model.transcribe(
                            [audio_path],
                            timestamps=need_timestamps,
                            batch_size=1,
                            num_workers=0,
                        )
            finally:
                # å°è¯•æ¢å¤åŸç­–ç•¥ï¼ˆè‹¥éœ€è¦ï¼‰
                os.environ['DECODING_STRATEGY'] = original_strategy
                DECODING_STRATEGY = original_strategy
                try:
                    configure_decoding_strategy(model)
                except Exception:
                    pass
        # éOOMé”™è¯¯åŸæ ·æŠ›å‡º
        raise
    except ValueError as e:
        # å¤„ç† NeMo TDT Beam åœ¨å¼€å¯ timestamps æ—¶ä¸æ”¯æŒ alignment preservation çš„æƒ…å†µ
        if 'Alignment preservation has not been implemented' in str(e):
            print("âš ï¸ æ£€æµ‹åˆ° TDT Beam è§£ç ä¸æ”¯æŒå¯¹é½ä¿ç•™ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° greedy è§£ç å¹¶é‡è¯•ä¸€æ¬¡â€¦")
            aggressive_memory_cleanup()
            original_strategy = DECODING_STRATEGY
            try:
                # å¼ºåˆ¶åˆ‡æ¢ä¸º greedy
                os.environ['DECODING_STRATEGY'] = 'greedy'
                globals()['DECODING_STRATEGY'] = 'greedy'
                configure_decoding_strategy(model)
                # é‡è¯•ï¼ˆè¿›ä¸€æ­¥é™ä½æ‰¹é‡ä¸å¹¶å‘ï¼‰
                if cuda_available:
                    with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.float16):
                        return model.transcribe(
                            [audio_path],
                            timestamps=need_timestamps,
                            batch_size=1,
                            num_workers=0,
                        )
                else:
                    with torch.inference_mode():
                        return model.transcribe(
                            [audio_path],
                            timestamps=need_timestamps,
                            batch_size=1,
                            num_workers=0,
                        )
            finally:
                # å°è¯•æ¢å¤åŸç­–ç•¥
                os.environ['DECODING_STRATEGY'] = original_strategy
                globals()['DECODING_STRATEGY'] = original_strategy
                try:
                    configure_decoding_strategy(model)
                except Exception:
                    pass
        # å…¶ä»– ValueError ç»§ç»­æŠ›å‡º
        raise

# --- Waitress æœåŠ¡å™¨å¯åŠ¨ ---
if __name__ == '__main__':
    
    # æ ¹æ®æ˜¯å¦å¯ç”¨æ‡’åŠ è½½æ¥å†³å®šæ˜¯é¢„åŠ è½½æ¨¡å‹è¿˜æ˜¯å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
    if ENABLE_LAZY_LOAD:
        print("æ‡’åŠ è½½æ¨¡å¼å·²å¯ç”¨ã€‚æ¨¡å‹å°†åœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶åŠ è½½ã€‚")
        # å¯åŠ¨åå°çº¿ç¨‹æ¥ç›‘æ§å’Œå¸è½½é—²ç½®çš„æ¨¡å‹
        if IDLE_TIMEOUT_MINUTES > 0:
            print(f"å°†å¯ç”¨æ¨¡å‹è‡ªåŠ¨å¸è½½åŠŸèƒ½ï¼Œé—²ç½®è¶…æ—¶: {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿã€‚")
            cleanup_thread = threading.Thread(target=model_cleanup_checker, daemon=True)
            cleanup_thread.start()
        else:
            print("æ¨¡å‹è‡ªåŠ¨å¸è½½åŠŸèƒ½å·²ç¦ç”¨ (IDLE_TIMEOUT_MINUTES=0)ã€‚")
        # å¯åŠ¨åå°çº¿ç¨‹åœ¨å®¹å™¨å¯åŠ¨æ—¶é¢„ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆä»…ä¸‹è½½åˆ°ç£ç›˜ï¼Œä¸åŠ è½½åˆ°å†…å­˜ï¼‰
        print("åœ¨åå°å¯åŠ¨æ¨¡å‹é¢„ä¸‹è½½çº¿ç¨‹ï¼ˆä»…ä¸‹è½½æ–‡ä»¶ï¼Œä¸åŠ è½½åˆ°å†…å­˜ï¼‰...")
        try:
            predownload_thread = threading.Thread(target=predownload_model_artifacts, daemon=True)
            predownload_thread.start()
        except Exception as e:
            print(f"å¯åŠ¨æ¨¡å‹é¢„ä¸‹è½½çº¿ç¨‹å¤±è´¥: {e}")
    else:
        # æ‡’åŠ è½½è¢«ç¦ç”¨ï¼Œåœ¨å¯åŠ¨æ—¶ç›´æ¥åŠ è½½æ¨¡å‹
        print("æ‡’åŠ è½½æ¨¡å¼å·²ç¦ç”¨ï¼Œæ­£åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹...")
        try:
            load_model_if_needed()
        except Exception as e:
            print(f"âŒ å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            # é¢„åŠ è½½å¤±è´¥æ—¶é€€å‡ºï¼Œä»¥é¿å…è¿è¡Œä¸€ä¸ªæŸåçš„æœåŠ¡
            exit(1)

    if API_KEY:
        print(f"API Key è®¤è¯å·²å¯ç”¨ã€‚è¯·åœ¨è¯·æ±‚å¤´ä¸­æä¾› 'Authorization: Bearer YOUR_API_KEY'")
    else:
        print("API Key è®¤è¯å·²ç¦ç”¨ï¼Œä»»ä½•è¯·æ±‚éƒ½å°†è¢«æ¥å—ã€‚")


    # === ç®€åŒ–é…ç½®é¢„è®¾æ¨å¯¼ ===
    # è®¡ç®—å¯ç”¨ GPU æ˜¾å­˜ï¼ˆæˆ–è¯»å–ç”¨æˆ·æä¾›çš„ GPU_VRAM_GBï¼‰ï¼Œç»“åˆ PRESET è®¾ç½®å…¶å®ƒå‚æ•°
    try:
        detected_vram_gb = None
        if check_cuda_compatibility():
            _, _, total_gb = get_gpu_memory_usage()
            detected_vram_gb = round(total_gb)
    except Exception:
        detected_vram_gb = None

    gpu_vram_gb = None
    try:
        gpu_vram_gb = int(GPU_VRAM_GB_ENV) if GPU_VRAM_GB_ENV else detected_vram_gb
    except Exception:
        gpu_vram_gb = detected_vram_gb

    preset = PRESET if PRESET in ['speed', 'balanced', 'quality', 'simple'] else 'balanced'
    if preset == 'simple':
        preset = 'balanced'

    # åŸºäºé¢„è®¾å’Œæ˜¾å­˜æ¨å¯¼å‚æ•°ï¼ˆä»…å½“ç”¨æˆ·æœªæ˜¾å¼è¦†ç›–æ—¶ç”Ÿæ•ˆï¼‰
    def set_if_default(name: str, current, value):
        # ä»…å½“ç¯å¢ƒå˜é‡æœªæ˜¾å¼è®¾ç½®æ—¶æ›¿æ¢é»˜è®¤
        if os.environ.get(name) is None:
            return value
        return current

    # CHUNK_MINITE
    if gpu_vram_gb is not None:
        if preset == 'speed':
            CHUNK_MINITE = set_if_default('CHUNK_MINITE', CHUNK_MINITE,  min(20, 10 if gpu_vram_gb < 12 else 15))
        elif preset == 'quality':
            CHUNK_MINITE = set_if_default('CHUNK_MINITE', CHUNK_MINITE,  max(6, 8 if gpu_vram_gb >= 8 else 6))
        else:  # balanced
            CHUNK_MINITE = set_if_default('CHUNK_MINITE', CHUNK_MINITE,  8 if gpu_vram_gb < 8 else 10)

    # å¹¶å‘ä¸æ˜¾å­˜å æ¯”
    if preset == 'speed':
        MAX_CONCURRENT_INFERENCES = set_if_default('MAX_CONCURRENT_INFERENCES', MAX_CONCURRENT_INFERENCES, 2 if (gpu_vram_gb and gpu_vram_gb >= 16) else 1)
        GPU_MEMORY_FRACTION = set_if_default('GPU_MEMORY_FRACTION', GPU_MEMORY_FRACTION, 0.95)
        DECODING_STRATEGY = set_if_default('DECODING_STRATEGY', DECODING_STRATEGY, 'greedy')
    elif preset == 'quality':
        MAX_CONCURRENT_INFERENCES = set_if_default('MAX_CONCURRENT_INFERENCES', MAX_CONCURRENT_INFERENCES, 1)
        GPU_MEMORY_FRACTION = set_if_default('GPU_MEMORY_FRACTION', GPU_MEMORY_FRACTION, 0.90)
        DECODING_STRATEGY = set_if_default('DECODING_STRATEGY', DECODING_STRATEGY, 'beam')
        RNNT_BEAM_SIZE = set_if_default('RNNT_BEAM_SIZE', RNNT_BEAM_SIZE, 4 if (gpu_vram_gb and gpu_vram_gb >= 8) else 2)
    else:  # balanced
        MAX_CONCURRENT_INFERENCES = set_if_default('MAX_CONCURRENT_INFERENCES', MAX_CONCURRENT_INFERENCES, 1)
        GPU_MEMORY_FRACTION = set_if_default('GPU_MEMORY_FRACTION', GPU_MEMORY_FRACTION, 0.92 if (gpu_vram_gb and gpu_vram_gb >= 12) else 0.90)
        DECODING_STRATEGY = set_if_default('DECODING_STRATEGY', DECODING_STRATEGY, 'greedy')

    # è®°å½•æœ€ç»ˆé¢„è®¾
    print(f"é¢„è®¾: {preset}  | GPU_VRAM_GB: {gpu_vram_gb if gpu_vram_gb is not None else 'unknown'}")
    print(f"æ¨å¯¼: CHUNK_MINITE={CHUNK_MINITE}, MAX_CONCURRENT_INFERENCES={MAX_CONCURRENT_INFERENCES}, GPU_MEMORY_FRACTION={GPU_MEMORY_FRACTION}, DECODING_STRATEGY={DECODING_STRATEGY}")

    # æ›´æ–°å¹¶å‘ä¿¡å·é‡ä»¥åŒ¹é…æ¨å¯¼å€¼
    try:
        new_max_conc = int(MAX_CONCURRENT_INFERENCES) if isinstance(MAX_CONCURRENT_INFERENCES, (int, float, str)) else 1
        if new_max_conc < 1:
            new_max_conc = 1
        globals()['inference_semaphore'] = threading.Semaphore(new_max_conc)
    except Exception as e:
        print(f"âš ï¸ åˆå§‹åŒ–å¹¶å‘ä¿¡å·é‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤1: {e}")
        globals()['inference_semaphore'] = threading.Semaphore(1)

    print(f"ğŸš€ æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print(f"API ç«¯ç‚¹: POST http://{host}:{port}/v1/audio/transcriptions")
    print(f"æœåŠ¡å°†ä½¿ç”¨ {threads} ä¸ªçº¿ç¨‹è¿è¡Œã€‚")
    print("")
    print("=== æ˜¾å­˜ä¼˜åŒ–é…ç½® ===")
    print(f"æ¿€è¿›æ˜¾å­˜æ¸…ç†: {'å¯ç”¨' if AGGRESSIVE_MEMORY_CLEANUP else 'ç¦ç”¨'}")
    print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {'å¯ç”¨' if ENABLE_GRADIENT_CHECKPOINTING else 'ç¦ç”¨'}")
    print(f"å¼ºåˆ¶æ¸…ç†é˜ˆå€¼: {FORCE_CLEANUP_THRESHOLD*100:.0f}%")
    print(f"æœ€å¤§chunkå†…å­˜: {MAX_CHUNK_MEMORY_MB}MB")
    print(f"é»˜è®¤chunkæ—¶é•¿: {CHUNK_MINITE} åˆ†é’Ÿ")
    print("=" * 25)
    print("")
    print("=== é—²ç½®èµ„æºä¼˜åŒ–é…ç½® ===")
    print(f"æ¨¡å‹é—²ç½®è¶…æ—¶: {IDLE_TIMEOUT_MINUTES} åˆ†é’Ÿ")
    print(f"è‡ªåŠ¨æ¨¡å‹å¸è½½é˜ˆå€¼: {AUTO_MODEL_UNLOAD_THRESHOLD_MINUTES} åˆ†é’Ÿ")
    print(f"é—²ç½®å†…å­˜æ¸…ç†é—´éš”: {IDLE_MEMORY_CLEANUP_INTERVAL} ç§’")
    print(f"æ·±åº¦æ¸…ç†é˜ˆå€¼: {IDLE_DEEP_CLEANUP_THRESHOLD} ç§’")
    print(f"é—²ç½®CPUä¼˜åŒ–: {'å¯ç”¨' if ENABLE_IDLE_CPU_OPTIMIZATION else 'ç¦ç”¨'}")
    print(f"ç›‘æ§é—´éš”: {IDLE_MONITORING_INTERVAL} ç§’")
    print(f"è¶…çº§æ¿€è¿›ä¼˜åŒ–: {'å¯ç”¨' if ENABLE_AGGRESSIVE_IDLE_OPTIMIZATION else 'ç¦ç”¨'}")
    print(f"è¯·æ±‚åç«‹å³æ¸…ç†: {'å¯ç”¨' if IMMEDIATE_CLEANUP_AFTER_REQUEST else 'ç¦ç”¨'}")
    print(f"å†…å­˜å‘Šè­¦é˜ˆå€¼: {MEMORY_USAGE_ALERT_THRESHOLD_GB:.1f}GB")
    # åˆå§‹åŒ–CUDAå…¼å®¹æ€§æ£€æŸ¥
    print("æ­£åœ¨æ£€æŸ¥CUDAå…¼å®¹æ€§...")
    cuda_available = check_cuda_compatibility()
    
    if cuda_available:
        _, _, total_memory = get_gpu_memory_usage()
        print(f"GPUæ€»æ˜¾å­˜: {total_memory:.1f}GB")
    else:
        memory = psutil.virtual_memory()
        print(f"ç³»ç»Ÿå†…å­˜: {memory.total/1024**3:.1f}GB")
    print("=" * 25)
    print("")
    print("=== Tensor Core é…ç½® ===")
    print(f"Tensor Core: {'å¯ç”¨' if ENABLE_TENSOR_CORE else 'ç¦ç”¨'}")
    print(f"cuDNN Benchmark: {'å¯ç”¨' if ENABLE_CUDNN_BENCHMARK else 'ç¦ç”¨'}")
    print(f"ç²¾åº¦æ¨¡å¼: {TENSOR_CORE_PRECISION}")
    if cuda_available:
        print(f"GPUæ”¯æŒ: {get_tensor_core_info()}")
    else:
        print("GPUæ”¯æŒ: N/A - CUDAä¸å¯ç”¨æˆ–ä¸å…¼å®¹")
    print("=" * 25)
    print("")
    print("=== å¥å­å®Œæ•´æ€§ä¼˜åŒ– ===")
    print(f"é‡å åˆ†å‰²: {'å¯ç”¨' if ENABLE_OVERLAP_CHUNKING else 'ç¦ç”¨'}")
    if ENABLE_OVERLAP_CHUNKING:
        print(f"é‡å æ—¶é•¿: {CHUNK_OVERLAP_SECONDS}s")
        print(f"è¾¹ç•Œé˜ˆå€¼: {SENTENCE_BOUNDARY_THRESHOLD}")
    print("=" * 25)
    serve(app, host=host, port=port, threads=threads)